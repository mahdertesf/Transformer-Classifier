{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1><b>Pretraining a Transformer Model from Scratch on an Amharic Dataset and Fine-Tuning for Amharic Hate Speech Recognition Task</b></h1>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THqdlyo9Dnql"
   },
   "source": [
    "## Table of Contents  \n",
    "1. [Introduction](#introduction)  \n",
    "2. [Importing Packages](#importing-packages)  \n",
    "3. [Dataset Collection & Preprocessing](#dataset-collection--preprocessing)  \n",
    "   - 3.1 [Data Collection](#data-collection)  \n",
    "   - 3.2 [Data Loading and Cleaning](#data-loading-and-cleaning)  \n",
    "   - 3.3 [Tokenization](#tokenization)  \n",
    "   - 3.4 [Tokenizing and Masking](#tokenizing-and-masking)  \n",
    "   - 3.5 [Creating Training Data Pairs](#creating-training-data-pairs)  \n",
    "4. [Pretraining the Transformer Model](#pretraining-the-transformer-model)  \n",
    "   - 4.1 [Positional Encoding](#positional-encoding)  \n",
    "   - 4.2 [Masking](#masking)  \n",
    "   - 4.3 [Self Attention](#self-attention)  \n",
    "   - 4.4 [Encoder](#encoder)  \n",
    "   - 4.5 [Decoder](#decoder)  \n",
    "   - 4.6 [Transformer](#transformer)  \n",
    "   - 4.7 [Initialize_Model](#initialize-model)  \n",
    "   - 4.8 [Pre-training](#pre-training)  \n",
    "5. [Fine-Tuning for Hate Speech Recognition](#fine-tuning-for-hate-speech-recognition)  \n",
    "   - 5.1 [Preparing the Fine-Tuning Dataset](#preparing-the-fine-tuning-dataset)  \n",
    "   - 5.2 [Adjusting the Model Architecture](#adjusting-the-model-architecture)  \n",
    "   - 5.3 [Fine Tunning The Model](#Fine-tunning-the-model)  \n",
    "6. [Evaluation](#evaluation)  \n",
    "7. [Deployment on Mahder AI App](#deployment-on-mahder-ai-app)  \n",
    "8. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HNqZKWpDnqm"
   },
   "source": [
    "## 1. Introduction  \n",
    "\n",
    "Note.Before starting the project note that all the file and folder structures mentioned here are relative to this notebook.So to avoid file not found errors read the README.md file and you will get a link where to find the datas (They are on my kaggle account)\n",
    "\n",
    "In this notebook, I will pretrain a Transformer network on an Amharic dataset collected from a variety of Telegram channels, using the Masked Language Model (MLM). The primary objective of pretraining is to enable the model to learn contextualized word and phrase representations, thereby enhancing its understanding of language semantics. The Transformerâ€™s self-attention mechanism plays a crucial role by allowing the model to dynamically weigh different parts of the input sequence, effectively capturing long-range dependencies in the data.  \n",
    "\n",
    "After pretraining, I will fine-tune the model on a labeled dataset of hate speech and deploy the resulting model on **Mahder AI** web app.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commented cells are only needed when running this notebook on Google Colab with the appropriate TensorFlow version. Uncomment them only if you are using Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20609,
     "status": "ok",
     "timestamp": 1740392163162,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "6FpRqNOfEfVA",
    "outputId": "4446d25e-06c3-47ee-baf1-6a3a5e19c03e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.0\n",
      "Uninstalling tensorflow-2.18.0:\n",
      "  Successfully uninstalled tensorflow-2.18.0\n",
      "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Note, selecting 'nvidia-driver-550-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.144.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.154.05' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-docker2' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-560-server-560.28.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-570-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cuda-toolkit-doc' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-imex' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-450-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.154.05' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-390' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cuda-toolkit-gcc' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-418' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-430' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-435' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-440' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-455' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-560-560.35.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-560-560.35.05' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-465' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-470-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-495' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-515-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-440-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-555-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-535-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-325-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-460-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-primus-vk-common' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-opencl-dev' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-565-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-418-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cg-toolkit' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-525-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-510-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-libopencl1-dev' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-texture-tools' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-525-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-opencl-icd' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-565-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-450-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-390' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-418' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-430' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-435' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-440' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-455' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-465' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-495' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-390' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-470-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-550-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-418' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-430' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-435' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-440' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-455' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-465' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-495' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-381-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fs' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-container-runtime' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-390' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-410' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-418' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-430' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-435' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-440' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-455' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-465' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-495' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-565-server-565.57.01' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-550-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-440-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-545-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-560-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-460-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-510-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-387-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-570-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.90.07' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.90.12' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-515-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-520-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-535-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-450-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-535-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-470-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-primus-vk-wrapper' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-515-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-535-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-docker' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-570-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-535-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-560-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-378-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-313-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-560-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-440-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-open-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-open-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-open-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-460-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-565-565.77' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-390' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-545-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-418' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-430' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-435' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-440' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-455' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-465' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-495' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-319-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-510-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-530-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-565-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-egl-wayland-common' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-384-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-304-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-550-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-550-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-418-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-525-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-515-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-418-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-525-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-570-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-545-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-560-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-565-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-440-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-375-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-310-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-assistant' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.216.01' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.216.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-460-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-515-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.216.01' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.216.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-535-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-530-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-550-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-364-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-hpc-benchmarks-mpich' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.146.02' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-555-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-11-7' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-11-8' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-12-0' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-12-1' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-12-2' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-12-3' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-12-4' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-12-5' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-12-6' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds-12-8' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.146.02' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-515-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-565-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-545-server-545.29.06' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-565-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-565-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.67' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-560-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-555-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-525-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-515-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-530-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-450-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-legacy-390xx-opencl-icd' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.107.02' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-470-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-profiler' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-361-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.54.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.107.02' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-565-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-current-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-510-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-visual-profiler' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-515-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-367-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-535-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-550-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-570-server-570.86.10' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-570-server-570.86.15' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-570-server-570.86.16' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-390' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-396' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-545-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-418' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-430' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-435' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-440' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-455' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-465' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-495' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-535-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-legacy-390xx-smi' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-prebuilt-kernel' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-418-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-current' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.127.05' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.127.08' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-525-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fs-modules' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-egl-gbm1' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.127.05' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.127.08' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.161.07' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.161.08' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-560-server-560.35.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-560-server-560.35.05' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-440-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-358-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-565-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-515-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-460-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.161.07' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.161.08' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cuda-dev' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-565-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cuda-doc' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-515-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.171.04' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-535-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cuda-gdb' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-340-dev' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.171.04' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cuda-toolkit' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-545-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-510-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-560-560.28.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-535-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-550-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-418-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-525-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-550-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-prime' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-510-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-545-545.29.06' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-dkms' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.54.14' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.54.15' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-515-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-515-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-355-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-535-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-550-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-520-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-550-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-opencl-icd-340' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-opencl-icd-384' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-450-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-304' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-310' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-313' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-319' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-325' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-331' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-334' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-337' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-340' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-343' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-346' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-349' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-352' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-355' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-358' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-361' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-364' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-367' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-375' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-378' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-381' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-384' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-387' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-390' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fs-dkms' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-565-server-565.77' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-470-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-440-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-418-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-340-uvm' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-525-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.120' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.135' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.142' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-535-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-346-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fs-prebuilt-kernel' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-460-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-550-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-530-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-520-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-535-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-settings' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-565-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-390' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-555-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-418' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-430' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-435' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-440' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-455' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-465' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-495' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-container-runtime-hook' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cudnn' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-352-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-560-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-565-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-555-555.42.02' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-555-555.42.06' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-550-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.113.01' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-libopencl1' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-570-570.86.10' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-570-570.86.15' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-570-570.86.16' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-dev-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-565-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.113.01' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-418-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-525-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-450-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-530-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-470-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-343-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-520-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-565-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-450-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.54.14' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-server-550.54.15' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-common-470-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-510-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-570-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-349-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.67' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-555-server-555.42.02' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-555-server-555.42.06' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fabricmanager-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-334-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-535-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-550-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-550-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-container-toolkit' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-libopencl1-340' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-libopencl1-384' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-hpc-benchmarks-openmpi' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-555-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-550-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-hpc-benchmarks' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-container-toolkit-base' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-440-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.129.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-515-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-390' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-440-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-340-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-460-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-418' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-430' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-435' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-440' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-450' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-455' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-460' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-465' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-470' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-495' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-510' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-515' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-525-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-535-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-520' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-525' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-530' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-535' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-545' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-555' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-460-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-565-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-565-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-520-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.129.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-555-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.104.05' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.104.12' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-535-server-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-570-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-384-dev' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-304' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-310' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-313' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-319' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-325' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-331' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-334' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-337' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-340' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-343' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-346' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-349' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-352' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-355' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-358' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-361' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-364' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-367' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-375' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-378' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-381' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-384' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-experimental-387' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.104.05' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.104.12' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-550-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-imex-550' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-imex-560' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-imex-565' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-imex-570' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-331-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.86.05' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-535-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source-525-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-560-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-337-updates' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-gds' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.183.01' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-535.183.06' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.120' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.135' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.142' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.54.03' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-565-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-418-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-fs-prebuilt' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-525-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-555-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-570-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-450-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.183.01' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-535-server-535.183.06' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-550-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-settings-binary' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-kernel' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-modprobe' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-utils-470-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-compute-utils-565-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cg-dev' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-dkms-535-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-cg-doc' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-kernel-source' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-common' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-persistenced' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.90.07' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.90.12' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-565-565.57.01' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-headless-no-dkms-525-open' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-binary' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-smi' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-driver-510-server' for regex '^nvidia-.*'\n",
      "Note, selecting 'nvidia-firmware-550-550.144.03' for regex '^nvidia-.*'\n",
      "Package 'nvidia-egl-wayland-common' is not installed, so not removed\n",
      "Note, selecting 'nvidia-settings' instead of 'nvidia-settings-binary'\n",
      "Package 'nvidia-390' is not installed, so not removed\n",
      "Note, selecting 'libnvtt-bin' instead of 'nvidia-texture-tools'\n",
      "Package 'nvidia-libopencl1-dev' is not installed, so not removed\n",
      "Package 'nvidia-current' is not installed, so not removed\n",
      "Package 'nvidia-current-updates' is not installed, so not removed\n",
      "Package 'nvidia-libopencl1' is not installed, so not removed\n",
      "Package 'nvidia-driver-410' is not installed, so not removed\n",
      "Package 'nvidia-387' is not installed, so not removed\n",
      "Package 'nvidia-387-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-387' is not installed, so not removed\n",
      "Package 'nvidia-384-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-384' is not installed, so not removed\n",
      "Package 'nvidia-381' is not installed, so not removed\n",
      "Package 'nvidia-381-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-381' is not installed, so not removed\n",
      "Package 'nvidia-378' is not installed, so not removed\n",
      "Package 'nvidia-378-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-378' is not installed, so not removed\n",
      "Package 'nvidia-375' is not installed, so not removed\n",
      "Package 'nvidia-375-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-375' is not installed, so not removed\n",
      "Package 'nvidia-367' is not installed, so not removed\n",
      "Package 'nvidia-367-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-367' is not installed, so not removed\n",
      "Package 'nvidia-364' is not installed, so not removed\n",
      "Package 'nvidia-364-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-364' is not installed, so not removed\n",
      "Package 'nvidia-361' is not installed, so not removed\n",
      "Package 'nvidia-361-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-361' is not installed, so not removed\n",
      "Package 'nvidia-358' is not installed, so not removed\n",
      "Package 'nvidia-358-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-358' is not installed, so not removed\n",
      "Package 'nvidia-355' is not installed, so not removed\n",
      "Package 'nvidia-355-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-355' is not installed, so not removed\n",
      "Package 'nvidia-352' is not installed, so not removed\n",
      "Package 'nvidia-352-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-352' is not installed, so not removed\n",
      "Package 'nvidia-349' is not installed, so not removed\n",
      "Package 'nvidia-349-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-349' is not installed, so not removed\n",
      "Package 'nvidia-346' is not installed, so not removed\n",
      "Package 'nvidia-346-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-346' is not installed, so not removed\n",
      "Package 'nvidia-343' is not installed, so not removed\n",
      "Package 'nvidia-343-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-343' is not installed, so not removed\n",
      "Package 'nvidia-340-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-340' is not installed, so not removed\n",
      "Package 'nvidia-337' is not installed, so not removed\n",
      "Package 'nvidia-337-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-337' is not installed, so not removed\n",
      "Package 'nvidia-334' is not installed, so not removed\n",
      "Package 'nvidia-334-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-334' is not installed, so not removed\n",
      "Package 'nvidia-331' is not installed, so not removed\n",
      "Package 'nvidia-331-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-331' is not installed, so not removed\n",
      "Package 'nvidia-325' is not installed, so not removed\n",
      "Package 'nvidia-325-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-325' is not installed, so not removed\n",
      "Package 'nvidia-319' is not installed, so not removed\n",
      "Package 'nvidia-319-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-319' is not installed, so not removed\n",
      "Package 'nvidia-313' is not installed, so not removed\n",
      "Package 'nvidia-313-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-313' is not installed, so not removed\n",
      "Package 'nvidia-310' is not installed, so not removed\n",
      "Package 'nvidia-310-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-310' is not installed, so not removed\n",
      "Package 'nvidia-304' is not installed, so not removed\n",
      "Package 'nvidia-304-updates' is not installed, so not removed\n",
      "Package 'nvidia-experimental-304' is not installed, so not removed\n",
      "Package 'nvidia-legacy-390xx-opencl-icd' is not installed, so not removed\n",
      "Package 'nvidia-legacy-390xx-smi' is not installed, so not removed\n",
      "Package 'nvidia-cuda-doc' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.113.01' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.146.02' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.171.04' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.104.12' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.161.08' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.183.06' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.216.01' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.216.03' is not installed, so not removed\n",
      "Package 'nvidia-firmware-545-server-545.29.06' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.107.02' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.120' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.67' is not installed, so not removed\n",
      "Package 'nvidia-driver-555-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-555-server-open' is not installed, so not removed\n",
      "Package 'nvidia-container-runtime-hook' is not installed, so not removed\n",
      "Package 'nvidia-docker' is not installed, so not removed\n",
      "Package 'nvidia-firmware-555-server-555.42.02' is not installed, so not removed\n",
      "Package 'nvidia-firmware-555-server-555.42.06' is not installed, so not removed\n",
      "Package 'nvidia-fs-prebuilt' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-396' is not installed, so not removed\n",
      "Package 'nvidia-kernel-dkms' is not installed, so not removed\n",
      "Package 'nvidia-driver-560-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-560-server-open' is not installed, so not removed\n",
      "Package 'nvidia-firmware-560-server-560.28.03' is not installed, so not removed\n",
      "Package 'nvidia-firmware-560-server-560.35.03' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.90.12' is not installed, so not removed\n",
      "Package 'nvidia-hpc-benchmarks' is not installed, so not removed\n",
      "Package 'nvidia-firmware-560-server-560.35.05' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.144.03' is not installed, so not removed\n",
      "Package 'nvidia-driver-570-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-570-server-open' is not installed, so not removed\n",
      "Package 'nvidia-firmware-570-server-570.86.10' is not installed, so not removed\n",
      "Package 'nvidia-firmware-570-server-570.86.15' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.135' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.142' is not installed, so not removed\n",
      "Package 'nvidia-firmware-565-server-565.77' is not installed, so not removed\n",
      "Package 'nvidia-firmware-570-server-570.86.16' is not installed, so not removed\n",
      "Package 'nvidia-prime' is not installed, so not removed\n",
      "Package 'nvidia-340' is not installed, so not removed\n",
      "Package 'nvidia-340-dev' is not installed, so not removed\n",
      "Package 'nvidia-340-uvm' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-418' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-435' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-440' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-450' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-455' is not installed, so not removed\n",
      "Package 'nvidia-dkms-418' is not installed, so not removed\n",
      "Package 'nvidia-dkms-435' is not installed, so not removed\n",
      "Package 'nvidia-dkms-440' is not installed, so not removed\n",
      "Package 'nvidia-dkms-450' is not installed, so not removed\n",
      "Package 'nvidia-dkms-455' is not installed, so not removed\n",
      "Package 'nvidia-driver-418' is not installed, so not removed\n",
      "Package 'nvidia-driver-435' is not installed, so not removed\n",
      "Package 'nvidia-driver-440' is not installed, so not removed\n",
      "Package 'nvidia-driver-450' is not installed, so not removed\n",
      "Package 'nvidia-driver-455' is not installed, so not removed\n",
      "Package 'nvidia-headless-418' is not installed, so not removed\n",
      "Package 'nvidia-headless-435' is not installed, so not removed\n",
      "Package 'nvidia-headless-440' is not installed, so not removed\n",
      "Package 'nvidia-headless-450' is not installed, so not removed\n",
      "Package 'nvidia-headless-455' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-418' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-435' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-440' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-450' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-455' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-418' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-435' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-440' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-450' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-455' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-418' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-435' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-440' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-450' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-455' is not installed, so not removed\n",
      "Package 'nvidia-utils-418' is not installed, so not removed\n",
      "Package 'nvidia-utils-435' is not installed, so not removed\n",
      "Package 'nvidia-utils-440' is not installed, so not removed\n",
      "Package 'nvidia-utils-450' is not installed, so not removed\n",
      "Package 'nvidia-utils-455' is not installed, so not removed\n",
      "Package 'nvidia-cg-dev' is not installed, so not removed\n",
      "Package 'nvidia-cg-doc' is not installed, so not removed\n",
      "Package 'nvidia-cg-toolkit' is not installed, so not removed\n",
      "Package 'nvidia-cuda-dev' is not installed, so not removed\n",
      "Package 'nvidia-cuda-gdb' is not installed, so not removed\n",
      "Package 'nvidia-cuda-toolkit' is not installed, so not removed\n",
      "Package 'nvidia-cuda-toolkit-doc' is not installed, so not removed\n",
      "Package 'nvidia-cuda-toolkit-gcc' is not installed, so not removed\n",
      "Package 'nvidia-cudnn' is not installed, so not removed\n",
      "Package 'nvidia-libopencl1-340' is not installed, so not removed\n",
      "Package 'nvidia-opencl-icd-340' is not installed, so not removed\n",
      "Package 'nvidia-primus-vk-common' is not installed, so not removed\n",
      "Package 'nvidia-primus-vk-wrapper' is not installed, so not removed\n",
      "Package 'nvidia-profiler' is not installed, so not removed\n",
      "Package 'nvidia-visual-profiler' is not installed, so not removed\n",
      "Package 'nvidia-384' is not installed, so not removed\n",
      "Package 'nvidia-384-dev' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-390' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-418-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-440-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-450-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-460' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-460-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-465' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-470' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-470-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-495' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-510' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-510-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-515' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-515-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-520' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-525' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-525-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-535-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-545' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-550-server' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-565-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-390' is not installed, so not removed\n",
      "Package 'nvidia-dkms-418-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-440-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-450-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-460' is not installed, so not removed\n",
      "Package 'nvidia-dkms-460-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-465' is not installed, so not removed\n",
      "Package 'nvidia-dkms-470' is not installed, so not removed\n",
      "Package 'nvidia-dkms-470-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-495' is not installed, so not removed\n",
      "Package 'nvidia-dkms-510' is not installed, so not removed\n",
      "Package 'nvidia-dkms-510-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-515' is not installed, so not removed\n",
      "Package 'nvidia-dkms-515-open' is not installed, so not removed\n",
      "Package 'nvidia-dkms-515-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-520' is not installed, so not removed\n",
      "Package 'nvidia-dkms-520-open' is not installed, so not removed\n",
      "Package 'nvidia-dkms-525' is not installed, so not removed\n",
      "Package 'nvidia-dkms-525-open' is not installed, so not removed\n",
      "Package 'nvidia-dkms-525-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-535-open' is not installed, so not removed\n",
      "Package 'nvidia-dkms-535-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-535-server-open' is not installed, so not removed\n",
      "Package 'nvidia-dkms-545' is not installed, so not removed\n",
      "Package 'nvidia-dkms-545-open' is not installed, so not removed\n",
      "Package 'nvidia-dkms-550-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-550-server-open' is not installed, so not removed\n",
      "Package 'nvidia-dkms-565-server' is not installed, so not removed\n",
      "Package 'nvidia-dkms-565-server-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-390' is not installed, so not removed\n",
      "Package 'nvidia-driver-418-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-440-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-450-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-460' is not installed, so not removed\n",
      "Package 'nvidia-driver-460-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-465' is not installed, so not removed\n",
      "Package 'nvidia-driver-470' is not installed, so not removed\n",
      "Package 'nvidia-driver-470-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-495' is not installed, so not removed\n",
      "Package 'nvidia-driver-510' is not installed, so not removed\n",
      "Package 'nvidia-driver-510-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-515' is not installed, so not removed\n",
      "Package 'nvidia-driver-515-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-515-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-520' is not installed, so not removed\n",
      "Package 'nvidia-driver-520-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-525' is not installed, so not removed\n",
      "Package 'nvidia-driver-525-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-525-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-535-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-535-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-535-server-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-545' is not installed, so not removed\n",
      "Package 'nvidia-driver-545-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-550-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-550-server-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-565-server' is not installed, so not removed\n",
      "Package 'nvidia-driver-565-server-open' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-515' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-515' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.104.05' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.113.01' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.129.03' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.146.02' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.154.05' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.161.07' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.171.04' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.183.01' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.54.03' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-535.86.05' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.104.05' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.104.12' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.129.03' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.154.05' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.161.07' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.161.08' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.183.01' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.183.06' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.216.01' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.216.03' is not installed, so not removed\n",
      "Package 'nvidia-firmware-535-server-535.54.03' is not installed, so not removed\n",
      "Package 'nvidia-firmware-545-545.29.06' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.107.02' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.120' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.67' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.127.05' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.127.08' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.54.15' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.90.07' is not installed, so not removed\n",
      "Package 'nvidia-firmware-565-server-565.57.01' is not installed, so not removed\n",
      "Package 'nvidia-headless-390' is not installed, so not removed\n",
      "Package 'nvidia-headless-418-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-440-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-450-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-460' is not installed, so not removed\n",
      "Package 'nvidia-headless-460-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-465' is not installed, so not removed\n",
      "Package 'nvidia-headless-470' is not installed, so not removed\n",
      "Package 'nvidia-headless-470-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-495' is not installed, so not removed\n",
      "Package 'nvidia-headless-510' is not installed, so not removed\n",
      "Package 'nvidia-headless-510-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-515' is not installed, so not removed\n",
      "Package 'nvidia-headless-515-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-515-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-520' is not installed, so not removed\n",
      "Package 'nvidia-headless-520-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-525' is not installed, so not removed\n",
      "Package 'nvidia-headless-525-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-525-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-535-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-535-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-535-server-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-545' is not installed, so not removed\n",
      "Package 'nvidia-headless-545-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-550-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-550-server-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-565-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-565-server-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-390' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-418-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-440-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-450-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-460' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-460-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-465' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-470' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-470-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-495' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-510' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-510-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-515' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-515-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-515-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-520' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-520-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-525' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-525-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-525-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-535-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-535-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-535-server-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-545' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-545-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-550-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-550-server-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-565-server' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-565-server-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-390' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-418-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-440-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-450-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-460' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-460-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-465' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-470' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-470-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-495' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-510' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-510-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-515' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-515-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-520' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-525' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-525-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-535-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-545' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-550-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-565-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-390' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-418-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-440-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-450-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-460' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-460-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-465' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-470' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-470-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-495' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-510' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-510-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-515' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-515-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-515-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-520' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-520-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-525' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-525-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-525-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-535-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-535-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-535-server-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-545' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-545-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-550-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-550-server-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-565-server' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-565-server-open' is not installed, so not removed\n",
      "Package 'nvidia-utils-390' is not installed, so not removed\n",
      "Package 'nvidia-utils-418-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-440-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-450-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-460' is not installed, so not removed\n",
      "Package 'nvidia-utils-460-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-465' is not installed, so not removed\n",
      "Package 'nvidia-utils-470' is not installed, so not removed\n",
      "Package 'nvidia-utils-470-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-495' is not installed, so not removed\n",
      "Package 'nvidia-utils-510' is not installed, so not removed\n",
      "Package 'nvidia-utils-510-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-515' is not installed, so not removed\n",
      "Package 'nvidia-utils-515-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-520' is not installed, so not removed\n",
      "Package 'nvidia-utils-525' is not installed, so not removed\n",
      "Package 'nvidia-utils-525-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-535-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-545' is not installed, so not removed\n",
      "Package 'nvidia-utils-550-server' is not installed, so not removed\n",
      "Package 'nvidia-utils-565-server' is not installed, so not removed\n",
      "Package 'nvidia-common' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-450' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-460' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-470' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-510' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-450' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-460' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-470' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-510' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-server-550.54.14' is not installed, so not removed\n",
      "Package 'nvidia-libopencl1-384' is not installed, so not removed\n",
      "Package 'nvidia-opencl-icd-384' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-430' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-555' is not installed, so not removed\n",
      "Package 'nvidia-container-runtime' is not installed, so not removed\n",
      "Package 'nvidia-dkms-430' is not installed, so not removed\n",
      "Package 'nvidia-dkms-555' is not installed, so not removed\n",
      "Package 'nvidia-dkms-555-open' is not installed, so not removed\n",
      "Package 'nvidia-docker2' is not installed, so not removed\n",
      "Package 'nvidia-driver-430' is not installed, so not removed\n",
      "Package 'nvidia-driver-555' is not installed, so not removed\n",
      "Package 'nvidia-driver-555-open' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-520' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-525' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-530' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-545' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-555' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-520' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-525' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-530' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-545' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-555' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.54.14' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.54.15' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.90.07' is not installed, so not removed\n",
      "Package 'nvidia-firmware-555-555.42.02' is not installed, so not removed\n",
      "Package 'nvidia-firmware-555-555.42.06' is not installed, so not removed\n",
      "Package 'nvidia-gds-11-7' is not installed, so not removed\n",
      "Package 'nvidia-gds-11-8' is not installed, so not removed\n",
      "Package 'nvidia-gds-12-0' is not installed, so not removed\n",
      "Package 'nvidia-gds-12-1' is not installed, so not removed\n",
      "Package 'nvidia-gds-12-2' is not installed, so not removed\n",
      "Package 'nvidia-gds-12-3' is not installed, so not removed\n",
      "Package 'nvidia-gds-12-4' is not installed, so not removed\n",
      "Package 'nvidia-gds-12-5' is not installed, so not removed\n",
      "Package 'nvidia-headless-430' is not installed, so not removed\n",
      "Package 'nvidia-headless-555' is not installed, so not removed\n",
      "Package 'nvidia-headless-555-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-430' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-555' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-555-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-430' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-555' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-515' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-520' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-525' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-530' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-545' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-555' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-430' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-555' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-555-open' is not installed, so not removed\n",
      "Package 'nvidia-utils-430' is not installed, so not removed\n",
      "Package 'nvidia-utils-555' is not installed, so not removed\n",
      "Package 'nvidia-firmware-560-560.28.03' is not installed, so not removed\n",
      "Package 'nvidia-imex' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-530' is not installed, so not removed\n",
      "Package 'nvidia-dkms-530' is not installed, so not removed\n",
      "Package 'nvidia-dkms-530-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-530' is not installed, so not removed\n",
      "Package 'nvidia-driver-530-open' is not installed, so not removed\n",
      "Package 'nvidia-firmware-560-560.35.03' is not installed, so not removed\n",
      "Package 'nvidia-headless-530' is not installed, so not removed\n",
      "Package 'nvidia-headless-530-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-530' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-530-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-530' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-530' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-530-open' is not installed, so not removed\n",
      "Package 'nvidia-utils-530' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.90.12' is not installed, so not removed\n",
      "Package 'nvidia-hpc-benchmarks-mpich' is not installed, so not removed\n",
      "Package 'nvidia-hpc-benchmarks-openmpi' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.127.05' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-565' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-565' is not installed, so not removed\n",
      "Package 'nvidia-firmware-565-565.57.01' is not installed, so not removed\n",
      "Package 'nvidia-imex-565' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-565' is not installed, so not removed\n",
      "Package 'nvidia-open-565' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.127.08' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-560' is not installed, so not removed\n",
      "Package 'nvidia-dkms-560' is not installed, so not removed\n",
      "Package 'nvidia-dkms-560-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-560' is not installed, so not removed\n",
      "Package 'nvidia-driver-560-open' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-560' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-560' is not installed, so not removed\n",
      "Package 'nvidia-firmware-560-560.35.05' is not installed, so not removed\n",
      "Package 'nvidia-gds-12-6' is not installed, so not removed\n",
      "Package 'nvidia-headless-560' is not installed, so not removed\n",
      "Package 'nvidia-headless-560-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-560' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-560-open' is not installed, so not removed\n",
      "Package 'nvidia-imex-560' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-560' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-560' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-560' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-560-open' is not installed, so not removed\n",
      "Package 'nvidia-open-560' is not installed, so not removed\n",
      "Package 'nvidia-utils-560' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-535' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-550' is not installed, so not removed\n",
      "Package 'nvidia-dkms-535' is not installed, so not removed\n",
      "Package 'nvidia-dkms-550' is not installed, so not removed\n",
      "Package 'nvidia-dkms-550-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-535' is not installed, so not removed\n",
      "Package 'nvidia-driver-550' is not installed, so not removed\n",
      "Package 'nvidia-driver-550-open' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-535' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-550' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-535' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-550' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.144.03' is not installed, so not removed\n",
      "Package 'nvidia-headless-535' is not installed, so not removed\n",
      "Package 'nvidia-headless-550' is not installed, so not removed\n",
      "Package 'nvidia-headless-550-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-535' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-550' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-550-open' is not installed, so not removed\n",
      "Package 'nvidia-imex-550' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-535' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-550' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-535' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-550' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-535' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-550' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-550-open' is not installed, so not removed\n",
      "Package 'nvidia-utils-535' is not installed, so not removed\n",
      "Package 'nvidia-utils-550' is not installed, so not removed\n",
      "Package 'nvidia-firmware-570-570.86.10' is not installed, so not removed\n",
      "Package 'nvidia-fs' is not installed, so not removed\n",
      "Package 'nvidia-fs-dkms' is not installed, so not removed\n",
      "Package 'nvidia-gds' is not installed, so not removed\n",
      "Package 'nvidia-gds-12-8' is not installed, so not removed\n",
      "Package 'nvidia-container-toolkit' is not installed, so not removed\n",
      "Package 'nvidia-container-toolkit-base' is not installed, so not removed\n",
      "Package 'nvidia-driver-assistant' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-570' is not installed, so not removed\n",
      "Package 'nvidia-fabricmanager-dev-570' is not installed, so not removed\n",
      "Package 'nvidia-firmware-570-570.86.15' is not installed, so not removed\n",
      "Package 'nvidia-imex-570' is not installed, so not removed\n",
      "Package 'nvidia-kernel-open-570' is not installed, so not removed\n",
      "Package 'nvidia-modprobe' is not installed, so not removed\n",
      "Package 'nvidia-open-570' is not installed, so not removed\n",
      "Package 'nvidia-open' is not installed, so not removed\n",
      "Package 'nvidia-settings' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-565' is not installed, so not removed\n",
      "Package 'nvidia-compute-utils-570' is not installed, so not removed\n",
      "Package 'nvidia-dkms-565' is not installed, so not removed\n",
      "Package 'nvidia-dkms-565-open' is not installed, so not removed\n",
      "Package 'nvidia-dkms-570' is not installed, so not removed\n",
      "Package 'nvidia-dkms-570-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-565' is not installed, so not removed\n",
      "Package 'nvidia-driver-565-open' is not installed, so not removed\n",
      "Package 'nvidia-driver-570' is not installed, so not removed\n",
      "Package 'nvidia-driver-570-open' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.135' is not installed, so not removed\n",
      "Package 'nvidia-firmware-550-550.142' is not installed, so not removed\n",
      "Package 'nvidia-firmware-565-565.77' is not installed, so not removed\n",
      "Package 'nvidia-firmware-570-570.86.16' is not installed, so not removed\n",
      "Package 'nvidia-headless-565' is not installed, so not removed\n",
      "Package 'nvidia-headless-565-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-570' is not installed, so not removed\n",
      "Package 'nvidia-headless-570-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-565' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-565-open' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-570' is not installed, so not removed\n",
      "Package 'nvidia-headless-no-dkms-570-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-565' is not installed, so not removed\n",
      "Package 'nvidia-kernel-common-570' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-565' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-565-open' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-570' is not installed, so not removed\n",
      "Package 'nvidia-kernel-source-570-open' is not installed, so not removed\n",
      "Package 'nvidia-utils-565' is not installed, so not removed\n",
      "Package 'nvidia-utils-570' is not installed, so not removed\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  ocl-icd-opencl-dev opencl-c-headers opencl-clhpp-headers\n",
      "Use 'apt autoremove' to remove them.\n",
      "The following packages will be REMOVED:\n",
      "  nvidia-opencl-dev*\n",
      "0 upgraded, 0 newly installed, 1 to remove and 21 not upgraded.\n",
      "After this operation, 88.1 kB disk space will be freed.\n",
      "(Reading database ... 124926 files and directories currently installed.)\n",
      "Removing nvidia-opencl-dev:amd64 (11.5.1-1ubuntu1) ...\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y tensorflow tensorflow-gpu\n",
    "# !apt-get remove --purge '^nvidia-.*'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378128,
     "status": "ok",
     "timestamp": 1740392550438,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "UiwHVZHKEuVU",
    "outputId": "ce19fcc2-2127-4a9b-ac3b-eebc8fcd2920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mdpkg:\u001b[0m \u001b[1;31merror:\u001b[0m cannot access archive '-': No such file or directory\n",
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,698 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,317 kB]\n",
      "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,526 kB]\n",
      "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,939 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,230 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,661 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,610 kB]\n",
      "Fetched 21.4 MB in 2s (10.2 MB/s)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "33 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  ocl-icd-opencl-dev opencl-c-headers opencl-clhpp-headers\n",
      "Use 'apt autoremove' to remove them.\n",
      "The following additional packages will be installed:\n",
      "  cpp-12 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compiler-11-8 cuda-cudart-11-8\n",
      "  cuda-cudart-dev-11-8 cuda-cuobjdump-11-8 cuda-cupti-11-8 cuda-cupti-dev-11-8 cuda-cuxxfilt-11-8\n",
      "  cuda-demo-suite-11-8 cuda-documentation-11-8 cuda-driver-dev-11-8 cuda-drivers cuda-drivers-570\n",
      "  cuda-gdb-11-8 cuda-libraries-11-8 cuda-libraries-dev-11-8 cuda-memcheck-11-8 cuda-nsight-11-8\n",
      "  cuda-nsight-compute-11-8 cuda-nsight-systems-11-8 cuda-nvcc-11-8 cuda-nvdisasm-11-8\n",
      "  cuda-nvml-dev-11-8 cuda-nvprof-11-8 cuda-nvprune-11-8 cuda-nvrtc-11-8 cuda-nvrtc-dev-11-8\n",
      "  cuda-nvtx-11-8 cuda-nvvp-11-8 cuda-profiler-api-11-8 cuda-runtime-11-8 cuda-sanitizer-11-8\n",
      "  cuda-toolkit-11-8 cuda-toolkit-11-8-config-common cuda-toolkit-11-config-common cuda-tools-11-8\n",
      "  cuda-visual-tools-11-8 dctrl-tools default-jre default-jre-headless dkms fakeroot\n",
      "  fonts-dejavu-core fonts-dejavu-extra gcc-12 gds-tools-11-8 keyboard-configuration libasan8\n",
      "  libatk-wrapper-java libatk-wrapper-java-jni libcublas-11-8 libcublas-dev-11-8 libcufft-11-8\n",
      "  libcufft-dev-11-8 libcufile-11-8 libcufile-dev-11-8 libcurand-11-8 libcurand-dev-11-8\n",
      "  libcusolver-11-8 libcusolver-dev-11-8 libcusparse-11-8 libcusparse-dev-11-8 libfakeroot\n",
      "  libfontenc1 libgail-common libgail18 libgcc-12-dev libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
      "  libgudev-1.0-0 libjansson4 liblocale-gettext-perl libnpp-11-8 libnpp-dev-11-8 libnvidia-cfg1-570\n",
      "  libnvidia-common-570 libnvidia-compute-570 libnvidia-decode-570 libnvidia-encode-570\n",
      "  libnvidia-extra-570 libnvidia-fbc1-570 libnvidia-gl-570 libnvjpeg-11-8 libnvjpeg-dev-11-8\n",
      "  librsvg2-common libtinfo5 libtsan2 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
      "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcvt0 libxfont2\n",
      "  libxkbcommon-x11-0 libxkbfile1 libxtst6 libxxf86dga1 nsight-compute-2022.3.0\n",
      "  nsight-systems-2022.4.2 nvidia-compute-utils-570 nvidia-dkms-570 nvidia-driver-570\n",
      "  nvidia-firmware-570-570.86.15 nvidia-kernel-common-570 nvidia-kernel-source-570 nvidia-modprobe\n",
      "  nvidia-settings nvidia-utils-570 openjdk-11-jre python3-xkit screen-resolution-extra\n",
      "  switcheroo-control systemd-hwe-hwdb udev x11-utils x11-xkb-utils xcvt xfonts-base\n",
      "  xfonts-encodings xfonts-utils xserver-common xserver-xorg-core xserver-xorg-video-nvidia-570\n",
      "Suggested packages:\n",
      "  gcc-12-locales cpp-12-doc debtags menu gcc-12-multilib gcc-12-doc gvfs mesa-utils xfs | xserver\n",
      "  xfonts-100dpi | xfonts-75dpi xfonts-scalable\n",
      "Recommended packages:\n",
      "  libnvidia-compute-570:i386 libnvidia-decode-570:i386 libnvidia-encode-570:i386\n",
      "  libnvidia-fbc1-570:i386 libnvidia-gl-570:i386\n",
      "The following NEW packages will be installed:\n",
      "  cpp-12 cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compiler-11-8 cuda-cudart-11-8\n",
      "  cuda-cudart-dev-11-8 cuda-cuobjdump-11-8 cuda-cupti-11-8 cuda-cupti-dev-11-8 cuda-cuxxfilt-11-8\n",
      "  cuda-demo-suite-11-8 cuda-documentation-11-8 cuda-driver-dev-11-8 cuda-drivers cuda-drivers-570\n",
      "  cuda-gdb-11-8 cuda-libraries-11-8 cuda-libraries-dev-11-8 cuda-memcheck-11-8 cuda-nsight-11-8\n",
      "  cuda-nsight-compute-11-8 cuda-nsight-systems-11-8 cuda-nvcc-11-8 cuda-nvdisasm-11-8\n",
      "  cuda-nvml-dev-11-8 cuda-nvprof-11-8 cuda-nvprune-11-8 cuda-nvrtc-11-8 cuda-nvrtc-dev-11-8\n",
      "  cuda-nvtx-11-8 cuda-nvvp-11-8 cuda-profiler-api-11-8 cuda-runtime-11-8 cuda-sanitizer-11-8\n",
      "  cuda-toolkit-11-8 cuda-toolkit-11-8-config-common cuda-toolkit-11-config-common cuda-tools-11-8\n",
      "  cuda-visual-tools-11-8 dctrl-tools default-jre default-jre-headless dkms fakeroot\n",
      "  fonts-dejavu-core fonts-dejavu-extra gcc-12 gds-tools-11-8 keyboard-configuration libasan8\n",
      "  libatk-wrapper-java libatk-wrapper-java-jni libcublas-11-8 libcublas-dev-11-8 libcudnn8\n",
      "  libcufft-11-8 libcufft-dev-11-8 libcufile-11-8 libcufile-dev-11-8 libcurand-11-8\n",
      "  libcurand-dev-11-8 libcusolver-11-8 libcusolver-dev-11-8 libcusparse-11-8 libcusparse-dev-11-8\n",
      "  libfakeroot libfontenc1 libgail-common libgail18 libgcc-12-dev libgtk2.0-0 libgtk2.0-bin\n",
      "  libgtk2.0-common libgudev-1.0-0 libjansson4 liblocale-gettext-perl libnpp-11-8 libnpp-dev-11-8\n",
      "  libnvidia-cfg1-570 libnvidia-common-570 libnvidia-compute-570 libnvidia-decode-570\n",
      "  libnvidia-encode-570 libnvidia-extra-570 libnvidia-fbc1-570 libnvidia-gl-570 libnvjpeg-11-8\n",
      "  libnvjpeg-dev-11-8 librsvg2-common libtinfo5 libtsan2 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
      "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcvt0 libxfont2\n",
      "  libxkbcommon-x11-0 libxkbfile1 libxtst6 libxxf86dga1 nsight-compute-2022.3.0\n",
      "  nsight-systems-2022.4.2 nvidia-compute-utils-570 nvidia-dkms-570 nvidia-driver-570\n",
      "  nvidia-firmware-570-570.86.15 nvidia-kernel-common-570 nvidia-kernel-source-570 nvidia-modprobe\n",
      "  nvidia-settings nvidia-utils-570 openjdk-11-jre python3-xkit screen-resolution-extra\n",
      "  switcheroo-control systemd-hwe-hwdb udev x11-utils x11-xkb-utils xcvt xfonts-base\n",
      "  xfonts-encodings xfonts-utils xserver-common xserver-xorg-core xserver-xorg-video-nvidia-570\n",
      "0 upgraded, 132 newly installed, 0 to remove and 33 not upgraded.\n",
      "Need to get 3,569 MB of archives.\n",
      "After this operation, 8,991 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblocale-gettext-perl amd64 1.07-4build3 [17.1 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-config-common 11.8.89-1 [16.4 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 keyboard-configuration all 1.205ubuntu3 [206 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 cpp-12 amd64 12.3.0-1ubuntu1~22.04 [10.8 MB]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-8-config-common 11.8.89-1 [16.3 kB]\n",
      "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-11-8 11.8.89-1 [165 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libasan8 amd64 12.3.0-1ubuntu1~22.04 [2,442 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtsan2 amd64 12.3.0-1ubuntu1~22.04 [2,477 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgcc-12-dev amd64 12.3.0-1ubuntu1~22.04 [2,618 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gcc-12 amd64 12.3.0-1ubuntu1~22.04 [21.7 MB]\n",
      "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-11-8 11.8.89-1 [16.4 MB]\n",
      "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-11-8 11.11.3.6-1 [248 MB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 dctrl-tools amd64 2.24-3build2 [66.9 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dkms all 2.8.7-2ubuntu2.2 [70.1 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjansson4 amd64 2.13.1-1.1build3 [32.4 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcvt0 amd64 0.1.1-3 [5,494 B]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-xorg-core amd64 2:21.1.4-2ubuntu1.7~22.04.12 [1,477 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.26+4-1ubuntu1~22.04 [214 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfakeroot amd64 1.28-1ubuntu1 [31.5 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 fakeroot amd64 1.28-1ubuntu1 [60.4 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
      "Get:52 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
      "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-xkit all 0.5.0ubuntu5 [18.5 kB]\n",
      "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 screen-resolution-extra all 0.18.2 [4,396 B]\n",
      "Get:55 http://archive.ubuntu.com/ubuntu jammy/main amd64 switcheroo-control amd64 2.4-3build2 [16.5 kB]\n",
      "Get:56 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
      "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 xcvt amd64 0.1.1-3 [7,140 B]\n",
      "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
      "Get:59 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
      "Get:60 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
      "Get:61 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-11-8 10.9.0.58-1 [94.2 MB]\n",
      "Get:62 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-11-8 1.4.0.31-1 [474 kB]\n",
      "Get:63 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-11-8 10.3.0.86-1 [42.2 MB]\n",
      "Get:64 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-11-8 11.4.1.48-1 [52.3 MB]\n",
      "Get:65 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-11-8 11.7.5.86-1 [116 MB]\n",
      "Get:66 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-11-8 11.8.0.86-1 [102 MB]\n",
      "Get:67 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-11-8 11.9.0.86-1 [1,865 kB]\n",
      "Get:68 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-11-8 11.8.0-1 [2,518 B]\n",
      "Get:69 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-common-570 570.86.15-0ubuntu1 [15.9 kB]\n",
      "Get:70 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-gl-570 570.86.15-0ubuntu1 [162 MB]\n",
      "Get:71 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-source-570 570.86.15-0ubuntu1 [73.1 MB]\n",
      "Get:72 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-firmware-570-570.86.15 570.86.15-0ubuntu1 [64.5 MB]\n",
      "Get:73 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-modprobe 570.86.15-0ubuntu1 [14.9 kB]\n",
      "Get:74 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-common-570 570.86.15-0ubuntu1 [98.6 kB]\n",
      "Get:75 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-dkms-570 570.86.15-0ubuntu1 [14.9 kB]\n",
      "Get:76 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-decode-570 570.86.15-0ubuntu1 [2,434 kB]\n",
      "Get:77 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-compute-570 570.86.15-0ubuntu1 [44.0 MB]\n",
      "Get:78 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-extra-570 570.86.15-0ubuntu1 [72.7 kB]\n",
      "Get:79 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-compute-utils-570 570.86.15-0ubuntu1 [108 kB]\n",
      "Get:80 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-encode-570 570.86.15-0ubuntu1 [105 kB]\n",
      "Get:81 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-utils-570 570.86.15-0ubuntu1 [520 kB]\n",
      "Get:82 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-cfg1-570 570.86.15-0ubuntu1 [145 kB]\n",
      "Get:83 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  xserver-xorg-video-nvidia-570 570.86.15-0ubuntu1 [1,694 kB]\n",
      "Get:84 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-fbc1-570 570.86.15-0ubuntu1 [98.3 kB]\n",
      "Get:85 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-driver-570 570.86.15-0ubuntu1 [491 kB]\n",
      "Get:86 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers-570 570.86.15-0ubuntu1 [2,546 B]\n",
      "Get:87 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers 570.86.15-0ubuntu1 [2,498 B]\n",
      "Get:88 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-runtime-11-8 11.8.0-1 [2,424 B]\n",
      "Get:89 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuobjdump-11-8 11.8.86-1 [165 kB]\n",
      "Get:90 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuxxfilt-11-8 11.8.86-1 [189 kB]\n",
      "Get:91 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cccl-11-8 11.8.89-1 [1,040 kB]\n",
      "Get:92 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-driver-dev-11-8 11.8.89-1 [27.3 kB]\n",
      "Get:93 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-dev-11-8 11.8.89-1 [820 kB]\n",
      "Get:94 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvcc-11-8 11.8.89-1 [43.5 MB]\n",
      "Get:95 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprune-11-8 11.8.86-1 [58.1 kB]\n",
      "Get:96 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-compiler-11-8 11.8.0-1 [2,432 B]\n",
      "Get:97 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-profiler-api-11-8 11.8.86-1 [18.5 kB]\n",
      "Get:98 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-dev-11-8 11.8.89-1 [13.5 MB]\n",
      "Get:99 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-dev-11-8 11.11.3.6-1 [269 MB]\n",
      "Get:100 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-dev-11-8 10.9.0.58-1 [189 MB]\n",
      "Get:101 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-dev-11-8 1.4.0.31-1 [1,062 kB]\n",
      "Get:102 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-dev-11-8 10.3.0.86-1 [42.9 MB]\n",
      "Get:103 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-dev-11-8 11.4.1.48-1 [35.7 MB]\n",
      "Get:104 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-dev-11-8 11.7.5.86-1 [116 MB]\n",
      "Get:105 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-dev-11-8 11.8.0.86-1 [100 MB]\n",
      "Get:106 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-dev-11-8 11.9.0.86-1 [1,536 kB]\n",
      "Get:107 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-dev-11-8 11.8.0-1 [2,554 B]\n",
      "Get:108 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-11-8 11.8.87-1 [15.4 MB]\n",
      "Get:109 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-dev-11-8 11.8.87-1 [2,552 kB]\n",
      "Get:110 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvdisasm-11-8 11.8.86-1 [50.8 MB]\n",
      "Get:111 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-gdb-11-8 11.8.86-1 [4,138 kB]\n",
      "Get:112 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-memcheck-11-8 11.8.86-1 [142 kB]\n",
      "Get:113 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprof-11-8 11.8.87-1 [1,959 kB]\n",
      "Get:114 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvtx-11-8 11.8.86-1 [51.3 kB]\n",
      "Get:115 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-sanitizer-11-8 11.8.86-1 [8,784 kB]\n",
      "Get:116 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-command-line-tools-11-8 11.8.0-1 [2,472 B]\n",
      "Get:117 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-compute-2022.3.0 2022.3.0.22-1 [580 MB]\n",
      "Get:118 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-compute-11-8 11.8.0-1 [3,790 B]\n",
      "Get:119 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2022.4.2 2022.4.2.50-32196742v0 [286 MB]\n",
      "Get:120 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-systems-11-8 11.8.0-1 [3,310 B]\n",
      "Get:121 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-11-8 11.8.86-1 [119 MB]\n",
      "Get:122 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvml-dev-11-8 11.8.86-1 [81.4 kB]\n",
      "Get:123 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvp-11-8 11.8.87-1 [114 MB]\n",
      "Get:124 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-visual-tools-11-8 11.8.0-1 [2,870 B]\n",
      "Get:125 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  gds-tools-11-8 1.4.0.31-1 [38.7 MB]\n",
      "Get:126 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-tools-11-8 11.8.0-1 [2,390 B]\n",
      "Get:127 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-documentation-11-8 11.8.86-1 [49.8 kB]\n",
      "Get:128 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-8 11.8.0-1 [3,374 B]\n",
      "Get:129 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-demo-suite-11-8 11.8.86-1 [3,997 kB]\n",
      "Get:130 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-11-8 11.8.0-1 [2,450 B]\n",
      "Get:131 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8 8.6.0.163-1+cuda11.8 [446 MB]\n",
      "Get:132 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-settings 570.86.15-0ubuntu1 [959 kB]\n",
      "Fetched 3,569 MB in 1min 10s (50.6 MB/s)\n",
      "Extracting templates from packages: 100%\n",
      "Preconfiguring packages ...\n",
      "Selecting previously unselected package liblocale-gettext-perl.\n",
      "(Reading database ... 124923 files and directories currently installed.)\n",
      "Preparing to unpack .../000-liblocale-gettext-perl_1.07-4build3_amd64.deb ...\n",
      "Unpacking liblocale-gettext-perl (1.07-4build3) ...\n",
      "Selecting previously unselected package keyboard-configuration.\n",
      "Preparing to unpack .../001-keyboard-configuration_1.205ubuntu3_all.deb ...\n",
      "Unpacking keyboard-configuration (1.205ubuntu3) ...\n",
      "Selecting previously unselected package cpp-12.\n",
      "Preparing to unpack .../002-cpp-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libasan8:amd64.\n",
      "Preparing to unpack .../003-libasan8_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libtsan2:amd64.\n",
      "Preparing to unpack .../004-libtsan2_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libgcc-12-dev:amd64.\n",
      "Preparing to unpack .../005-libgcc-12-dev_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package gcc-12.\n",
      "Preparing to unpack .../006-gcc-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package dctrl-tools.\n",
      "Preparing to unpack .../007-dctrl-tools_2.24-3build2_amd64.deb ...\n",
      "Unpacking dctrl-tools (2.24-3build2) ...\n",
      "Selecting previously unselected package dkms.\n",
      "Preparing to unpack .../008-dkms_2.8.7-2ubuntu2.2_all.deb ...\n",
      "Unpacking dkms (2.8.7-2ubuntu2.2) ...\n",
      "Selecting previously unselected package udev.\n",
      "Preparing to unpack .../009-udev_249.11-0ubuntu3.12_amd64.deb ...\n",
      "Unpacking udev (249.11-0ubuntu3.12) ...\n",
      "Selecting previously unselected package libjansson4:amd64.\n",
      "Preparing to unpack .../010-libjansson4_2.13.1-1.1build3_amd64.deb ...\n",
      "Unpacking libjansson4:amd64 (2.13.1-1.1build3) ...\n",
      "Selecting previously unselected package cuda-toolkit-11-config-common.\n",
      "Preparing to unpack .../011-cuda-toolkit-11-config-common_11.8.89-1_all.deb ...\n",
      "Unpacking cuda-toolkit-11-config-common (11.8.89-1) ...\n",
      "Selecting previously unselected package cuda-toolkit-11-8-config-common.\n",
      "Preparing to unpack .../012-cuda-toolkit-11-8-config-common_11.8.89-1_all.deb ...\n",
      "Unpacking cuda-toolkit-11-8-config-common (11.8.89-1) ...\n",
      "Selecting previously unselected package cuda-cudart-11-8.\n",
      "Preparing to unpack .../013-cuda-cudart-11-8_11.8.89-1_amd64.deb ...\n",
      "Unpacking cuda-cudart-11-8 (11.8.89-1) ...\n",
      "Selecting previously unselected package cuda-nvrtc-11-8.\n",
      "Preparing to unpack .../014-cuda-nvrtc-11-8_11.8.89-1_amd64.deb ...\n",
      "Unpacking cuda-nvrtc-11-8 (11.8.89-1) ...\n",
      "Selecting previously unselected package libcublas-11-8.\n",
      "Preparing to unpack .../015-libcublas-11-8_11.11.3.6-1_amd64.deb ...\n",
      "Unpacking libcublas-11-8 (11.11.3.6-1) ...\n",
      "Selecting previously unselected package libcufft-11-8.\n",
      "Preparing to unpack .../016-libcufft-11-8_10.9.0.58-1_amd64.deb ...\n",
      "Unpacking libcufft-11-8 (10.9.0.58-1) ...\n",
      "Selecting previously unselected package libcufile-11-8.\n",
      "Preparing to unpack .../017-libcufile-11-8_1.4.0.31-1_amd64.deb ...\n",
      "Unpacking libcufile-11-8 (1.4.0.31-1) ...\n",
      "Selecting previously unselected package libcurand-11-8.\n",
      "Preparing to unpack .../018-libcurand-11-8_10.3.0.86-1_amd64.deb ...\n",
      "Unpacking libcurand-11-8 (10.3.0.86-1) ...\n",
      "Selecting previously unselected package libcusolver-11-8.\n",
      "Preparing to unpack .../019-libcusolver-11-8_11.4.1.48-1_amd64.deb ...\n",
      "Unpacking libcusolver-11-8 (11.4.1.48-1) ...\n",
      "Selecting previously unselected package libcusparse-11-8.\n",
      "Preparing to unpack .../020-libcusparse-11-8_11.7.5.86-1_amd64.deb ...\n",
      "Unpacking libcusparse-11-8 (11.7.5.86-1) ...\n",
      "Selecting previously unselected package libnpp-11-8.\n",
      "Preparing to unpack .../021-libnpp-11-8_11.8.0.86-1_amd64.deb ...\n",
      "Unpacking libnpp-11-8 (11.8.0.86-1) ...\n",
      "Selecting previously unselected package libnvjpeg-11-8.\n",
      "Preparing to unpack .../022-libnvjpeg-11-8_11.9.0.86-1_amd64.deb ...\n",
      "Unpacking libnvjpeg-11-8 (11.9.0.86-1) ...\n",
      "Selecting previously unselected package cuda-libraries-11-8.\n",
      "Preparing to unpack .../023-cuda-libraries-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-libraries-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package libnvidia-common-570.\n",
      "Preparing to unpack .../024-libnvidia-common-570_570.86.15-0ubuntu1_all.deb ...\n",
      "Unpacking libnvidia-common-570 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-gl-570:amd64.\n",
      "Preparing to unpack .../025-libnvidia-gl-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "\u001b[1mdpkg-query:\u001b[0m no packages found matching libnvidia-gl-535\n",
      "Unpacking libnvidia-gl-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-kernel-source-570.\n",
      "Preparing to unpack .../026-nvidia-kernel-source-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-kernel-source-570 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-firmware-570-570.86.15.\n",
      "Preparing to unpack .../027-nvidia-firmware-570-570.86.15_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-firmware-570-570.86.15 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-modprobe.\n",
      "Preparing to unpack .../028-nvidia-modprobe_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-modprobe (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-kernel-common-570.\n",
      "Preparing to unpack .../029-nvidia-kernel-common-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-kernel-common-570 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-dkms-570.\n",
      "Preparing to unpack .../030-nvidia-dkms-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-dkms-570 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-decode-570:amd64.\n",
      "Preparing to unpack .../031-libnvidia-decode-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-decode-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-570:amd64.\n",
      "Preparing to unpack .../032-libnvidia-compute-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-extra-570:amd64.\n",
      "Preparing to unpack .../033-libnvidia-extra-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-extra-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-compute-utils-570.\n",
      "Preparing to unpack .../034-nvidia-compute-utils-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-compute-utils-570 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-encode-570:amd64.\n",
      "Preparing to unpack .../035-libnvidia-encode-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-encode-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-utils-570.\n",
      "Preparing to unpack .../036-nvidia-utils-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-utils-570 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-cfg1-570:amd64.\n",
      "Preparing to unpack .../037-libnvidia-cfg1-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-cfg1-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package libxkbfile1:amd64.\n",
      "Preparing to unpack .../038-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
      "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "Selecting previously unselected package x11-xkb-utils.\n",
      "Preparing to unpack .../039-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
      "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
      "Selecting previously unselected package xserver-common.\n",
      "Preparing to unpack .../040-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
      "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
      "Selecting previously unselected package libxcvt0:amd64.\n",
      "Preparing to unpack .../041-libxcvt0_0.1.1-3_amd64.deb ...\n",
      "Unpacking libxcvt0:amd64 (0.1.1-3) ...\n",
      "Selecting previously unselected package libfontenc1:amd64.\n",
      "Preparing to unpack .../042-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
      "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Selecting previously unselected package libxfont2:amd64.\n",
      "Preparing to unpack .../043-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
      "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
      "Selecting previously unselected package xserver-xorg-core.\n",
      "Preparing to unpack .../044-xserver-xorg-core_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
      "Unpacking xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
      "Selecting previously unselected package xserver-xorg-video-nvidia-570.\n",
      "Preparing to unpack .../045-xserver-xorg-video-nvidia-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking xserver-xorg-video-nvidia-570 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-fbc1-570:amd64.\n",
      "Preparing to unpack .../046-libnvidia-fbc1-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-fbc1-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-driver-570.\n",
      "Preparing to unpack .../047-nvidia-driver-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-driver-570 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package cuda-drivers-570.\n",
      "Preparing to unpack .../048-cuda-drivers-570_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking cuda-drivers-570 (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package cuda-drivers.\n",
      "Preparing to unpack .../049-cuda-drivers_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking cuda-drivers (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package cuda-runtime-11-8.\n",
      "Preparing to unpack .../050-cuda-runtime-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-runtime-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package cuda-cuobjdump-11-8.\n",
      "Preparing to unpack .../051-cuda-cuobjdump-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-cuobjdump-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-cuxxfilt-11-8.\n",
      "Preparing to unpack .../052-cuda-cuxxfilt-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-cuxxfilt-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-cccl-11-8.\n",
      "Preparing to unpack .../053-cuda-cccl-11-8_11.8.89-1_amd64.deb ...\n",
      "Unpacking cuda-cccl-11-8 (11.8.89-1) ...\n",
      "Selecting previously unselected package cuda-driver-dev-11-8.\n",
      "Preparing to unpack .../054-cuda-driver-dev-11-8_11.8.89-1_amd64.deb ...\n",
      "Unpacking cuda-driver-dev-11-8 (11.8.89-1) ...\n",
      "Selecting previously unselected package cuda-cudart-dev-11-8.\n",
      "Preparing to unpack .../055-cuda-cudart-dev-11-8_11.8.89-1_amd64.deb ...\n",
      "Unpacking cuda-cudart-dev-11-8 (11.8.89-1) ...\n",
      "Selecting previously unselected package cuda-nvcc-11-8.\n",
      "Preparing to unpack .../056-cuda-nvcc-11-8_11.8.89-1_amd64.deb ...\n",
      "Unpacking cuda-nvcc-11-8 (11.8.89-1) ...\n",
      "Selecting previously unselected package cuda-nvprune-11-8.\n",
      "Preparing to unpack .../057-cuda-nvprune-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-nvprune-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-compiler-11-8.\n",
      "Preparing to unpack .../058-cuda-compiler-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-compiler-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package cuda-profiler-api-11-8.\n",
      "Preparing to unpack .../059-cuda-profiler-api-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-profiler-api-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-nvrtc-dev-11-8.\n",
      "Preparing to unpack .../060-cuda-nvrtc-dev-11-8_11.8.89-1_amd64.deb ...\n",
      "Unpacking cuda-nvrtc-dev-11-8 (11.8.89-1) ...\n",
      "Selecting previously unselected package libcublas-dev-11-8.\n",
      "Preparing to unpack .../061-libcublas-dev-11-8_11.11.3.6-1_amd64.deb ...\n",
      "Unpacking libcublas-dev-11-8 (11.11.3.6-1) ...\n",
      "Selecting previously unselected package libcufft-dev-11-8.\n",
      "Preparing to unpack .../062-libcufft-dev-11-8_10.9.0.58-1_amd64.deb ...\n",
      "Unpacking libcufft-dev-11-8 (10.9.0.58-1) ...\n",
      "Selecting previously unselected package libcufile-dev-11-8.\n",
      "Preparing to unpack .../063-libcufile-dev-11-8_1.4.0.31-1_amd64.deb ...\n",
      "Unpacking libcufile-dev-11-8 (1.4.0.31-1) ...\n",
      "Selecting previously unselected package libcurand-dev-11-8.\n",
      "Preparing to unpack .../064-libcurand-dev-11-8_10.3.0.86-1_amd64.deb ...\n",
      "Unpacking libcurand-dev-11-8 (10.3.0.86-1) ...\n",
      "Selecting previously unselected package libcusolver-dev-11-8.\n",
      "Preparing to unpack .../065-libcusolver-dev-11-8_11.4.1.48-1_amd64.deb ...\n",
      "Unpacking libcusolver-dev-11-8 (11.4.1.48-1) ...\n",
      "Selecting previously unselected package libcusparse-dev-11-8.\n",
      "Preparing to unpack .../066-libcusparse-dev-11-8_11.7.5.86-1_amd64.deb ...\n",
      "Unpacking libcusparse-dev-11-8 (11.7.5.86-1) ...\n",
      "Selecting previously unselected package libnpp-dev-11-8.\n",
      "Preparing to unpack .../067-libnpp-dev-11-8_11.8.0.86-1_amd64.deb ...\n",
      "Unpacking libnpp-dev-11-8 (11.8.0.86-1) ...\n",
      "Selecting previously unselected package libnvjpeg-dev-11-8.\n",
      "Preparing to unpack .../068-libnvjpeg-dev-11-8_11.9.0.86-1_amd64.deb ...\n",
      "Unpacking libnvjpeg-dev-11-8 (11.9.0.86-1) ...\n",
      "Selecting previously unselected package cuda-libraries-dev-11-8.\n",
      "Preparing to unpack .../069-cuda-libraries-dev-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-libraries-dev-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package cuda-cupti-11-8.\n",
      "Preparing to unpack .../070-cuda-cupti-11-8_11.8.87-1_amd64.deb ...\n",
      "Unpacking cuda-cupti-11-8 (11.8.87-1) ...\n",
      "Selecting previously unselected package cuda-cupti-dev-11-8.\n",
      "Preparing to unpack .../071-cuda-cupti-dev-11-8_11.8.87-1_amd64.deb ...\n",
      "Unpacking cuda-cupti-dev-11-8 (11.8.87-1) ...\n",
      "Selecting previously unselected package cuda-nvdisasm-11-8.\n",
      "Preparing to unpack .../072-cuda-nvdisasm-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-nvdisasm-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-gdb-11-8.\n",
      "Preparing to unpack .../073-cuda-gdb-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-gdb-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-memcheck-11-8.\n",
      "Preparing to unpack .../074-cuda-memcheck-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-memcheck-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-nvprof-11-8.\n",
      "Preparing to unpack .../075-cuda-nvprof-11-8_11.8.87-1_amd64.deb ...\n",
      "Unpacking cuda-nvprof-11-8 (11.8.87-1) ...\n",
      "Selecting previously unselected package cuda-nvtx-11-8.\n",
      "Preparing to unpack .../076-cuda-nvtx-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-nvtx-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-sanitizer-11-8.\n",
      "Preparing to unpack .../077-cuda-sanitizer-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-sanitizer-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-command-line-tools-11-8.\n",
      "Preparing to unpack .../078-cuda-command-line-tools-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-command-line-tools-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package nsight-compute-2022.3.0.\n",
      "Preparing to unpack .../079-nsight-compute-2022.3.0_2022.3.0.22-1_amd64.deb ...\n",
      "Unpacking nsight-compute-2022.3.0 (2022.3.0.22-1) ...\n",
      "Selecting previously unselected package cuda-nsight-compute-11-8.\n",
      "Preparing to unpack .../080-cuda-nsight-compute-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-nsight-compute-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package libtinfo5:amd64.\n",
      "Preparing to unpack .../081-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
      "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
      "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
      "Preparing to unpack .../082-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-icccm4:amd64.\n",
      "Preparing to unpack .../083-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
      "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
      "Selecting previously unselected package libxcb-util1:amd64.\n",
      "Preparing to unpack .../084-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
      "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
      "Selecting previously unselected package libxcb-image0:amd64.\n",
      "Preparing to unpack .../085-libxcb-image0_0.4.0-2_amd64.deb ...\n",
      "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
      "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
      "Preparing to unpack .../086-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
      "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
      "Selecting previously unselected package libxcb-render-util0:amd64.\n",
      "Preparing to unpack .../087-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
      "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
      "Selecting previously unselected package libxcb-xkb1:amd64.\n",
      "Preparing to unpack .../088-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
      "Preparing to unpack .../089-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libxcb-xinput0:amd64.\n",
      "Preparing to unpack .../090-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package nsight-systems-2022.4.2.\n",
      "Preparing to unpack .../091-nsight-systems-2022.4.2_2022.4.2.50-32196742v0_amd64.deb ...\n",
      "Unpacking nsight-systems-2022.4.2 (2022.4.2.50-32196742v0) ...\n",
      "Selecting previously unselected package cuda-nsight-systems-11-8.\n",
      "Preparing to unpack .../092-cuda-nsight-systems-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-nsight-systems-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package default-jre-headless.\n",
      "Preparing to unpack .../093-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
      "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "Preparing to unpack .../094-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Selecting previously unselected package openjdk-11-jre:amd64.\n",
      "Preparing to unpack .../095-openjdk-11-jre_11.0.26+4-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package default-jre.\n",
      "Preparing to unpack .../096-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
      "Unpacking default-jre (2:1.11-72build2) ...\n",
      "Selecting previously unselected package cuda-nsight-11-8.\n",
      "Preparing to unpack .../097-cuda-nsight-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-nsight-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-nvml-dev-11-8.\n",
      "Preparing to unpack .../098-cuda-nvml-dev-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-nvml-dev-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-nvvp-11-8.\n",
      "Preparing to unpack .../099-cuda-nvvp-11-8_11.8.87-1_amd64.deb ...\n",
      "Unpacking cuda-nvvp-11-8 (11.8.87-1) ...\n",
      "Selecting previously unselected package cuda-visual-tools-11-8.\n",
      "Preparing to unpack .../100-cuda-visual-tools-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-visual-tools-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package gds-tools-11-8.\n",
      "Preparing to unpack .../101-gds-tools-11-8_1.4.0.31-1_amd64.deb ...\n",
      "Unpacking gds-tools-11-8 (1.4.0.31-1) ...\n",
      "Selecting previously unselected package cuda-tools-11-8.\n",
      "Preparing to unpack .../102-cuda-tools-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-tools-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package cuda-documentation-11-8.\n",
      "Preparing to unpack .../103-cuda-documentation-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-documentation-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-toolkit-11-8.\n",
      "Preparing to unpack .../104-cuda-toolkit-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-toolkit-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package cuda-demo-suite-11-8.\n",
      "Preparing to unpack .../105-cuda-demo-suite-11-8_11.8.86-1_amd64.deb ...\n",
      "Unpacking cuda-demo-suite-11-8 (11.8.86-1) ...\n",
      "Selecting previously unselected package cuda-11-8.\n",
      "Preparing to unpack .../106-cuda-11-8_11.8.0-1_amd64.deb ...\n",
      "Unpacking cuda-11-8 (11.8.0-1) ...\n",
      "Selecting previously unselected package libfakeroot:amd64.\n",
      "Preparing to unpack .../107-libfakeroot_1.28-1ubuntu1_amd64.deb ...\n",
      "Unpacking libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
      "Selecting previously unselected package fakeroot.\n",
      "Preparing to unpack .../108-fakeroot_1.28-1ubuntu1_amd64.deb ...\n",
      "Unpacking fakeroot (1.28-1ubuntu1) ...\n",
      "Selecting previously unselected package fonts-dejavu-core.\n",
      "Preparing to unpack .../109-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
      "Selecting previously unselected package fonts-dejavu-extra.\n",
      "Preparing to unpack .../110-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
      "Selecting previously unselected package libxxf86dga1:amd64.\n",
      "Preparing to unpack .../111-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
      "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Selecting previously unselected package x11-utils.\n",
      "Preparing to unpack .../112-x11-utils_7.7+5build2_amd64.deb ...\n",
      "Unpacking x11-utils (7.7+5build2) ...\n",
      "Selecting previously unselected package libatk-wrapper-java.\n",
      "Preparing to unpack .../113-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
      "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
      "Preparing to unpack .../114-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
      "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libcudnn8.\n",
      "Preparing to unpack .../115-libcudnn8_8.6.0.163-1+cuda11.8_amd64.deb ...\n",
      "Unpacking libcudnn8 (8.6.0.163-1+cuda11.8) ...\n",
      "Selecting previously unselected package libgtk2.0-common.\n",
      "Preparing to unpack .../116-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
      "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgtk2.0-0:amd64.\n",
      "Preparing to unpack .../117-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
      "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgail18:amd64.\n",
      "Preparing to unpack .../118-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
      "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgail-common:amd64.\n",
      "Preparing to unpack .../119-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
      "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgtk2.0-bin.\n",
      "Preparing to unpack .../120-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
      "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
      "Preparing to unpack .../121-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
      "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
      "Selecting previously unselected package librsvg2-common:amd64.\n",
      "Preparing to unpack .../122-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "Selecting previously unselected package python3-xkit.\n",
      "Preparing to unpack .../123-python3-xkit_0.5.0ubuntu5_all.deb ...\n",
      "Unpacking python3-xkit (0.5.0ubuntu5) ...\n",
      "Selecting previously unselected package screen-resolution-extra.\n",
      "Preparing to unpack .../124-screen-resolution-extra_0.18.2_all.deb ...\n",
      "Unpacking screen-resolution-extra (0.18.2) ...\n",
      "Selecting previously unselected package nvidia-settings.\n",
      "Preparing to unpack .../125-nvidia-settings_570.86.15-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-settings (570.86.15-0ubuntu1) ...\n",
      "Selecting previously unselected package switcheroo-control.\n",
      "Preparing to unpack .../126-switcheroo-control_2.4-3build2_amd64.deb ...\n",
      "Unpacking switcheroo-control (2.4-3build2) ...\n",
      "Selecting previously unselected package systemd-hwe-hwdb.\n",
      "Preparing to unpack .../127-systemd-hwe-hwdb_249.11.5_all.deb ...\n",
      "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
      "Selecting previously unselected package xcvt.\n",
      "Preparing to unpack .../128-xcvt_0.1.1-3_amd64.deb ...\n",
      "Unpacking xcvt (0.1.1-3) ...\n",
      "Selecting previously unselected package xfonts-encodings.\n",
      "Preparing to unpack .../129-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
      "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
      "Selecting previously unselected package xfonts-utils.\n",
      "Preparing to unpack .../130-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
      "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
      "Selecting previously unselected package xfonts-base.\n",
      "Preparing to unpack .../131-xfonts-base_1%3a1.0.5_all.deb ...\n",
      "Unpacking xfonts-base (1:1.0.5) ...\n",
      "Setting up cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up cuda-nvml-dev-11-8 (11.8.86-1) ...\n",
      "Setting up default-jre-headless (2:1.11-72build2) ...\n",
      "Setting up cuda-toolkit-11-config-common (11.8.89-1) ...\n",
      "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up cuda-cccl-11-8 (11.8.89-1) ...\n",
      "Setting up cuda-cuobjdump-11-8 (11.8.86-1) ...\n",
      "Setting up cuda-nvrtc-11-8 (11.8.89-1) ...\n",
      "Setting up cuda-sanitizer-11-8 (11.8.86-1) ...\n",
      "Setting up libnvidia-extra-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Setting up libcudnn8 (8.6.0.163-1+cuda11.8) ...\n",
      "Setting up cuda-cupti-11-8 (11.8.87-1) ...\n",
      "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
      "Setting up nvidia-kernel-source-570 (570.86.15-0ubuntu1) ...\n",
      "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
      "Setting up openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
      "Setting up cuda-nvdisasm-11-8 (11.8.86-1) ...\n",
      "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
      "Setting up default-jre (2:1.11-72build2) ...\n",
      "Setting up cuda-cuxxfilt-11-8 (11.8.86-1) ...\n",
      "Setting up libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
      "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
      "Setting up libjansson4:amd64 (2.13.1-1.1build3) ...\n",
      "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up nvidia-modprobe (570.86.15-0ubuntu1) ...\n",
      "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
      "Setting up fakeroot (1.28-1ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
      "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Setting up libnvidia-common-570 (570.86.15-0ubuntu1) ...\n",
      "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up cuda-nvvp-11-8 (11.8.87-1) ...\n",
      "Setting up nvidia-firmware-570-570.86.15 (570.86.15-0ubuntu1) ...\n",
      "Setting up cuda-nvtx-11-8 (11.8.86-1) ...\n",
      "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
      "Setting up libnvidia-cfg1-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Setting up cuda-gdb-11-8 (11.8.86-1) ...\n",
      "Setting up cuda-toolkit-11-8-config-common (11.8.89-1) ...\n",
      "Setting alternatives\n",
      "update-alternatives: using /usr/local/cuda-11.8 to provide /usr/local/cuda-11 (cuda-11) in auto mode\n",
      "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
      "Setting up udev (249.11-0ubuntu3.12) ...\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up libcusolver-11-8 (11.4.1.48-1) ...\n",
      "Setting up cuda-nvrtc-dev-11-8 (11.8.89-1) ...\n",
      "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
      "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "Setting up cuda-driver-dev-11-8 (11.8.89-1) ...\n",
      "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
      "Setting up cuda-memcheck-11-8 (11.8.86-1) ...\n",
      "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
      "Setting up gds-tools-11-8 (1.4.0.31-1) ...\n",
      "Setting up cuda-nsight-11-8 (11.8.86-1) ...\n",
      "Setting up cuda-profiler-api-11-8 (11.8.86-1) ...\n",
      "Setting up libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up libxcvt0:amd64 (0.1.1-3) ...\n",
      "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
      "Setting up cuda-documentation-11-8 (11.8.86-1) ...\n",
      "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "Setting up libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up python3-xkit (0.5.0ubuntu5) ...\n",
      "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
      "Setting up cuda-nvprune-11-8 (11.8.86-1) ...\n",
      "Setting up cuda-cudart-11-8 (11.8.89-1) ...\n",
      "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
      "Setting up libnvjpeg-11-8 (11.9.0.86-1) ...\n",
      "Setting up liblocale-gettext-perl (1.07-4build3) ...\n",
      "Setting up libnvidia-fbc1-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
      "Setting up dctrl-tools (2.24-3build2) ...\n",
      "Setting up cuda-nvprof-11-8 (11.8.87-1) ...\n",
      "Setting up nsight-compute-2022.3.0 (2022.3.0.22-1) ...\n",
      "Setting up nvidia-kernel-common-570 (570.86.15-0ubuntu1) ...\n",
      "Setting up nsight-systems-2022.4.2 (2022.4.2.50-32196742v0) ...\n",
      "update-alternatives: using /opt/nvidia/nsight-systems/2022.4.2/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
      "update-alternatives: using /opt/nvidia/nsight-systems/2022.4.2/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
      "Setting up libcusparse-11-8 (11.7.5.86-1) ...\n",
      "Setting up libnvidia-gl-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Setting up libcufft-11-8 (10.9.0.58-1) ...\n",
      "Setting up cuda-cupti-dev-11-8 (11.8.87-1) ...\n",
      "Setting up libcufft-dev-11-8 (10.9.0.58-1) ...\n",
      "Setting up cuda-cudart-dev-11-8 (11.8.89-1) ...\n",
      "Setting up x11-xkb-utils (7.7+5build4) ...\n",
      "Setting up libnpp-11-8 (11.8.0.86-1) ...\n",
      "Setting up libcusolver-dev-11-8 (11.4.1.48-1) ...\n",
      "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Setting up xfonts-utils (1:7.7+6build2) ...\n",
      "Setting up xcvt (0.1.1-3) ...\n",
      "Setting up cuda-nsight-systems-11-8 (11.8.0-1) ...\n",
      "Setting up cuda-command-line-tools-11-8 (11.8.0-1) ...\n",
      "Setting up libcusparse-dev-11-8 (11.7.5.86-1) ...\n",
      "Setting up xfonts-base (1:1.0.5) ...\n",
      "Setting up libcurand-11-8 (10.3.0.86-1) ...\n",
      "Setting up libcufile-11-8 (1.4.0.31-1) ...\n",
      "Setting alternatives\n",
      "Setting up libcublas-11-8 (11.11.3.6-1) ...\n",
      "Setting up libnpp-dev-11-8 (11.8.0.86-1) ...\n",
      "Setting up libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up cuda-libraries-11-8 (11.8.0-1) ...\n",
      "Setting up cuda-nsight-compute-11-8 (11.8.0-1) ...\n",
      "Setting up screen-resolution-extra (0.18.2) ...\n",
      "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
      "Setting up switcheroo-control (2.4-3build2) ...\n",
      "Created symlink /etc/systemd/system/graphical.target.wants/switcheroo-control.service â†’ /lib/systemd/system/switcheroo-control.service.\n",
      "Setting up x11-utils (7.7+5build2) ...\n",
      "Setting up nvidia-settings (570.86.15-0ubuntu1) ...\n",
      "Setting up libnvjpeg-dev-11-8 (11.9.0.86-1) ...\n",
      "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
      "Setting up keyboard-configuration (1.205ubuntu3) ...\n",
      "Your console font configuration will be updated the next time your system\n",
      "boots. If you want to update it now, run 'setupcon' from a virtual console.\n",
      "Setting up cuda-nvcc-11-8 (11.8.89-1) ...\n",
      "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Setting up xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
      "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "Setting up libcublas-dev-11-8 (11.11.3.6-1) ...\n",
      "Setting up libcurand-dev-11-8 (10.3.0.86-1) ...\n",
      "Setting up libcufile-dev-11-8 (1.4.0.31-1) ...\n",
      "Setting up gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up cuda-compiler-11-8 (11.8.0-1) ...\n",
      "Setting up xserver-xorg-video-nvidia-570 (570.86.15-0ubuntu1) ...\n",
      "Setting up cuda-libraries-dev-11-8 (11.8.0-1) ...\n",
      "Setting up dkms (2.8.7-2ubuntu2.2) ...\n",
      "Setting up cuda-visual-tools-11-8 (11.8.0-1) ...\n",
      "Setting up nvidia-dkms-570 (570.86.15-0ubuntu1) ...\n",
      "Loading new nvidia-570.86.15 DKMS files...\n",
      "It is likely that 6.1.85+ belongs to a chroot's host\n",
      "Building for 5.15.0-133-generic\n",
      "Building for architecture x86_64\n",
      "Building initial module for 5.15.0-133-generic\n",
      "Done.\n",
      "\n",
      "nvidia.ko:\n",
      "Running module version sanity check.\n",
      " - Original module\n",
      "   - No original module exists within this kernel\n",
      " - Installation\n",
      "   - Installing to /lib/modules/5.15.0-133-generic/updates/dkms/\n",
      "\n",
      "nvidia-modeset.ko:\n",
      "Running module version sanity check.\n",
      " - Original module\n",
      "   - No original module exists within this kernel\n",
      " - Installation\n",
      "   - Installing to /lib/modules/5.15.0-133-generic/updates/dkms/\n",
      "\n",
      "nvidia-drm.ko:\n",
      "Running module version sanity check.\n",
      " - Original module\n",
      "   - No original module exists within this kernel\n",
      " - Installation\n",
      "   - Installing to /lib/modules/5.15.0-133-generic/updates/dkms/\n",
      "\n",
      "nvidia-uvm.ko:\n",
      "Running module version sanity check.\n",
      " - Original module\n",
      "   - No original module exists within this kernel\n",
      " - Installation\n",
      "   - Installing to /lib/modules/5.15.0-133-generic/updates/dkms/\n",
      "\n",
      "nvidia-peermem.ko:\n",
      "Running module version sanity check.\n",
      " - Original module\n",
      "   - No original module exists within this kernel\n",
      " - Installation\n",
      "   - Installing to /lib/modules/5.15.0-133-generic/updates/dkms/\n",
      "\n",
      "depmod...\n",
      "Setting up cuda-tools-11-8 (11.8.0-1) ...\n",
      "Setting up cuda-toolkit-11-8 (11.8.0-1) ...\n",
      "Setting alternatives\n",
      "Setting up libnvidia-decode-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Setting up libnvidia-encode-570:amd64 (570.86.15-0ubuntu1) ...\n",
      "Setting up nvidia-utils-570 (570.86.15-0ubuntu1) ...\n",
      "Setting up nvidia-compute-utils-570 (570.86.15-0ubuntu1) ...\n",
      "Setting up nvidia-driver-570 (570.86.15-0ubuntu1) ...\n",
      "Setting up cuda-drivers-570 (570.86.15-0ubuntu1) ...\n",
      "Setting up cuda-drivers (570.86.15-0ubuntu1) ...\n",
      "Setting up cuda-runtime-11-8 (11.8.0-1) ...\n",
      "Setting up cuda-demo-suite-11-8 (11.8.86-1) ...\n",
      "Setting up cuda-11-8 (11.8.0-1) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
      "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
      "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
     ]
    }
   ],
   "source": [
    "# !wget -qO- https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb | sudo dpkg -i -\n",
    "# !apt update\n",
    "# !apt install -y cuda-11-8 libcudnn8=8.6.0.163-1+cuda11.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2210,
     "status": "ok",
     "timestamp": 1740393200859,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "kPzFdIEUHodH",
    "outputId": "e4c04465-3f2c-438c-f77f-8a1802a7f760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.13.0 in /usr/local/lib/python3.11/dist-packages (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (1.70.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (3.12.1)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (18.1.1)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (4.25.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (1.17.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (1.17.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.13.0) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.45.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#install the appropriate version of tensorflow\n",
    "!pip install tensorflow==2.13.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2333,
     "status": "ok",
     "timestamp": 1740393214068,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "WW8at-CvIL6V",
    "outputId": "dd05e504-d0ed-4d25-8297-5b40bf659529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.4/590.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: emoji\n",
      "Successfully installed emoji-2.14.1\n"
     ]
    }
   ],
   "source": [
    "#install emoji module for handling emojis(emoji module is not available on colab by default so install it if you are running this notebook on colab)\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCHEbWZGDnqm"
   },
   "source": [
    "\n",
    "## 2. Importing the Packages\n",
    "\n",
    "Let's start by importing all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3592,
     "status": "ok",
     "timestamp": 1740393224158,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "Lg2APpbGDnqm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import emoji\n",
    "import sentencepiece as spm\n",
    "import string\n",
    "from collections import Counter\n",
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Collection & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740393228376,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "J9-RKYPFIgjY",
    "outputId": "755e9857-5dcf-4194-ea15-6bc0bff222c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# if you are using GPUs or colab GPU Check the availability of gpu for tf\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "#Alse make sure you have tf version 2.13.0\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCk6TgLaDnqn"
   },
   "source": [
    "### 3.1 Data Collection  \n",
    "\n",
    "In order to pretrain the Transformer network from scratch, we will use **self-supervised learning**, which requires a large corpus of unlabeled text. We will apply a **Masked Language Model (MLM)** to pre-train the model.  \n",
    "\n",
    "#### **Why Telegram Channels?**  \n",
    "Telegram is the most widely used platform for information storage in Ethiopia. For this reason, I have chosen **Telegram channels** as the primary data source. Most of the selected channels are news channels, ensuring a diverse and rich dataset.  \n",
    "\n",
    "#### **Data Collection Method**  \n",
    "To collect the data, I used the **Telethon Python library** and the **Telegram API** to scrape text from selected channels.  \n",
    "\n",
    "#### **Selected Telegram Channels**  \n",
    "The dataset has been collected from the following Telegram channels:  \n",
    "\n",
    "- [Tikvah Ethiopia](https://t.me/tikvahethiopia)  \n",
    "- [Addis Standard Amharic](https://t.me/AddisstandardAmh)  \n",
    "- [Tarikn Wedehuala](https://t.me/TariknWedehuala)  \n",
    "- [Addis News](https://t.me/Addis_News)  \n",
    "- [Zena 24 Now](https://t.me/zena24now)  \n",
    "- [Tikvah University](https://t.me/TikvahUniversity)  \n",
    "- [Tikvah Ethiopia Magazine](https://t.me/tikvahethmagazine)  \n",
    "- [Tikvah Ethiopia Sport](https://t.me/tikvahethsport)  \n",
    "- [Philosophy Thoughts](https://t.me/Philosophy_Thoughts1)  \n",
    "- [Mudenyaz](https://t.me/Mudenyaz)  \n",
    "- [Yemeri Terekoch](https://t.me/yemeri_terekoch)  \n",
    "- [Bemnet Library](https://t.me/Bemnet_Library)  \n",
    "- [Amazing Fact](https://t.me/amazing_fact_433)  \n",
    "- [Zephilosophy](https://t.me/Zephilosophy)  \n",
    "- [Huluezih](https://t.me/huluezih)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1Qa0MApDnqo"
   },
   "source": [
    "### 3.2 Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIHZnrjcDnqo"
   },
   "source": [
    "Next, we will define a function that will load data from a JSON file as an array of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740393245465,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "rKd5x-4DDnqo"
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    return_data=[]\n",
    "    with open (filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "        datas=json.load(file)\n",
    "        for data in datas:\n",
    "            return_data.append(data[\"text\"])\n",
    "\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hzi36FeDnqo"
   },
   "source": [
    "The following code shows how the news look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "error",
     "timestamp": 1740393247224,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "iXO7Af_nDnqo",
    "outputId": "2c4355af-ab05-416c-f438-9be1384a70ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#áˆ˜á‰„á‹¶áŠ•á‹«\n",
      "\n",
      "áˆ°á‹áŠ• áˆˆáˆ˜áˆ­á‹³á‰µ áˆ°á‹ áˆ˜áˆ†áŠ• á‰ á‰‚ áŠá‹ !\n",
      "\n",
      "á‰µáˆ‹áŠ•á‰µ á‹¨áŠ«á‰²á‰µ 1/2017 á‹“/áˆ á‰ áŒ€áˆ˜áˆ¨á‹ á‹¨áˆ˜á‰„á‹¶áŠ•á‹« á‹¨áŠ áˆ¨áŒ‹á‹Šá‹«áŠ• áŠ¥áŠ“ á‹¨áŠ áŠ¥áˆáˆ® áˆ…áˆ™áˆ›áŠ• áˆ˜áˆ­áŒƒ áˆ›á‹•áŠ¨áˆ á‹¨á‹µáŒ‹á áˆ›áˆ°á‰£áˆ°á‰¥ á‹˜áˆ˜á‰» áŠ¥áˆµáŠ©áŠ• 120,000,000 á‰¥áˆ­ á‰°áˆ°á‰¥áˆµá‰§áˆá¢\n",
      "\n",
      "áˆ˜á‰„á‹¶áŠ•á‹« á‰ áˆšá‹«áˆµáŒˆáŠá‰£á‹ áˆ†áˆµá’á‰³áˆ áŒ­áˆáˆ­ á‹«áˆˆá‹ áˆ…áŠ•áƒ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹¨áŒˆáŠ•á‹˜á‰¥ áŠ¥áŒ¥áˆ¨á‰µ áŠ áŒ‹áŒ¥áˆžá‰³áˆá¢ áˆ…áŠ•áƒá‹ áˆˆáˆ›áŒ áŠ“á‰€á‰… áŒˆáŠ•á‹˜á‰¥ á‰°á‰¸áŒáˆ¨áŠ“áˆá¢ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹ˆá‹° 5 á‰¢áˆŠá‹®áŠ• á‰¥áˆ­ á‹«áˆµáˆáˆáŒ‹áˆá¢\n",
      "\n",
      "á‰ á‰€áŒ¥á‰³ á‹­áŠ¨á‰³á‰°áˆ‰ ðŸ‘‡\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "á‹¨áˆá‰µá‰½áˆ‰á‰µáŠ• áˆáˆ‰ á‹µáŒ‹á áŠ á‹µáˆ­áŒ‰á¢\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ”Š #á‹¨áˆ áˆ«á‰°áŠžá‰½á‹µáˆáŒ½\n",
      "\n",
      "\" á‰‹áˆš áˆ áˆ«á‰°áŠžá‰½ áˆ†áŠáŠ• áˆ³áˆˆ á‰ á‹°áˆžá‹ áˆ›áˆ»áˆ»á‹«á‹ áŠ áˆá‰°áŠ«á‰°á‰µáŠ•áˆ \" - á‹¨áˆ€á‹‹áˆ³ á‹™áˆªá‹« á‹ˆáˆ¨á‹³ áˆ˜áŠ•áŒáˆµá‰µ áˆ áˆ«á‰°áŠžá‰½\n",
      "\n",
      "á‹¨áˆ›áŠ­áˆ® áŠ¢áŠ®áŠ–áˆš áˆ›áˆ»áˆ»á‹« áˆªáŽáˆ­áˆ™áŠ• á‰°áŠ¨á‰µáˆŽ á‹¨áˆšáŠ¨áˆ°á‰± á‹¨áŠ‘áˆ® á‹‰á‹µáŠá‰µáŠ“ á‰°á‹«á‹«á‹¥ áŒ‰á‹³á‹®á‰½áŠ• á‰³áˆ³á‰¢ á‰ áˆ›á‹µáˆ¨áŒ á‹¨áˆ˜áŠ•áŒáˆµá‰µ áˆ áˆ«á‰°áŠžá‰½ á‹°áˆžá‹ áˆ›áˆ»áˆ»á‹« á‰°á‹°áˆ­áŒŽ áŠ¨áŒ¥á‰…áˆá‰µ á‹ˆáˆ­ 2017 á‹“/áˆ áŒ€áˆáˆ® á‰°áŒá‰£áˆ«á‹Š á‹¨á‰°á‹°áˆ¨áŒˆ áˆ˜áˆ†áŠ‘ á‹­á‰³á‹ˆá‰ƒáˆá¢\n",
      "\n",
      "á‰ áˆ²á‹³áˆ› áŠ­áˆáˆá¤ áˆ°áˆœáŠ“á‹Š áˆ²á‹³áˆ› á‹žáŠ•á¤ áˆ€á‹‹áˆ³ á‹™áˆªá‹« á‹ˆáˆ¨á‹³ á‰ á‰°áˆˆá‹«á‹© á‹¨áˆ˜áŠ•áŒáˆµá‰µ áˆ˜áˆµáˆªá‹« á‰¤á‰¶á‰½ á‹¨áˆšáˆ°áˆ© á‹¨áˆ˜áŠ•áŒáˆµá‰µ áˆ áˆ«á‰°áŠžá‰½ áŒáŠ• \" áŠ¨2012 á‹“/áˆ áŒ€áˆáˆ® á‰ á‰‹áˆšáŠá‰µ á‰°á‰€áŒ¥áˆ¨áŠ• áŠ¥á‹¨áˆ°áˆ«áŠ• á‹«áˆˆáŠ• á‰¢áˆ†áŠ•áˆ á‰ áŠ á‹²áˆ± á‹¨áˆ˜áŠ•áŒáˆµá‰µ áˆ áˆ«á‰°áŠžá‰½ á‹¨á‹°áˆ˜á‹ˆá‹ áˆ›áˆ»áˆ»á‹« áŠ áˆá‰°áŠ«á‰°á‰µáŠ•áˆ \" áˆ²áˆ‰ á‰…áˆ¬á‰³á‰¸á‹‰áŠ• áˆˆá‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« áŠ áˆµáŒˆá‰¥á‰°á‹‹áˆá¢\n",
      "\n",
      "á‰…áˆ¬á‰³á‰¸á‹‰áŠ• áŠ«á‹°áˆ¨áˆ±áŠ• áˆ˜áŠ«áŠ¨áˆ á¦\n",
      "- á‰ áŠ¨á‰°áˆ› áˆáˆ›á‰µáŠ“ áŠ®áŠ•áˆµá‰µáˆ«áŠ­áˆ½áŠ•á£\n",
      "- áˆ›á‹˜áŒ‹áŒƒ á‰¤á‰¶á‰½á£\n",
      "- á‰ á‰µáˆáˆ…áˆ­á‰µ á‹˜áˆ­á á£\n",
      "- á‰ áˆ´á‰¶á‰½áŠ“ áˆ•áƒáŠ“á‰µ áŠ¥áŠ•á‹²áˆáˆ á‰ áˆ•á‰¥áˆ¨á‰µ áˆµáˆ« áŒ½/á‰¤á‰¶á‰½ á‹¨áˆšáˆ°áˆ© áˆ áˆ«á‰°áŠžá‰½ áŠ“á‰¸á‹á¢\n",
      "\n",
      "\" á‰ á‹ˆá‰…á‰± á‰ áŠ áŒá‰£á‰¡ áˆ›áˆµá‰³á‹ˆá‰‚á‹« á‹ˆáŒ¥á‰¶ á‰°áˆ˜á‹áŒá‰ áŠ•áŠ“ á‰°á‹ˆá‹³á‹µáˆ¨áŠ• áˆ›áˆˆá‹á‰½áŠ• á‰°áˆ¨áŒ‹áŒáŒ¦ á‹¨á‰‹áˆšáŠá‰µ á‹°á‰¥á‹³á‰¤ á‰°áˆ°áŒ¥á‰¶áŠ• áˆ‹áˆˆá‰á‰µ áŠ áˆáˆµá‰µáŠ“ áˆµá‹µáˆµá‰µ á‹“áˆ˜á‰³á‰µ á‹°áˆžá‹ áˆ²áŠ¨áˆáˆˆáŠ• á‰ á‰†á‹¨áŠ•á‰£á‰¸á‹ áˆ˜á‹°á‰¦á‰½ áˆ‹á‹­ áŠ¥á‹¨áˆ°áˆ«áŠ• á‰£áˆˆáŠ•á‰ á‰µ á‰ áŠ á‹²áˆ± á‹¨á‹°áˆžá‹ áˆ›áˆ»áˆ»á‹« áŠ áˆˆáˆ˜áŠ«á‰°á‰³á‰½áŠ• áˆˆá‹˜áˆ­áˆ á‰¥á‹™ á‰½áŒáˆ®á‰½ á‹³áˆ­áŒŽáŠ“áˆ \" á‰¥áˆˆá‹‹áˆá¢\n",
      "\n",
      "\" áˆˆá‹ˆáˆ¨á‹³á‹‰ áá‰¥áˆŠáŠ­ áˆ°áˆ­á‰ªáˆµáŠ“ á‹¨áˆ°á‹‰ áˆƒá‰¥á‰µ áˆáˆ›á‰µ áŒ½/á‰¤á‰µ áŠ¥áŠ“ áˆˆáŠ­áˆáˆ‰ áá‰¥áˆŠáŠ­ áˆ°áˆ­á‰ªáˆµ á‰¢áˆ® á‰…áˆ¬á‰³á‰½áŠ•áŠ• á‰ áŠ áŠ«áˆáŠ“ á‰ á…áˆá á‰¥áŠ“á‰€áˆ­á‰¥áˆ á‰°áŒˆá‰¢á‹‰ áˆáˆ‹áˆ½ áŠ áˆá‰°áˆ°áŒ áŠ•áˆ áŒ‰á‹³á‹©áŠ• áˆˆáŠ¢á‰µá‹®áŒµá‹« áŠ¥áˆá‰£ áŒ á‰£á‰‚ á‰°á‰‹áˆ áˆˆáˆ›á‰…áˆ¨á‰¥ áˆ˜áˆ¨áŒƒ áŠ¥á‹«á‹°áˆ«áŒ€áŠ• áŠá‹ \" áˆ²áˆ‰ á‰°áŠ“áŒáˆ¨á‹‹áˆá¢\n",
      "\n",
      "á‰ƒáˆ‹á‰¸á‹áŠ• áˆˆá‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« á‹¨áˆ°áŒ¡á‰µ á¤ á‹¨áˆ€á‹‹áˆ³ á‹™áˆªá‹« á‹ˆáˆ¨á‹³ áá‰¥áˆŠáŠ­ áˆ°áˆ­á‰ªáˆµ áŠ¥áŠ“ á‹¨áˆ°á‹‰ áˆƒá‰¥á‰µ áˆáˆ›á‰µ áŒ½/á‰¤á‰µ áŠƒáˆ‹áŠ áŠ á‰¶ áˆƒá‹­áˆ‰ áŠ á‰¢áŠ– á¥ \" á‰ á‹ˆáˆ¨á‹³á‹‰ á‰ 2012 á‹“/áˆ á‹¨áŠá‰ áˆ¨á‹ áŠ áŒá‰£á‰¥áŠá‰µ á‰ áˆŒáˆˆá‹ á‰…áŒ¥áˆ­ á‰ áŠ áŠ•á‹µ áˆ˜á‹°á‰¥ áˆ¶áˆµá‰µáŠ“ áŠ áˆ«á‰µ áˆ°á‹Žá‰½áŠ• á‰ á‰°á‹°áˆ«áˆ«á‰¢áŠá‰µ á‹¨áˆ˜á‰…áŒ áˆ­ áˆáŠ”á‰³á‹Žá‰½ áŠ áˆáŠ• áˆˆá‰°áˆáŒ áˆ¨á‹ á‰½áŒáˆ­ á‹‹áŠáŠ› áˆáŠ­áŠ•á‹«á‰µ áˆ†áŠ—áˆ \" áˆ²áˆ‰ áŒˆáˆáŒ¸á‹‹áˆá¢\n",
      "\n",
      "áŠ¨á‹žáŠ‘áŠ“ á‹¨áŠ­áˆáˆ‰ áá‰¥áˆáŠ­ áˆ°áˆ­á‰ªáˆµ áŒ‹áˆ­ á‰ áˆ˜áŠ“á‰ á‰¥ áˆ˜áá‰µáˆ” áŠ¥á‹«áˆáˆ‹áˆˆáŒ‰ áˆµáˆˆáˆ˜áˆ†áŠ‘áˆ áŒ á‰áˆ˜á‹‹áˆá¢\n",
      "\n",
      "á‰ á‹ˆá‰…á‰± á‹­áˆ…áŠ• á‰°áŒá‰£áˆ­ á‹¨áˆá€áˆ™ áŠ áˆ˜áˆ«áˆ®á‰½ áŠ¥áŠ“ á‹¨áˆ°á‹‰ áˆƒá‰¥á‰µ áˆáˆ›á‰µ áŠƒáˆ‹áŠá‹Žá‰½ áˆ‹á‹­ áŠ¥áˆ­áˆáŒƒ áˆ˜á‹ˆáˆ°á‹±áŠ• á‹¨áˆšáŠ“áŒˆáˆ©á‰µ áŠƒáˆ‹áŠá‹‰ á‰ á‹ˆáˆ¨á‹³á‹‰ á‰ á‹šáˆ… áˆ˜áˆáŠ­ á‰°áŒ á‰€áŒ¥áˆ¨á‹‰ á‰ áŠ á‹²áˆ± á‹¨á‹°áˆ˜á‹ˆá‹ áˆ›áˆ»áˆ»á‹« á‹«áˆáŠ«á‰°á‰±áŠ“ á‰ á‰€áŒ£á‹­ áˆ˜áá‰µáˆ” á‹¨áˆšáˆáˆˆáŒáˆ‹á‰¸á‹‰ 470 á‰ á‰°áˆˆá‹«á‹© áˆ˜áˆµáˆªá‹« á‰¤á‰¶á‰½ á‹‰áˆµáŒ¥ á‹¨á‰°áˆˆá‹© áˆ°áˆ«á‰°áŠžá‰½ áˆµáˆˆáˆ˜áŠ–áˆ«á‰¸á‹‰ áŠ áŠ­áˆˆá‹‹áˆá¢\n",
      "\n",
      "á‹¨áˆ°áˆœáŠ“á‹Š áˆ²á‹³áˆ› á‹žáŠ• áá‰¥áˆáŠ­ áˆ°áˆ­á‰ªáˆµáŠ“ á‹¨áˆ°á‹‰ áˆƒá‹­áˆ áˆáˆ›á‰µ áˆ˜áˆáˆªá‹« áŠƒáˆ‹áŠ áŠ á‰¶ á‰ á‹›á‰¥áˆ… á‰£áˆ­áˆ¶ á‰ á‰ áŠ©áˆ‹á‰¸á‹ á‰ 2011 áŠ¥áŠ“ 2012 á‰ áŠ áŠ¨á‰£á‰¢á‹ áˆ•áŒˆá‹ˆáŒ¥ á‰…áŒ¥áˆ®á‰½ áˆ˜áˆá€áˆ›á‰¸á‹áŠ• áŒˆáˆáŒ¸á‹‹áˆá¢\n",
      "\n",
      "á‰ á‹ˆáˆ¨á‹³á‹‰ áŠ áŒ£áˆª á‰¡á‹µáŠ• á‰°á‰‹á‰áˆž á‰ áŠ á‹²áˆ± á‹°áˆžá‹ á‹«áˆá‰°áŠ«á‰°á‰±áŠ•áŠ“ á‰ á‹ˆáˆ¨á‹³á‹ á‰…áŒ¥áˆ­ á‹«áˆá‰°áˆá€áˆ˜á‰£á‰¸á‹‰ áŠ­áá‰µ áˆ˜á‹°á‰¦á‰½áŠ• á‹¨áˆ˜áˆˆá‹¨á‰µ áˆµáˆ« áˆ˜áŠ¨áŠ“á‹ˆáŠ‘áŠ• áŠ áŠ•áˆµá‰°á‹‰ á‰ áˆ€á‹‹áˆ³ á‹™áˆªá‹« á‹ˆáˆ¨á‹³ á‰¥á‰» 407 áŠ­áá‰µ áˆ˜á‹°á‰¦á‰½ áˆ˜áŠ–áˆ«á‰¸á‹‰áŠ• áˆˆáˆ›á‹ˆá‰… áˆ˜á‰»áˆ‰áŠ• áŒˆáˆá€á‹‹áˆá¢\n",
      "\n",
      "á‹¨áŠ­áˆáˆ‰ á‹¨á‰ áˆ‹á‹­ áŠ áˆ˜áˆ«áˆ®á‰½ á‰ áˆšá‹«áˆµá‰€áˆáŒ¡á‰µ áŠ á‰…áŒ£áŒ« áˆ˜áˆ°áˆ¨á‰µ áŠ¥áŠá‹šáˆ…áŠ• áˆ áˆ«á‰°áŠžá‰½ á‰ áŠá‹šáˆ… áŠ­áá‰µ áˆ˜á‹°á‰¦á‰½ á‹¨áˆ˜á‹°áˆá‹°áˆáŠ“ áˆŒáˆŽá‰½áˆ áˆ•áŒ‹á‹Š áŠ áˆ˜áˆ«áŒ®á‰½ á‰ áˆ˜áˆáˆˆáŒ á‰ áŠ áŒ­áˆ­ áŒŠá‹œ á‹‰áˆµáŒ¥ áŠ¥áˆá‰£á‰µ áˆˆáˆ˜áˆµáŒ á‰µ áŠ¥á‹¨á‰°áˆ°áˆ« áˆ˜áˆ†áŠ‘áŠ• áŠ áˆµá‰³á‹á‰€á‹‹áˆá¢\n",
      "\n",
      "á‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« áŒ‰á‹³á‹©áŠ• áŠ¥áˆµáŠ¨áˆ˜áŒ¨áˆ¨áˆ» á‰°áŠ¨á‰³á‰µáˆŽ áˆ˜áˆ¨áŒƒá‹áŠ• á‹­áˆáŠ«áˆá¢\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "á‹¨IMF áˆ›áŠ”áŒ‚áŠ•áŒ á‹³á‹­áˆ¬áŠ­á‰°áˆ¯ áˆáŠ• áŠ áˆ‰ ?\n",
      "\n",
      "á‹¨á‹“áˆˆáˆ áŠ á‰€á á‹¨áŒˆáŠ•á‹˜á‰¥ á‰°á‰‹áˆ (IMF) áˆ›áŠ”áŒ‚áŠ•áŒ á‹³á‹­áˆ¬áŠ­á‰°áˆ­ áŠ­áˆªáˆµá‰³áˆŠáŠ“ áŒ†áˆ­áŒ‚á‹¬á‰« á‰ áŠ¢á‰µá‹®áŒµá‹« á‹¨áˆµáˆ« á‰†á‹­á‰³ áŠ á‹µáˆ­áŒˆá‹‹áˆá¢\n",
      "\n",
      "á‰ á‹šáˆ…áˆ á‹ˆá‰…á‰µ áŠ¨áŒ /áˆš á‹á‰¢á‹­ áŠ áˆ…áˆ˜á‹µ (á‹¶/áˆ­) áŒ‹áˆ­ áŒ¨áˆáˆ® áŠ¨áŒá‹´áˆ«áˆ áŠ¨áá‰°áŠ› á‰£áˆˆáˆµáˆáŒ£áŠ“á‰µ áŒ‹áˆ­ áˆ˜áŠ­áˆ¨á‹‹áˆá¢\n",
      "\n",
      "á‹¨áŠá‰ áˆ«á‰¸á‹áŠ• á‰†á‹­á‰³ á‰ á‰°áˆ˜áˆˆáŠ¨á‰° áŠ¨áŒˆáŠ•á‹˜á‰¥ áˆšáŠ’áˆµá‰µáˆ© áŠ á‰¶ áŠ áˆ…áˆ˜á‹µ áˆ½á‹´ áŒ‹áˆ­ á‰ áŒ‹áˆ« áˆ˜áŒáˆˆáŒ« áˆ°áŒ¥á‰°á‹ áŠá‰ áˆ­á¢\n",
      "\n",
      "áˆáŠ• áŠ áˆ‰ ?\n",
      "\n",
      "á‹³á‹­áˆ¬áŠ­á‰°áˆ¯ á¤ \" á‹¨áŠ¢á‰µá‹®áŒµá‹« áˆªáŽáˆ­áˆ áŠ¨á‰£á‹µ áŠ¥áŠ“ áŒŠá‹œ á‹¨áˆšá‹ˆáˆµá‹µ áŠá‹ á¤ áŠ¥á‰£áŠ«á‰½áˆ á‰³áŒˆáˆ± \" á‹¨áˆšáˆ áŒ¥áˆª áŠ á‰…áˆ­á‰ á‹‹áˆá¢\n",
      "\n",
      "áŠ¢á‰µá‹®áŒµá‹«á‹á‹«áŠ• áˆˆá‰µá‹•áŒáˆµá‰µ áŠ¥áŠ•á‹²á‹«áˆ³á‹© áŠ¥áŠ“ áŠ¨áˆ˜áŠ•áŒáˆµá‰µ á‹¨áŠ¢áŠ®áŠ–áˆš áˆ›áˆ»áˆ»á‹« áŒ¥áˆ¨á‰¶á‰½ áŒŽáŠ• áŠ¥áŠ•á‹²á‰†áˆ™ áŒ á‹­á‰€á‹‹áˆá¢\n",
      "\n",
      "áŒ†áˆ­áŒ‚á‹¬á‰« á¥ \" á‹¨áˆªáŽáˆ­áˆ™áŠ• áŒá‰¦á‰½ áˆˆáˆ›áˆ³áŠ«á‰µ á‹¨áŠ áŠ•á‹µáŠá‰µ áŠ áˆµáˆáˆ‹áŒŠ áŠá‹ \" áˆ²áˆ‰ áŠ á…áŠ•áŠ¦á‰µ áˆ°áŒ¥á‰°á‹‹áˆá¢\n",
      "\n",
      "\" áŠ¢á‰µá‹®áŒµá‹« á‹¨á‰°á‰€á‰ áˆˆá‰½á‹ áˆªáŽáˆ­áˆ áŠ¨á‰£á‹µ áŠ¥áŠ“ áŒŠá‹œ á‹¨áˆšá‹ˆáˆµá‹µ á‰¢áˆ†áŠ•áˆ áŠ¥áŒ…áŒ á‰µáˆá‰… á‹áŒ¤á‰µ á‹«áˆµáŒˆáŠ›áˆ \" áˆ²áˆ‰ á‰°áŠ“áŒáˆ¨á‹‹áˆá¢\n",
      "\n",
      "\" áˆ…á‹á‰¡ á‰ á‰µá‹•áŒáˆµá‰µ áŠ¥áŠ•á‹²áŒ á‰¥á‰… áŒ¥áˆªá‹¬áŠ• áŠ á‰€áˆ­á‰£áˆˆáˆ \" á‹«áˆ‰á‰µ áˆ›áŠ”áŒ‚áŠ•áŒ á‹³á‹­áˆ¬áŠ­á‰°áˆ¯ \" áˆ…á‰¥áˆ¨á‰°áˆ°á‰¡ áŠ¨áˆªáŽáˆ­áˆ™ áŒ€áˆ­á‰£ á‰ áˆ˜áˆ°á‰£áˆ°á‰¥ á‰ áŠ áŠ•á‹µáŠá‰µ á‹µáŒ‹á áˆ›á‹µáˆ¨áŒ áŠ áˆˆá‰ á‰µ \" á‰¥áˆˆá‹‹áˆá¢\n",
      "\n",
      "áŒ†áˆ­áŒ‚á‹¬á‰« á¥ áŠ¢áŠ®áŠ–áˆšá‹áŠ• á‹¨á‰ áˆˆáŒ  áŠ áŒ¥áŒ‹á‰¢áŠ“ á‰¥á‰ áˆˆáˆ›á‹µáˆ¨áŒ á‰¥á‹™ á‹¨áˆšáˆ áˆ« áˆ¥áˆ« áŠ áˆˆ \" á‰¥áˆˆá‹ \" áŠ¥á‰£áŠ«á‰½áˆ áˆ˜áŠ•áŒáˆ¥á‰µ áˆ¥áˆ«á‹áŠ• áŠ¥áŠ•á‹²á‹«áŒ áŠ“á‰…á‰… á‹µáŒ‹á áŠ á‹µáˆ­áŒ‰ \" á‹¨áˆšáˆ áŒ¥áˆª áŠ á‰…áˆ­á‰ á‹‹áˆá¢\n",
      "\n",
      "á‹¨á‹‹áŒ‹ áŠ•áˆ¨á‰µáŠ• áˆˆáˆ˜áá‰³á‰µ á‹¨áˆšáˆ°áˆ«á‹ áˆµáˆ« á‹áˆµá‰¥áˆµá‰¥ áˆ˜áˆ†áŠ‘áŠ• á‹«áˆáˆ¸áˆ¸áŒ‰á‰µ á‹³á‹­áˆ¬áŠ­á‰°áˆ« \" á‹¨á‹‹áŒ‹ áŠ•áˆ¨á‰µáŠ• á‹ˆá‹° á‰³á‰½ áˆˆáˆ›á‹áˆ¨á‹µ áŒ áŠ•áŠ«áˆ« á‹¨áŒˆáŠ•á‹˜á‰¥áŠ“ á‹¨áŠáˆµáŠ«áˆ á–áˆŠáˆ²á‹Žá‰½á£ á‹¨áŠ¢áŠ®áŠ–áˆšá‹áŠ• á‹¨áˆ›áˆáˆ¨á‰µ áŠ á‰…áˆ áˆ›áˆµá‹á‰µá£ á‹¨á‹ˆáŒª áŠ•áŒá‹µáŠ“ á‹¨á‹áŒ­ áˆáŠ•á‹›áˆª áŒˆá‰¢áŠ• áˆ›áˆ³á‹°áŒ áŠ¥áŠ“ á‹¨áŒáˆ‰ áˆ´áŠ­á‰°áˆ­áŠ• áˆ›á‰¥á‰ƒá‰µ á‹­áŒ á‹­á‰ƒáˆ \" á‰¥áˆˆá‹‹áˆá¢\n",
      "\n",
      "áˆŒáˆ‹á‹ á‹«áŠáˆ±á‰µ áŒ‰á‹³á‹­ á‰ G20 á‹¨áŒ‹áˆ« áˆ›á‹•á‰€á áŠ¢á‰µá‹®áŒµá‹« áŠ¥á‹«áŠ«áˆ„á‹°á‰½ á‹«áˆˆá‰½á‹áŠ• á‹¨á‹•á‹³ áˆ˜áˆáˆ¶ áˆ›á‹°áˆ«áŒ€á‰µ á‹µáˆ­á‹µáˆ­ á‰ á‰°áˆ˜áˆˆáŠ¨á‰° áŠá‹á¢\n",
      "\n",
      "áŒ†áˆ­áŒ‚á‹¬á‰« á¤ \" á‹¨á‹•á‹³ áˆ˜áˆáˆ¶ áˆ›á‹‹á‰€áˆ­ áˆ‚á‹°á‰µ á‹¨áˆ˜áŒ¨áˆ¨áˆ» á‹°áˆ¨áŒƒ áˆ‹á‹­ á‹­áŒˆáŠ›áˆ á¤ áŠ¨áŠ¢á‰µá‹®áŒµá‹« áŠ á‰ á‹³áˆªá‹Žá‰½ áŒ‹áˆ­ á‰£áˆˆáŠ áŒáŠ•áŠ™áŠá‰µ á‹­áˆ… á‰…á‹µáˆšá‹« á‹¨áˆšáˆ°áŒ á‹ áŒ‰á‹³á‹­ áŠá‹ \" áˆ²áˆ‰ áŒˆáˆáŒ¸á‹‹áˆá¢ \n",
      "\n",
      "á‹¨IMF á•áˆ®áŒáˆ«áˆ áŠ áŠ«áˆ áˆ†áŠá‹áŠ• á‹¨á‰³áŠ­áˆµ áŠ¥áˆ­áˆáŒƒá‹Žá‰½áŠ• á‰ á‰°áˆ˜áˆˆáŠ¨á‰°áˆ á¤ á‹¨áŠ¢á‰µá‹®áŒµá‹« á‰£áˆˆáˆµáˆáŒ£áŠ“á‰µ áˆˆá‰¥áˆ„áˆ«á‹Š á‰ áŒ€á‰± á‹µáŒ‹á áˆˆáˆ›á‹µáˆ¨áŒ á‹ˆáˆ³áŠ á‹¨áˆ†áŠ‘ á‹¨á‰³áŠ­áˆµ áŠ á‰…áˆžá‰½áŠ• áˆ˜áˆˆá‹¨á‰³á‰¸á‹áŠ• áŒ á‰áˆ˜á‹‹áˆá¢ \n",
      "\n",
      "áŒ†áˆ­áŒ‚á‹¬á‰« á¥ á‹¨áŠ¢á‰µá‹®áŒµá‹« áŠ áŒ á‰ƒáˆ‹á‹­ á‹¨áˆ€áŒˆáˆ­ á‹áˆµáŒ¥ áˆáˆ­á‰µ á‹•á‹µáŒˆá‰µ áŠ¨IMF á‹¨áˆ˜áŒ€áˆ˜áˆªá‹« á‰µáŠ•á‰ á‹«á‹Žá‰½ áˆ˜á‰¥áˆˆáŒ¡áŠ• áˆ›á‰¥áˆ«áˆ«á‰³á‰¸á‹áŠ• á‹˜áˆªá–áˆ­á‰°áˆ­ áŠ áˆµáŠá‰¥á‰§áˆá¢\n",
      "\n",
      "á‹¨áˆ›áŠ”áŒ‚áŠ•áŒ á‹³á‹­áˆ¬áŠ­á‰°áˆ« áŠ•áŒáŒáˆ­ á‰°áŠ¨á‰µáˆŽ \" áˆ˜áˆ¬á‰µ áˆ‹á‹­ áŠ«áˆˆá‹ áŠ¥á‹áŠá‰³ áŒ‹áˆ­ á‹¨áˆšáŒˆáŠ“áŠ áŠ á‹­á‹°áˆˆáˆ \" á‹¨áˆšáˆ‰ áŠ áˆµá‰°á‹«á‹¨á‰¶á‰½ áˆ²áˆ°áŒ¡áˆ á‰°áˆ˜áˆáŠ­á‰°áŠ“áˆá¢\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "áˆá‰³áˆµáˆ˜áˆ­á‰€á‹‰ áŠ¨á‰¤á‰°áˆ°á‰¦á‰¿ á‰°á‹°á‰¥á‰ƒ á‹¨áˆ˜áŒ£á‰½ áŠ¥áŒ®áŠ›á‹‰áŠ• áŒ­áŠ«áŠ” á‰ á‰°áˆžáˆ‹á‰ á‰µ áˆáŠ”á‰³ á‹¨áŒˆá‹°áˆˆá‹‰ áŒáˆˆáˆ°á‰¥ á‰ 20 á‹“áˆ˜á‰µ áŒ½áŠ‘ áŠ¥áˆµáˆ«á‰µ á‰°á‰€áŒ£á¢\n",
      "\n",
      "á‰ á‹°á‰¡á‰¥ áŠ¢á‰µá‹®áŒµá‹« áŠ­áˆáˆ áŒ‹áˆž á‹žáŠ• áŠ áˆ­á‰£áˆáŠ•áŒ­ áŠ¨á‰°áˆ› áŠ áˆµá‰°á‹³á‹°áˆ­ áŒ‰áˆ­á‰£ á‰ áˆšá‰£áˆ á‰€á‰ áˆŒ á‹¨áŒˆá‹› áŠ¥áŒ®áŠ›á‹‰áŠ• áŒ­áŠ«áŠ” á‰ á‰°áˆžáˆ‹á‰ á‰µ áˆ˜áˆáŠ© á‰ áŠ áˆ°á‰ƒá‰‚ áˆáŠ”á‰³ á‰ áˆµáˆˆá‰µ áŠ áŠ•áŒˆá‰·áŠ• á‰ áˆ˜á‰áˆ¨áŒ¥ áˆ•á‹­á‹ˆá‰· áŠ¥áŠ•á‹²á‹«áˆá á‹«á‹°áˆ¨áŒˆá‹‰ á‹ˆáŒ£á‰µ á‰ á…áŠ‘ áŠ¥áˆµáˆ«á‰µ áˆ˜á‰€áŒ£á‰±áŠ• á‹¨áŠ áˆ­á‰£ áˆáŠ•áŒ­ áŠ¨á‰°áˆ› áŠ áˆµá‰°á‹³á‹°áˆ­ á–áˆŠáˆµ áˆ˜áˆáˆªá‹« áŠ á‹›á‹¥ áˆ/áŠ¢áŠ•áˆµá”áŠ­á‰°áˆ­ áŒ‹á‹áˆ® á‰¶áˆ›áˆµ áˆˆá‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« á‰°áŠ“áŒáˆ¨á‹‹áˆá¢\n",
      "\n",
      "á‹¨á‹ˆáŠ•áŒ€áˆ á‹µáˆ­áŒŠá‰± á‹¨á‰°áˆá€áˆ˜á‹‰ áŠáˆáˆ´ 16/2016 á‹“/áˆ á‰ áŠ áˆ­á‰£áˆáŠ•áŒ­ áŠ¨á‰°áˆ› áŒ‰áˆ­á‰£ á‰€á‰ áˆŒ áŠá‹á¢\n",
      "\n",
      "á‰°áŠ¨áˆ³áˆ½ á‹®áŠ“áˆµ áŒ«áŠá‰„ á‹¨á‰°á‰£áˆˆá‹ áŒáˆˆáˆ°á‰¥ á‹¨áŒ‚áŠ•áŠ« á‹©áŠ’á‰¨áˆ­áˆ²á‰² 1áŠ›Â  á‹“áˆ˜á‰µ á‰°áˆ›áˆª á‹¨áˆ†áŠá‰½á‹‰áŠ• áˆŸá‰½ áˆŠá‹²á‹« á‹®áˆáŠ•áˆµ áŠ¥áˆ±áŠ• áˆˆáˆ›áˆµáˆ˜áˆ¨á‰… á‹ˆá‹° áŠ áˆ­á‰£ áˆáŠ•áŒ­ áŠ¨á‰°áˆ› á‰ áˆ˜áŒ£á‰½á‰ á‰µ á‰°áŠ¨áˆ«á‹­á‰¶ á‰ áˆšáŠ–áˆ­á‰ á‰µ á‰¤á‰µ áŠ áˆ°á‰ƒá‰‚ á‹µáˆ­áŒŠá‰±áŠ• áˆ˜áˆá€áˆ™áŠ• á‹¨áˆáˆ­áˆ˜áˆ« áˆ˜á‹áŒˆá‰¡ á‹«áˆµáˆ¨á‹³áˆá¢\n",
      "\n",
      "á‹ˆáŒ£á‰· \" áŠ¥áŒ®áŠ›á‹¬ á‹­áˆ˜áˆ¨á‰…áˆáŠ›áˆ \" á‰ áˆšáˆ á‹°áˆµá‰³ áŠ¨á‰¤á‰°áˆ°á‰¦á‰¿ á‰°á‹°á‰¥á‰ƒ á‰°áŠ¨áˆ³áˆ½ á‰°áŠ¨áˆ«á‹­á‰¶ á‹ˆá‹° áˆšáˆ›áˆ­á‰ á‰µÂ  á‰¤á‰µ áˆ˜áŒ¥á‰³ á‰ á‹‹á‹œáˆ›á‹‰ áˆˆáˆáˆ¨á‰ƒá‹‰ á‹¨áˆšáˆ†áŠ‘ á‹¨á‹²áŠ®áˆ­á£ á‹¨á‹³á‰¦áŠ“ áˆˆáˆµáˆ‹áˆ³ áˆ˜áŒ áŒ¦á‰½áŠ“ á‰ á‰¡áŠ“ á‹áŒáŒ…á‰µ á‰¤á‰±áŠ• áŠ áˆ°áˆ›áˆáˆ« á‰ áˆáˆ½á‰±áˆ áŒá‰¢ á‹‰áˆµáŒ¥ á‹«áˆ‰ á‰°áŠ¨áˆ«á‹®á‰½áŠ• áŒ áˆ­á‰°á‹‰ áŠ¨áˆ¸áŠ™ á‰ áŠ‹áˆ‹ áˆŸá‰½ áˆ€áŒˆáˆ­ áˆ°áˆ‹áˆ á‰¥áˆ‹ á‰ á‰°áŠ›á‰½á‰ á‰µ áŠ¨áˆŒáˆŠá‰± 7 áˆ°á‹“á‰µ áŒˆá‹°áˆ› áŠ¥áˆ«áˆ·áŠ• áˆ˜áŠ¨áˆ‹áŠ¨áˆ á‰ áˆ›á‰µá‰½áˆá‰ á‰µ áˆáŠ”á‰³ á‰ á‰¢áˆ‹á‹‹ áŠ áŠ•áŒˆá‰·áŠ• áŠ áˆ­á‹¶ áˆ˜áŒá‹°áˆ‰áŠ• á‹¨áˆáˆ­áˆ˜áˆ« áˆ˜á‹áŒˆá‰¡áŠ• á‹‹á‰¢ áŠ á‹µáˆ­áŒˆá‹ á–áˆŠáˆµ áŠ á‹›á‹¡ áŒˆáˆá€á‹‹áˆá¢\n",
      "\n",
      "á–áˆŠáˆµ áŠ á‹›á‹¡ áŠ áŠ­áˆˆá‹ áŠ¥áŠ•á‹°áŒˆáˆˆáŒ¹á‰µ á¥ á‰ á‹ˆá‰…á‰± á‰ á‰°á‹°áˆ¨áŒˆá‹‰ áˆ›áŒ£áˆ«á‰µáˆ áˆ†áŠ á‰ áŠ­áˆµ áˆ˜á‹áŒˆá‰¡ áˆ‹á‹­ áŠ¥áŠ•á‹°áˆ°áˆáˆ¨á‹‰ á‹ˆáŠ•áŒ€áˆˆáŠ›á‹  \" á‹ˆá‹° á‹©áŠ’á‰¨áˆ­áˆ²á‰² á‰ áˆ„á‹µáˆ½á‰ á‰µ áˆŒáˆ‹ á‹¨á‹ˆáŠ•á‹µ áŒ“á‹°áŠ› á‹­á‹˜áˆ»áˆ \" á‰ áˆšáˆ áŠá‹ á‰ áˆ­ á‹˜áŒá‰¶ áŠ áˆ°á‰ƒá‰‚ á‹¨á‹ˆáŠ•áŒ€áˆ á‹µáˆ­áŒŠá‰±áŠ• á‹¨áˆáŒ¸áˆ˜á‹á¢\n",
      "\n",
      "á–áˆŠáˆµ á‰ á‹šáˆ… á‹˜áŒáŠ“áŠ á‹ˆáŠ•áŒ€áˆ á‹™áˆªá‹« á‰°áŒˆá‰¢á‹‰áŠ• áˆ›áŒ£áˆ«á‰µáŠ“ áˆáˆ­áˆ˜áˆ« áŠ á‹µáˆ­áŒŽ áˆˆá‹á‰ƒá‰¤ áˆ•áŒ áˆ›á‰…áˆ¨á‰¡áŠ• áŠ áˆµá‰³á‹á‰€á‹‹áˆá¢\n",
      "\n",
      "á‹á‰ƒá‰¤ áˆ•áŒáˆ áŠ­áˆµ á‰ áˆ˜áˆ˜áˆµáˆ¨á‰µ áˆˆááˆ­á‹µ á‰¤á‰± á‰°áŠ¨áˆ³áˆ½ á‹¨á‹ˆáŠ•áŒ€áˆ á‹µáˆ­áŒŠá‰±áŠ• áˆ˜áˆá€áˆ™áŠ• á‹¨áˆ°á‹‰á£ á‹¨áˆ°áŠá‹µáŠ“ á‹¨áˆ…áŠ­áˆáŠ“ áˆ›áˆµáˆ¨áŒƒá‹Žá‰½áŠ• á‰ áˆ›á‰…áˆ¨á‰¥ áŠ áˆµáˆ¨á‹µá‰·áˆá¢\n",
      "\n",
      "á‰ á‰€áˆ¨á‰¡ áˆ›áˆµáˆ¨áŒƒá‹Žá‰½ áŠ¥áŠ“ áˆáˆµáŠ­áˆ®á‰½ áŒáˆ« á‰€áŠ™áŠ• áˆ²á‹«áŒ£áˆ« á‹¨á‰†á‹¨á‹‰ á‹¨áŒ‹áˆž á‹žáŠ• áŠ¨áá‰°áŠ›á‹‰ ááˆ­á‹µ á‰¤á‰µ á‰ á‰€áŠ• 29/5/2017 á‹“áˆ á‰ á‹‹áˆˆá‹‰ á‰½áˆŽá‰µ á‰°áŠ¨áˆ³áˆ½ á‹®áŠ“áˆµ áŒ¨áŠá‰„ á‰ á‰°áŠ¨áˆ°áˆ°á‰ á‰µ á‰ áŠ áˆ°á‰ƒá‰‚ áˆáŠ”á‰³ áŠá‰¥áˆµ á‹¨áˆ›áŒ¥á‹á‰µ á‹ˆáŠ•áŒ€áˆ áŒ¥á‹á‰°áŠ› áˆ˜áˆ†áŠ‘áŠ• á‰ áˆ›áˆ¨áŒ‹áŒˆáŒ¥ á‰ 20 á‹“áˆ˜á‰µ á…áŠ‘ áŠ¥áˆµáˆ«á‰µ áŠ¥áŠ•á‹²á‰€áŒ£ áˆ˜á‹ˆáˆ°áŠ‘áŠ•áˆ áŠ¢áŠ•áˆµá”áŠ­á‰°áˆ­ áŒ‹á‹áˆ® á‰¶áˆ›áˆµ áˆˆá‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« á‰°áŠ“áŒáˆ¨á‹‹áˆá¢\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "#áˆ˜á‰„á‹¶áŠ•á‹«\n",
      "\n",
      "\" áˆ°á‹áŠ• áˆˆáˆ˜áˆ­á‹³á‰µ áˆ°á‹ áˆ˜áˆ†áŠ• á‰ á‰‚ áŠá‹ !! \"\n",
      "\n",
      "áˆ˜á‰„á‹¶áŠ•á‹« á‹¨áŠ áˆ¨áŒ‹á‹Šá‹«áŠ•áŠ“ á‹¨áŠ áŠ¥áˆáˆ® áˆ…áˆ™áˆ›áŠ• áˆ˜áˆ­áŒƒ áˆ›á‹•áŠ¨áˆ á‰ áˆšá‹«áˆµáŒˆáŠá‰£á‹ áˆ†áˆµá’á‰³áˆ áŒ­áˆáˆ­ á‹«áˆˆá‹ áˆ…áŠ•áƒ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹¨áŒˆáŠ•á‹˜á‰¥ áŠ¥áŒ¥áˆ¨á‰µ áˆµáˆˆáŒˆáŒ áˆ˜á‹ áŠ¥áŒá‹› áŠ¥áŠ•á‹²á‹°áˆ¨áŒ áˆˆáˆ˜áˆ‹á‹ áŠ¢á‰µá‹®áŒµá‹«á‹á‹«áŠ• áŒ¥áˆª áˆ˜á‰…áˆ¨á‰¡ á‹­á‰³á‹ˆáˆ³áˆá¢\n",
      "\n",
      "á‹›áˆ¬ â€œ á‰ áˆ°á‹­á‰ áŠ¢á‰¢áŠ¤áˆµ á‹¨á‹©á‰±á‰¥ á‰»áŠáˆ â€ á‹µáŒ‹á áˆ›á‹µáˆ¨áŒŠá‹« áˆ˜áˆ­áˆ€ áŒá‰¥áˆ­ áŠ¥á‹¨á‰°áŠ«áˆ„á‹° áŠá‹á¢\n",
      "\n",
      "á‹°áŒ‹áŒŽá‰½ áˆáˆ‰ á‹µáŒ‹á áŠ¥áŠ•á‹µá‰³á‹°áˆ­áŒ‰ áŒ¥áˆª áŠ¥áŠ“á‰€áˆ­á‰£áˆˆáŠ•á¢\n",
      "\n",
      "á‹¨á‹µáŒ‹á áˆ›á‹µáˆ¨áŒŠá‹« áŠ áˆ›áˆ«áŒ®á‰½ áŠ¨áˆ‹á‹­ á‰°á‹«á‹­á‹˜á‹‹áˆá¢\n",
      "\n",
      "áˆ˜á‰„á‹¶áŠ•á‹« â€œ áˆ…áŠ•áƒá‹ áˆˆáˆ›áŒ áŠ“á‰€á‰… áŒˆáŠ•á‹˜á‰¥ á‰°á‰¸áŒáˆ¨áŠ“áˆá¢ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹ˆá‹° 5 á‰¢áˆŠá‹®áŠ• á‰¥áˆ­ á‹«áˆµáˆáˆáŒˆáŠ“áˆ â€ áˆ›áˆˆá‰± áŠ á‹­á‹˜áŠáŒ‹áˆá¢\n",
      "\n",
      "á‰ á‰€áŒ¥á‰³ á‹­áŠ¨á‰³á‰°áˆ‰ ðŸ‘‡\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample=load_data(\"datas/sample.json\")\n",
    "for news in sample[:5]:\n",
    "    print(news)\n",
    "\n",
    "    print(\"---------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kjKE3-ZDnqp"
   },
   "source": [
    "Since we are working with a Telegram dataset, we aim to clean the text by removing substrings that are commonly used on the platform, such as hashtagged entities, usernames, hyperlinks,emojis, and english words. To achieve this, we will use Python's re library to perform regular expression operations. We will define specific search patterns and use the sub() method to remove matches by replacing them with an empty string (''). We will also remove unecessary multiple spaces to a single space\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1740393248207,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "aLRI3SvnDnqp"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    text=re.sub(r'@\\S+', '', text)\n",
    "    text=emoji.replace_emoji(text,\" \")\n",
    "    english_pattern = re.compile(r'\\b[A-Za-z]+\\b')\n",
    "    cleaned_text = re.sub(english_pattern, '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRFOcCgODnqp"
   },
   "source": [
    "\n",
    "Let's test the above function on sample data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5xmnWErDnqp"
   },
   "source": [
    "let's load our training data and see how many contents we have and what the first 5 contents look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9611,
     "status": "ok",
     "timestamp": 1740393258515,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "RF6OeM3kDnqp",
    "outputId": "a743aaf9-575c-4ead-b103-914c4692aad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of contents: 193419\n",
      "\n",
      "First 5 contents: \n",
      "\n",
      "#áˆ˜á‰„á‹¶áŠ•á‹«\n",
      "\n",
      "áˆ°á‹áŠ• áˆˆáˆ˜áˆ­á‹³á‰µ áˆ°á‹ áˆ˜áˆ†áŠ• á‰ á‰‚ áŠá‹ !\n",
      "\n",
      "á‰µáˆ‹áŠ•á‰µ á‹¨áŠ«á‰²á‰µ 1/2017 á‹“/áˆ á‰ áŒ€áˆ˜áˆ¨á‹ á‹¨áˆ˜á‰„á‹¶áŠ•á‹« á‹¨áŠ áˆ¨áŒ‹á‹Šá‹«áŠ• áŠ¥áŠ“ á‹¨áŠ áŠ¥áˆáˆ® áˆ…áˆ™áˆ›áŠ• áˆ˜áˆ­áŒƒ áˆ›á‹•áŠ¨áˆ á‹¨á‹µáŒ‹á áˆ›áˆ°á‰£áˆ°á‰¥ á‹˜áˆ˜á‰» áŠ¥áˆµáŠ©áŠ• 120,000,000 á‰¥áˆ­ á‰°áˆ°á‰¥áˆµá‰§áˆá¢\n",
      "\n",
      "áˆ˜á‰„á‹¶áŠ•á‹« á‰ áˆšá‹«áˆµáŒˆáŠá‰£á‹ áˆ†áˆµá’á‰³áˆ áŒ­áˆáˆ­ á‹«áˆˆá‹ áˆ…áŠ•áƒ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹¨áŒˆáŠ•á‹˜á‰¥ áŠ¥áŒ¥áˆ¨á‰µ áŠ áŒ‹áŒ¥áˆžá‰³áˆá¢ áˆ…áŠ•áƒá‹ áˆˆáˆ›áŒ áŠ“á‰€á‰… áŒˆáŠ•á‹˜á‰¥ á‰°á‰¸áŒáˆ¨áŠ“áˆá¢ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹ˆá‹° 5 á‰¢áˆŠá‹®áŠ• á‰¥áˆ­ á‹«áˆµáˆáˆáŒ‹áˆá¢\n",
      "\n",
      "á‰ á‰€áŒ¥á‰³ á‹­áŠ¨á‰³á‰°áˆ‰ ðŸ‘‡\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "á‹¨áˆá‰µá‰½áˆ‰á‰µáŠ• áˆáˆ‰ á‹µáŒ‹á áŠ á‹µáˆ­áŒ‰á¢\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ”Š #á‹¨áˆ áˆ«á‰°áŠžá‰½á‹µáˆáŒ½\n",
      "\n",
      "\" á‰‹áˆš áˆ áˆ«á‰°áŠžá‰½ áˆ†áŠáŠ• áˆ³áˆˆ á‰ á‹°áˆžá‹ áˆ›áˆ»áˆ»á‹«á‹ áŠ áˆá‰°áŠ«á‰°á‰µáŠ•áˆ \" - á‹¨áˆ€á‹‹áˆ³ á‹™áˆªá‹« á‹ˆáˆ¨á‹³ áˆ˜áŠ•áŒáˆµá‰µ áˆ áˆ«á‰°áŠžá‰½\n",
      "\n",
      "á‹¨áˆ›áŠ­áˆ® áŠ¢áŠ®áŠ–áˆš áˆ›áˆ»áˆ»á‹« áˆªáŽáˆ­áˆ™áŠ• á‰°áŠ¨á‰µáˆŽ á‹¨áˆšáŠ¨áˆ°á‰± á‹¨áŠ‘áˆ® á‹‰á‹µáŠá‰µáŠ“ á‰°á‹«á‹«á‹¥ áŒ‰á‹³á‹®á‰½áŠ• á‰³áˆ³á‰¢ á‰ áˆ›á‹µáˆ¨áŒ á‹¨áˆ˜áŠ•áŒáˆµá‰µ áˆ áˆ«á‰°áŠžá‰½ á‹°áˆžá‹ áˆ›áˆ»áˆ»á‹« á‰°á‹°áˆ­áŒŽ áŠ¨áŒ¥á‰…áˆá‰µ á‹ˆáˆ­ 2017 á‹“/áˆ áŒ€áˆáˆ® á‰°áŒá‰£áˆ«á‹Š á‹¨á‰°á‹°áˆ¨áŒˆ áˆ˜áˆ†áŠ‘ á‹­á‰³á‹ˆá‰ƒáˆá¢\n",
      "\n",
      "á‰ áˆ²á‹³áˆ› áŠ­áˆáˆá¤ áˆ°áˆœáŠ“á‹Š áˆ²á‹³áˆ› á‹žáŠ•á¤ áˆ€á‹‹áˆ³ á‹™áˆªá‹« á‹ˆáˆ¨á‹³ á‰ á‰°áˆˆá‹«á‹© á‹¨áˆ˜áŠ•áŒáˆµá‰µ áˆ˜áˆµáˆªá‹« á‰¤á‰¶á‰½ á‹¨áˆšáˆ°áˆ© á‹¨áˆ˜áŠ•áŒáˆµá‰µ áˆ áˆ«á‰°áŠžá‰½ áŒáŠ• \" áŠ¨2012 á‹“/áˆ áŒ€áˆáˆ® á‰ á‰‹áˆšáŠá‰µ á‰°á‰€áŒ¥áˆ¨áŠ• áŠ¥á‹¨áˆ°áˆ«áŠ• á‹«áˆˆáŠ• á‰¢áˆ†áŠ•áˆ á‰ áŠ á‹²áˆ± á‹¨áˆ˜áŠ•áŒáˆµá‰µ áˆ áˆ«á‰°áŠžá‰½ á‹¨á‹°áˆ˜á‹ˆá‹ áˆ›áˆ»áˆ»á‹« áŠ áˆá‰°áŠ«á‰°á‰µáŠ•áˆ \" áˆ²áˆ‰ á‰…áˆ¬á‰³á‰¸á‹‰áŠ• áˆˆá‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« áŠ áˆµáŒˆá‰¥á‰°á‹‹áˆá¢\n",
      "\n",
      "á‰…áˆ¬á‰³á‰¸á‹‰áŠ• áŠ«á‹°áˆ¨áˆ±áŠ• áˆ˜áŠ«áŠ¨áˆ á¦\n",
      "- á‰ áŠ¨á‰°áˆ› áˆáˆ›á‰µáŠ“ áŠ®áŠ•áˆµá‰µáˆ«áŠ­áˆ½áŠ•á£\n",
      "- áˆ›á‹˜áŒ‹áŒƒ á‰¤á‰¶á‰½á£\n",
      "- á‰ á‰µáˆáˆ…áˆ­á‰µ á‹˜áˆ­á á£\n",
      "- á‰ áˆ´á‰¶á‰½áŠ“ áˆ•áƒáŠ“á‰µ áŠ¥áŠ•á‹²áˆáˆ á‰ áˆ•á‰¥áˆ¨á‰µ áˆµáˆ« áŒ½/á‰¤á‰¶á‰½ á‹¨áˆšáˆ°áˆ© áˆ áˆ«á‰°áŠžá‰½ áŠ“á‰¸á‹á¢\n",
      "\n",
      "\" á‰ á‹ˆá‰…á‰± á‰ áŠ áŒá‰£á‰¡ áˆ›áˆµá‰³á‹ˆá‰‚á‹« á‹ˆáŒ¥á‰¶ á‰°áˆ˜á‹áŒá‰ áŠ•áŠ“ á‰°á‹ˆá‹³á‹µáˆ¨áŠ• áˆ›áˆˆá‹á‰½áŠ• á‰°áˆ¨áŒ‹áŒáŒ¦ á‹¨á‰‹áˆšáŠá‰µ á‹°á‰¥á‹³á‰¤ á‰°áˆ°áŒ¥á‰¶áŠ• áˆ‹áˆˆá‰á‰µ áŠ áˆáˆµá‰µáŠ“ áˆµá‹µáˆµá‰µ á‹“áˆ˜á‰³á‰µ á‹°áˆžá‹ áˆ²áŠ¨áˆáˆˆáŠ• á‰ á‰†á‹¨áŠ•á‰£á‰¸á‹ áˆ˜á‹°á‰¦á‰½ áˆ‹á‹­ áŠ¥á‹¨áˆ°áˆ«áŠ• á‰£áˆˆáŠ•á‰ á‰µ á‰ áŠ á‹²áˆ± á‹¨á‹°áˆžá‹ áˆ›áˆ»áˆ»á‹« áŠ áˆˆáˆ˜áŠ«á‰°á‰³á‰½áŠ• áˆˆá‹˜áˆ­áˆ á‰¥á‹™ á‰½áŒáˆ®á‰½ á‹³áˆ­áŒŽáŠ“áˆ \" á‰¥áˆˆá‹‹áˆá¢\n",
      "\n",
      "\" áˆˆá‹ˆáˆ¨á‹³á‹‰ áá‰¥áˆŠáŠ­ áˆ°áˆ­á‰ªáˆµáŠ“ á‹¨áˆ°á‹‰ áˆƒá‰¥á‰µ áˆáˆ›á‰µ áŒ½/á‰¤á‰µ áŠ¥áŠ“ áˆˆáŠ­áˆáˆ‰ áá‰¥áˆŠáŠ­ áˆ°áˆ­á‰ªáˆµ á‰¢áˆ® á‰…áˆ¬á‰³á‰½áŠ•áŠ• á‰ áŠ áŠ«áˆáŠ“ á‰ á…áˆá á‰¥áŠ“á‰€áˆ­á‰¥áˆ á‰°áŒˆá‰¢á‹‰ áˆáˆ‹áˆ½ áŠ áˆá‰°áˆ°áŒ áŠ•áˆ áŒ‰á‹³á‹©áŠ• áˆˆáŠ¢á‰µá‹®áŒµá‹« áŠ¥áˆá‰£ áŒ á‰£á‰‚ á‰°á‰‹áˆ áˆˆáˆ›á‰…áˆ¨á‰¥ áˆ˜áˆ¨áŒƒ áŠ¥á‹«á‹°áˆ«áŒ€áŠ• áŠá‹ \" áˆ²áˆ‰ á‰°áŠ“áŒáˆ¨á‹‹áˆá¢\n",
      "\n",
      "á‰ƒáˆ‹á‰¸á‹áŠ• áˆˆá‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« á‹¨áˆ°áŒ¡á‰µ á¤ á‹¨áˆ€á‹‹áˆ³ á‹™áˆªá‹« á‹ˆáˆ¨á‹³ áá‰¥áˆŠáŠ­ áˆ°áˆ­á‰ªáˆµ áŠ¥áŠ“ á‹¨áˆ°á‹‰ áˆƒá‰¥á‰µ áˆáˆ›á‰µ áŒ½/á‰¤á‰µ áŠƒáˆ‹áŠ áŠ á‰¶ áˆƒá‹­áˆ‰ áŠ á‰¢áŠ– á¥ \" á‰ á‹ˆáˆ¨á‹³á‹‰ á‰ 2012 á‹“/áˆ á‹¨áŠá‰ áˆ¨á‹ áŠ áŒá‰£á‰¥áŠá‰µ á‰ áˆŒáˆˆá‹ á‰…áŒ¥áˆ­ á‰ áŠ áŠ•á‹µ áˆ˜á‹°á‰¥ áˆ¶áˆµá‰µáŠ“ áŠ áˆ«á‰µ áˆ°á‹Žá‰½áŠ• á‰ á‰°á‹°áˆ«áˆ«á‰¢áŠá‰µ á‹¨áˆ˜á‰…áŒ áˆ­ áˆáŠ”á‰³á‹Žá‰½ áŠ áˆáŠ• áˆˆá‰°áˆáŒ áˆ¨á‹ á‰½áŒáˆ­ á‹‹áŠáŠ› áˆáŠ­áŠ•á‹«á‰µ áˆ†áŠ—áˆ \" áˆ²áˆ‰ áŒˆáˆáŒ¸á‹‹áˆá¢\n",
      "\n",
      "áŠ¨á‹žáŠ‘áŠ“ á‹¨áŠ­áˆáˆ‰ áá‰¥áˆáŠ­ áˆ°áˆ­á‰ªáˆµ áŒ‹áˆ­ á‰ áˆ˜áŠ“á‰ á‰¥ áˆ˜áá‰µáˆ” áŠ¥á‹«áˆáˆ‹áˆˆáŒ‰ áˆµáˆˆáˆ˜áˆ†áŠ‘áˆ áŒ á‰áˆ˜á‹‹áˆá¢\n",
      "\n",
      "á‰ á‹ˆá‰…á‰± á‹­áˆ…áŠ• á‰°áŒá‰£áˆ­ á‹¨áˆá€áˆ™ áŠ áˆ˜áˆ«áˆ®á‰½ áŠ¥áŠ“ á‹¨áˆ°á‹‰ áˆƒá‰¥á‰µ áˆáˆ›á‰µ áŠƒáˆ‹áŠá‹Žá‰½ áˆ‹á‹­ áŠ¥áˆ­áˆáŒƒ áˆ˜á‹ˆáˆ°á‹±áŠ• á‹¨áˆšáŠ“áŒˆáˆ©á‰µ áŠƒáˆ‹áŠá‹‰ á‰ á‹ˆáˆ¨á‹³á‹‰ á‰ á‹šáˆ… áˆ˜áˆáŠ­ á‰°áŒ á‰€áŒ¥áˆ¨á‹‰ á‰ áŠ á‹²áˆ± á‹¨á‹°áˆ˜á‹ˆá‹ áˆ›áˆ»áˆ»á‹« á‹«áˆáŠ«á‰°á‰±áŠ“ á‰ á‰€áŒ£á‹­ áˆ˜áá‰µáˆ” á‹¨áˆšáˆáˆˆáŒáˆ‹á‰¸á‹‰ 470 á‰ á‰°áˆˆá‹«á‹© áˆ˜áˆµáˆªá‹« á‰¤á‰¶á‰½ á‹‰áˆµáŒ¥ á‹¨á‰°áˆˆá‹© áˆ°áˆ«á‰°áŠžá‰½ áˆµáˆˆáˆ˜áŠ–áˆ«á‰¸á‹‰ áŠ áŠ­áˆˆá‹‹áˆá¢\n",
      "\n",
      "á‹¨áˆ°áˆœáŠ“á‹Š áˆ²á‹³áˆ› á‹žáŠ• áá‰¥áˆáŠ­ áˆ°áˆ­á‰ªáˆµáŠ“ á‹¨áˆ°á‹‰ áˆƒá‹­áˆ áˆáˆ›á‰µ áˆ˜áˆáˆªá‹« áŠƒáˆ‹áŠ áŠ á‰¶ á‰ á‹›á‰¥áˆ… á‰£áˆ­áˆ¶ á‰ á‰ áŠ©áˆ‹á‰¸á‹ á‰ 2011 áŠ¥áŠ“ 2012 á‰ áŠ áŠ¨á‰£á‰¢á‹ áˆ•áŒˆá‹ˆáŒ¥ á‰…áŒ¥áˆ®á‰½ áˆ˜áˆá€áˆ›á‰¸á‹áŠ• áŒˆáˆáŒ¸á‹‹áˆá¢\n",
      "\n",
      "á‰ á‹ˆáˆ¨á‹³á‹‰ áŠ áŒ£áˆª á‰¡á‹µáŠ• á‰°á‰‹á‰áˆž á‰ áŠ á‹²áˆ± á‹°áˆžá‹ á‹«áˆá‰°áŠ«á‰°á‰±áŠ•áŠ“ á‰ á‹ˆáˆ¨á‹³á‹ á‰…áŒ¥áˆ­ á‹«áˆá‰°áˆá€áˆ˜á‰£á‰¸á‹‰ áŠ­áá‰µ áˆ˜á‹°á‰¦á‰½áŠ• á‹¨áˆ˜áˆˆá‹¨á‰µ áˆµáˆ« áˆ˜áŠ¨áŠ“á‹ˆáŠ‘áŠ• áŠ áŠ•áˆµá‰°á‹‰ á‰ áˆ€á‹‹áˆ³ á‹™áˆªá‹« á‹ˆáˆ¨á‹³ á‰¥á‰» 407 áŠ­áá‰µ áˆ˜á‹°á‰¦á‰½ áˆ˜áŠ–áˆ«á‰¸á‹‰áŠ• áˆˆáˆ›á‹ˆá‰… áˆ˜á‰»áˆ‰áŠ• áŒˆáˆá€á‹‹áˆá¢\n",
      "\n",
      "á‹¨áŠ­áˆáˆ‰ á‹¨á‰ áˆ‹á‹­ áŠ áˆ˜áˆ«áˆ®á‰½ á‰ áˆšá‹«áˆµá‰€áˆáŒ¡á‰µ áŠ á‰…áŒ£áŒ« áˆ˜áˆ°áˆ¨á‰µ áŠ¥áŠá‹šáˆ…áŠ• áˆ áˆ«á‰°áŠžá‰½ á‰ áŠá‹šáˆ… áŠ­áá‰µ áˆ˜á‹°á‰¦á‰½ á‹¨áˆ˜á‹°áˆá‹°áˆáŠ“ áˆŒáˆŽá‰½áˆ áˆ•áŒ‹á‹Š áŠ áˆ˜áˆ«áŒ®á‰½ á‰ áˆ˜áˆáˆˆáŒ á‰ áŠ áŒ­áˆ­ áŒŠá‹œ á‹‰áˆµáŒ¥ áŠ¥áˆá‰£á‰µ áˆˆáˆ˜áˆµáŒ á‰µ áŠ¥á‹¨á‰°áˆ°áˆ« áˆ˜áˆ†áŠ‘áŠ• áŠ áˆµá‰³á‹á‰€á‹‹áˆá¢\n",
      "\n",
      "á‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« áŒ‰á‹³á‹©áŠ• áŠ¥áˆµáŠ¨áˆ˜áŒ¨áˆ¨áˆ» á‰°áŠ¨á‰³á‰µáˆŽ áˆ˜áˆ¨áŒƒá‹áŠ• á‹­áˆáŠ«áˆá¢\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "á‹¨IMF áˆ›áŠ”áŒ‚áŠ•áŒ á‹³á‹­áˆ¬áŠ­á‰°áˆ¯ áˆáŠ• áŠ áˆ‰ ?\n",
      "\n",
      "á‹¨á‹“áˆˆáˆ áŠ á‰€á á‹¨áŒˆáŠ•á‹˜á‰¥ á‰°á‰‹áˆ (IMF) áˆ›áŠ”áŒ‚áŠ•áŒ á‹³á‹­áˆ¬áŠ­á‰°áˆ­ áŠ­áˆªáˆµá‰³áˆŠáŠ“ áŒ†áˆ­áŒ‚á‹¬á‰« á‰ áŠ¢á‰µá‹®áŒµá‹« á‹¨áˆµáˆ« á‰†á‹­á‰³ áŠ á‹µáˆ­áŒˆá‹‹áˆá¢\n",
      "\n",
      "á‰ á‹šáˆ…áˆ á‹ˆá‰…á‰µ áŠ¨áŒ /áˆš á‹á‰¢á‹­ áŠ áˆ…áˆ˜á‹µ (á‹¶/áˆ­) áŒ‹áˆ­ áŒ¨áˆáˆ® áŠ¨áŒá‹´áˆ«áˆ áŠ¨áá‰°áŠ› á‰£áˆˆáˆµáˆáŒ£áŠ“á‰µ áŒ‹áˆ­ áˆ˜áŠ­áˆ¨á‹‹áˆá¢\n",
      "\n",
      "á‹¨áŠá‰ áˆ«á‰¸á‹áŠ• á‰†á‹­á‰³ á‰ á‰°áˆ˜áˆˆáŠ¨á‰° áŠ¨áŒˆáŠ•á‹˜á‰¥ áˆšáŠ’áˆµá‰µáˆ© áŠ á‰¶ áŠ áˆ…áˆ˜á‹µ áˆ½á‹´ áŒ‹áˆ­ á‰ áŒ‹áˆ« áˆ˜áŒáˆˆáŒ« áˆ°áŒ¥á‰°á‹ áŠá‰ áˆ­á¢\n",
      "\n",
      "áˆáŠ• áŠ áˆ‰ ?\n",
      "\n",
      "á‹³á‹­áˆ¬áŠ­á‰°áˆ¯ á¤ \" á‹¨áŠ¢á‰µá‹®áŒµá‹« áˆªáŽáˆ­áˆ áŠ¨á‰£á‹µ áŠ¥áŠ“ áŒŠá‹œ á‹¨áˆšá‹ˆáˆµá‹µ áŠá‹ á¤ áŠ¥á‰£áŠ«á‰½áˆ á‰³áŒˆáˆ± \" á‹¨áˆšáˆ áŒ¥áˆª áŠ á‰…áˆ­á‰ á‹‹áˆá¢\n",
      "\n",
      "áŠ¢á‰µá‹®áŒµá‹«á‹á‹«áŠ• áˆˆá‰µá‹•áŒáˆµá‰µ áŠ¥áŠ•á‹²á‹«áˆ³á‹© áŠ¥áŠ“ áŠ¨áˆ˜áŠ•áŒáˆµá‰µ á‹¨áŠ¢áŠ®áŠ–áˆš áˆ›áˆ»áˆ»á‹« áŒ¥áˆ¨á‰¶á‰½ áŒŽáŠ• áŠ¥áŠ•á‹²á‰†áˆ™ áŒ á‹­á‰€á‹‹áˆá¢\n",
      "\n",
      "áŒ†áˆ­áŒ‚á‹¬á‰« á¥ \" á‹¨áˆªáŽáˆ­áˆ™áŠ• áŒá‰¦á‰½ áˆˆáˆ›áˆ³áŠ«á‰µ á‹¨áŠ áŠ•á‹µáŠá‰µ áŠ áˆµáˆáˆ‹áŒŠ áŠá‹ \" áˆ²áˆ‰ áŠ á…áŠ•áŠ¦á‰µ áˆ°áŒ¥á‰°á‹‹áˆá¢\n",
      "\n",
      "\" áŠ¢á‰µá‹®áŒµá‹« á‹¨á‰°á‰€á‰ áˆˆá‰½á‹ áˆªáŽáˆ­áˆ áŠ¨á‰£á‹µ áŠ¥áŠ“ áŒŠá‹œ á‹¨áˆšá‹ˆáˆµá‹µ á‰¢áˆ†áŠ•áˆ áŠ¥áŒ…áŒ á‰µáˆá‰… á‹áŒ¤á‰µ á‹«áˆµáŒˆáŠ›áˆ \" áˆ²áˆ‰ á‰°áŠ“áŒáˆ¨á‹‹áˆá¢\n",
      "\n",
      "\" áˆ…á‹á‰¡ á‰ á‰µá‹•áŒáˆµá‰µ áŠ¥áŠ•á‹²áŒ á‰¥á‰… áŒ¥áˆªá‹¬áŠ• áŠ á‰€áˆ­á‰£áˆˆáˆ \" á‹«áˆ‰á‰µ áˆ›áŠ”áŒ‚áŠ•áŒ á‹³á‹­áˆ¬áŠ­á‰°áˆ¯ \" áˆ…á‰¥áˆ¨á‰°áˆ°á‰¡ áŠ¨áˆªáŽáˆ­áˆ™ áŒ€áˆ­á‰£ á‰ áˆ˜áˆ°á‰£áˆ°á‰¥ á‰ áŠ áŠ•á‹µáŠá‰µ á‹µáŒ‹á áˆ›á‹µáˆ¨áŒ áŠ áˆˆá‰ á‰µ \" á‰¥áˆˆá‹‹áˆá¢\n",
      "\n",
      "áŒ†áˆ­áŒ‚á‹¬á‰« á¥ áŠ¢áŠ®áŠ–áˆšá‹áŠ• á‹¨á‰ áˆˆáŒ  áŠ áŒ¥áŒ‹á‰¢áŠ“ á‰¥á‰ áˆˆáˆ›á‹µáˆ¨áŒ á‰¥á‹™ á‹¨áˆšáˆ áˆ« áˆ¥áˆ« áŠ áˆˆ \" á‰¥áˆˆá‹ \" áŠ¥á‰£áŠ«á‰½áˆ áˆ˜áŠ•áŒáˆ¥á‰µ áˆ¥áˆ«á‹áŠ• áŠ¥áŠ•á‹²á‹«áŒ áŠ“á‰…á‰… á‹µáŒ‹á áŠ á‹µáˆ­áŒ‰ \" á‹¨áˆšáˆ áŒ¥áˆª áŠ á‰…áˆ­á‰ á‹‹áˆá¢\n",
      "\n",
      "á‹¨á‹‹áŒ‹ áŠ•áˆ¨á‰µáŠ• áˆˆáˆ˜áá‰³á‰µ á‹¨áˆšáˆ°áˆ«á‹ áˆµáˆ« á‹áˆµá‰¥áˆµá‰¥ áˆ˜áˆ†áŠ‘áŠ• á‹«áˆáˆ¸áˆ¸áŒ‰á‰µ á‹³á‹­áˆ¬áŠ­á‰°áˆ« \" á‹¨á‹‹áŒ‹ áŠ•áˆ¨á‰µáŠ• á‹ˆá‹° á‰³á‰½ áˆˆáˆ›á‹áˆ¨á‹µ áŒ áŠ•áŠ«áˆ« á‹¨áŒˆáŠ•á‹˜á‰¥áŠ“ á‹¨áŠáˆµáŠ«áˆ á–áˆŠáˆ²á‹Žá‰½á£ á‹¨áŠ¢áŠ®áŠ–áˆšá‹áŠ• á‹¨áˆ›áˆáˆ¨á‰µ áŠ á‰…áˆ áˆ›áˆµá‹á‰µá£ á‹¨á‹ˆáŒª áŠ•áŒá‹µáŠ“ á‹¨á‹áŒ­ áˆáŠ•á‹›áˆª áŒˆá‰¢áŠ• áˆ›áˆ³á‹°áŒ áŠ¥áŠ“ á‹¨áŒáˆ‰ áˆ´áŠ­á‰°áˆ­áŠ• áˆ›á‰¥á‰ƒá‰µ á‹­áŒ á‹­á‰ƒáˆ \" á‰¥áˆˆá‹‹áˆá¢\n",
      "\n",
      "áˆŒáˆ‹á‹ á‹«áŠáˆ±á‰µ áŒ‰á‹³á‹­ á‰ G20 á‹¨áŒ‹áˆ« áˆ›á‹•á‰€á áŠ¢á‰µá‹®áŒµá‹« áŠ¥á‹«áŠ«áˆ„á‹°á‰½ á‹«áˆˆá‰½á‹áŠ• á‹¨á‹•á‹³ áˆ˜áˆáˆ¶ áˆ›á‹°áˆ«áŒ€á‰µ á‹µáˆ­á‹µáˆ­ á‰ á‰°áˆ˜áˆˆáŠ¨á‰° áŠá‹á¢\n",
      "\n",
      "áŒ†áˆ­áŒ‚á‹¬á‰« á¤ \" á‹¨á‹•á‹³ áˆ˜áˆáˆ¶ áˆ›á‹‹á‰€áˆ­ áˆ‚á‹°á‰µ á‹¨áˆ˜áŒ¨áˆ¨áˆ» á‹°áˆ¨áŒƒ áˆ‹á‹­ á‹­áŒˆáŠ›áˆ á¤ áŠ¨áŠ¢á‰µá‹®áŒµá‹« áŠ á‰ á‹³áˆªá‹Žá‰½ áŒ‹áˆ­ á‰£áˆˆáŠ áŒáŠ•áŠ™áŠá‰µ á‹­áˆ… á‰…á‹µáˆšá‹« á‹¨áˆšáˆ°áŒ á‹ áŒ‰á‹³á‹­ áŠá‹ \" áˆ²áˆ‰ áŒˆáˆáŒ¸á‹‹áˆá¢ \n",
      "\n",
      "á‹¨IMF á•áˆ®áŒáˆ«áˆ áŠ áŠ«áˆ áˆ†áŠá‹áŠ• á‹¨á‰³áŠ­áˆµ áŠ¥áˆ­áˆáŒƒá‹Žá‰½áŠ• á‰ á‰°áˆ˜áˆˆáŠ¨á‰°áˆ á¤ á‹¨áŠ¢á‰µá‹®áŒµá‹« á‰£áˆˆáˆµáˆáŒ£áŠ“á‰µ áˆˆá‰¥áˆ„áˆ«á‹Š á‰ áŒ€á‰± á‹µáŒ‹á áˆˆáˆ›á‹µáˆ¨áŒ á‹ˆáˆ³áŠ á‹¨áˆ†áŠ‘ á‹¨á‰³áŠ­áˆµ áŠ á‰…áˆžá‰½áŠ• áˆ˜áˆˆá‹¨á‰³á‰¸á‹áŠ• áŒ á‰áˆ˜á‹‹áˆá¢ \n",
      "\n",
      "áŒ†áˆ­áŒ‚á‹¬á‰« á¥ á‹¨áŠ¢á‰µá‹®áŒµá‹« áŠ áŒ á‰ƒáˆ‹á‹­ á‹¨áˆ€áŒˆáˆ­ á‹áˆµáŒ¥ áˆáˆ­á‰µ á‹•á‹µáŒˆá‰µ áŠ¨IMF á‹¨áˆ˜áŒ€áˆ˜áˆªá‹« á‰µáŠ•á‰ á‹«á‹Žá‰½ áˆ˜á‰¥áˆˆáŒ¡áŠ• áˆ›á‰¥áˆ«áˆ«á‰³á‰¸á‹áŠ• á‹˜áˆªá–áˆ­á‰°áˆ­ áŠ áˆµáŠá‰¥á‰§áˆá¢\n",
      "\n",
      "á‹¨áˆ›áŠ”áŒ‚áŠ•áŒ á‹³á‹­áˆ¬áŠ­á‰°áˆ« áŠ•áŒáŒáˆ­ á‰°áŠ¨á‰µáˆŽ \" áˆ˜áˆ¬á‰µ áˆ‹á‹­ áŠ«áˆˆá‹ áŠ¥á‹áŠá‰³ áŒ‹áˆ­ á‹¨áˆšáŒˆáŠ“áŠ áŠ á‹­á‹°áˆˆáˆ \" á‹¨áˆšáˆ‰ áŠ áˆµá‰°á‹«á‹¨á‰¶á‰½ áˆ²áˆ°áŒ¡áˆ á‰°áˆ˜áˆáŠ­á‰°áŠ“áˆá¢\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "áˆá‰³áˆµáˆ˜áˆ­á‰€á‹‰ áŠ¨á‰¤á‰°áˆ°á‰¦á‰¿ á‰°á‹°á‰¥á‰ƒ á‹¨áˆ˜áŒ£á‰½ áŠ¥áŒ®áŠ›á‹‰áŠ• áŒ­áŠ«áŠ” á‰ á‰°áˆžáˆ‹á‰ á‰µ áˆáŠ”á‰³ á‹¨áŒˆá‹°áˆˆá‹‰ áŒáˆˆáˆ°á‰¥ á‰ 20 á‹“áˆ˜á‰µ áŒ½áŠ‘ áŠ¥áˆµáˆ«á‰µ á‰°á‰€áŒ£á¢\n",
      "\n",
      "á‰ á‹°á‰¡á‰¥ áŠ¢á‰µá‹®áŒµá‹« áŠ­áˆáˆ áŒ‹áˆž á‹žáŠ• áŠ áˆ­á‰£áˆáŠ•áŒ­ áŠ¨á‰°áˆ› áŠ áˆµá‰°á‹³á‹°áˆ­ áŒ‰áˆ­á‰£ á‰ áˆšá‰£áˆ á‰€á‰ áˆŒ á‹¨áŒˆá‹› áŠ¥áŒ®áŠ›á‹‰áŠ• áŒ­áŠ«áŠ” á‰ á‰°áˆžáˆ‹á‰ á‰µ áˆ˜áˆáŠ© á‰ áŠ áˆ°á‰ƒá‰‚ áˆáŠ”á‰³ á‰ áˆµáˆˆá‰µ áŠ áŠ•áŒˆá‰·áŠ• á‰ áˆ˜á‰áˆ¨áŒ¥ áˆ•á‹­á‹ˆá‰· áŠ¥áŠ•á‹²á‹«áˆá á‹«á‹°áˆ¨áŒˆá‹‰ á‹ˆáŒ£á‰µ á‰ á…áŠ‘ áŠ¥áˆµáˆ«á‰µ áˆ˜á‰€áŒ£á‰±áŠ• á‹¨áŠ áˆ­á‰£ áˆáŠ•áŒ­ áŠ¨á‰°áˆ› áŠ áˆµá‰°á‹³á‹°áˆ­ á–áˆŠáˆµ áˆ˜áˆáˆªá‹« áŠ á‹›á‹¥ áˆ/áŠ¢áŠ•áˆµá”áŠ­á‰°áˆ­ áŒ‹á‹áˆ® á‰¶áˆ›áˆµ áˆˆá‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« á‰°áŠ“áŒáˆ¨á‹‹áˆá¢\n",
      "\n",
      "á‹¨á‹ˆáŠ•áŒ€áˆ á‹µáˆ­áŒŠá‰± á‹¨á‰°áˆá€áˆ˜á‹‰ áŠáˆáˆ´ 16/2016 á‹“/áˆ á‰ áŠ áˆ­á‰£áˆáŠ•áŒ­ áŠ¨á‰°áˆ› áŒ‰áˆ­á‰£ á‰€á‰ áˆŒ áŠá‹á¢\n",
      "\n",
      "á‰°áŠ¨áˆ³áˆ½ á‹®áŠ“áˆµ áŒ«áŠá‰„ á‹¨á‰°á‰£áˆˆá‹ áŒáˆˆáˆ°á‰¥ á‹¨áŒ‚áŠ•áŠ« á‹©áŠ’á‰¨áˆ­áˆ²á‰² 1áŠ›Â  á‹“áˆ˜á‰µ á‰°áˆ›áˆª á‹¨áˆ†áŠá‰½á‹‰áŠ• áˆŸá‰½ áˆŠá‹²á‹« á‹®áˆáŠ•áˆµ áŠ¥áˆ±áŠ• áˆˆáˆ›áˆµáˆ˜áˆ¨á‰… á‹ˆá‹° áŠ áˆ­á‰£ áˆáŠ•áŒ­ áŠ¨á‰°áˆ› á‰ áˆ˜áŒ£á‰½á‰ á‰µ á‰°áŠ¨áˆ«á‹­á‰¶ á‰ áˆšáŠ–áˆ­á‰ á‰µ á‰¤á‰µ áŠ áˆ°á‰ƒá‰‚ á‹µáˆ­áŒŠá‰±áŠ• áˆ˜áˆá€áˆ™áŠ• á‹¨áˆáˆ­áˆ˜áˆ« áˆ˜á‹áŒˆá‰¡ á‹«áˆµáˆ¨á‹³áˆá¢\n",
      "\n",
      "á‹ˆáŒ£á‰· \" áŠ¥áŒ®áŠ›á‹¬ á‹­áˆ˜áˆ¨á‰…áˆáŠ›áˆ \" á‰ áˆšáˆ á‹°áˆµá‰³ áŠ¨á‰¤á‰°áˆ°á‰¦á‰¿ á‰°á‹°á‰¥á‰ƒ á‰°áŠ¨áˆ³áˆ½ á‰°áŠ¨áˆ«á‹­á‰¶ á‹ˆá‹° áˆšáˆ›áˆ­á‰ á‰µÂ  á‰¤á‰µ áˆ˜áŒ¥á‰³ á‰ á‹‹á‹œáˆ›á‹‰ áˆˆáˆáˆ¨á‰ƒá‹‰ á‹¨áˆšáˆ†áŠ‘ á‹¨á‹²áŠ®áˆ­á£ á‹¨á‹³á‰¦áŠ“ áˆˆáˆµáˆ‹áˆ³ áˆ˜áŒ áŒ¦á‰½áŠ“ á‰ á‰¡áŠ“ á‹áŒáŒ…á‰µ á‰¤á‰±áŠ• áŠ áˆ°áˆ›áˆáˆ« á‰ áˆáˆ½á‰±áˆ áŒá‰¢ á‹‰áˆµáŒ¥ á‹«áˆ‰ á‰°áŠ¨áˆ«á‹®á‰½áŠ• áŒ áˆ­á‰°á‹‰ áŠ¨áˆ¸áŠ™ á‰ áŠ‹áˆ‹ áˆŸá‰½ áˆ€áŒˆáˆ­ áˆ°áˆ‹áˆ á‰¥áˆ‹ á‰ á‰°áŠ›á‰½á‰ á‰µ áŠ¨áˆŒáˆŠá‰± 7 áˆ°á‹“á‰µ áŒˆá‹°áˆ› áŠ¥áˆ«áˆ·áŠ• áˆ˜áŠ¨áˆ‹áŠ¨áˆ á‰ áˆ›á‰µá‰½áˆá‰ á‰µ áˆáŠ”á‰³ á‰ á‰¢áˆ‹á‹‹ áŠ áŠ•áŒˆá‰·áŠ• áŠ áˆ­á‹¶ áˆ˜áŒá‹°áˆ‰áŠ• á‹¨áˆáˆ­áˆ˜áˆ« áˆ˜á‹áŒˆá‰¡áŠ• á‹‹á‰¢ áŠ á‹µáˆ­áŒˆá‹ á–áˆŠáˆµ áŠ á‹›á‹¡ áŒˆáˆá€á‹‹áˆá¢\n",
      "\n",
      "á–áˆŠáˆµ áŠ á‹›á‹¡ áŠ áŠ­áˆˆá‹ áŠ¥áŠ•á‹°áŒˆáˆˆáŒ¹á‰µ á¥ á‰ á‹ˆá‰…á‰± á‰ á‰°á‹°áˆ¨áŒˆá‹‰ áˆ›áŒ£áˆ«á‰µáˆ áˆ†áŠ á‰ áŠ­áˆµ áˆ˜á‹áŒˆá‰¡ áˆ‹á‹­ áŠ¥áŠ•á‹°áˆ°áˆáˆ¨á‹‰ á‹ˆáŠ•áŒ€áˆˆáŠ›á‹  \" á‹ˆá‹° á‹©áŠ’á‰¨áˆ­áˆ²á‰² á‰ áˆ„á‹µáˆ½á‰ á‰µ áˆŒáˆ‹ á‹¨á‹ˆáŠ•á‹µ áŒ“á‹°áŠ› á‹­á‹˜áˆ»áˆ \" á‰ áˆšáˆ áŠá‹ á‰ áˆ­ á‹˜áŒá‰¶ áŠ áˆ°á‰ƒá‰‚ á‹¨á‹ˆáŠ•áŒ€áˆ á‹µáˆ­áŒŠá‰±áŠ• á‹¨áˆáŒ¸áˆ˜á‹á¢\n",
      "\n",
      "á–áˆŠáˆµ á‰ á‹šáˆ… á‹˜áŒáŠ“áŠ á‹ˆáŠ•áŒ€áˆ á‹™áˆªá‹« á‰°áŒˆá‰¢á‹‰áŠ• áˆ›áŒ£áˆ«á‰µáŠ“ áˆáˆ­áˆ˜áˆ« áŠ á‹µáˆ­áŒŽ áˆˆá‹á‰ƒá‰¤ áˆ•áŒ áˆ›á‰…áˆ¨á‰¡áŠ• áŠ áˆµá‰³á‹á‰€á‹‹áˆá¢\n",
      "\n",
      "á‹á‰ƒá‰¤ áˆ•áŒáˆ áŠ­áˆµ á‰ áˆ˜áˆ˜áˆµáˆ¨á‰µ áˆˆááˆ­á‹µ á‰¤á‰± á‰°áŠ¨áˆ³áˆ½ á‹¨á‹ˆáŠ•áŒ€áˆ á‹µáˆ­áŒŠá‰±áŠ• áˆ˜áˆá€áˆ™áŠ• á‹¨áˆ°á‹‰á£ á‹¨áˆ°áŠá‹µáŠ“ á‹¨áˆ…áŠ­áˆáŠ“ áˆ›áˆµáˆ¨áŒƒá‹Žá‰½áŠ• á‰ áˆ›á‰…áˆ¨á‰¥ áŠ áˆµáˆ¨á‹µá‰·áˆá¢\n",
      "\n",
      "á‰ á‰€áˆ¨á‰¡ áˆ›áˆµáˆ¨áŒƒá‹Žá‰½ áŠ¥áŠ“ áˆáˆµáŠ­áˆ®á‰½ áŒáˆ« á‰€áŠ™áŠ• áˆ²á‹«áŒ£áˆ« á‹¨á‰†á‹¨á‹‰ á‹¨áŒ‹áˆž á‹žáŠ• áŠ¨áá‰°áŠ›á‹‰ ááˆ­á‹µ á‰¤á‰µ á‰ á‰€áŠ• 29/5/2017 á‹“áˆ á‰ á‹‹áˆˆá‹‰ á‰½áˆŽá‰µ á‰°áŠ¨áˆ³áˆ½ á‹®áŠ“áˆµ áŒ¨áŠá‰„ á‰ á‰°áŠ¨áˆ°áˆ°á‰ á‰µ á‰ áŠ áˆ°á‰ƒá‰‚ áˆáŠ”á‰³ áŠá‰¥áˆµ á‹¨áˆ›áŒ¥á‹á‰µ á‹ˆáŠ•áŒ€áˆ áŒ¥á‹á‰°áŠ› áˆ˜áˆ†áŠ‘áŠ• á‰ áˆ›áˆ¨áŒ‹áŒˆáŒ¥ á‰ 20 á‹“áˆ˜á‰µ á…áŠ‘ áŠ¥áˆµáˆ«á‰µ áŠ¥áŠ•á‹²á‰€áŒ£ áˆ˜á‹ˆáˆ°áŠ‘áŠ•áˆ áŠ¢áŠ•áˆµá”áŠ­á‰°áˆ­ áŒ‹á‹áˆ® á‰¶áˆ›áˆµ áˆˆá‰²áŠ­á‰«áˆ… áŠ¢á‰µá‹®áŒµá‹« á‰°áŠ“áŒáˆ¨á‹‹áˆá¢\n",
      "\n",
      "#TikvahEthiopiaFamilyHW\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "#áˆ˜á‰„á‹¶áŠ•á‹«\n",
      "\n",
      "\" áˆ°á‹áŠ• áˆˆáˆ˜áˆ­á‹³á‰µ áˆ°á‹ áˆ˜áˆ†áŠ• á‰ á‰‚ áŠá‹ !! \"\n",
      "\n",
      "áˆ˜á‰„á‹¶áŠ•á‹« á‹¨áŠ áˆ¨áŒ‹á‹Šá‹«áŠ•áŠ“ á‹¨áŠ áŠ¥áˆáˆ® áˆ…áˆ™áˆ›áŠ• áˆ˜áˆ­áŒƒ áˆ›á‹•áŠ¨áˆ á‰ áˆšá‹«áˆµáŒˆáŠá‰£á‹ áˆ†áˆµá’á‰³áˆ áŒ­áˆáˆ­ á‹«áˆˆá‹ áˆ…áŠ•áƒ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹¨áŒˆáŠ•á‹˜á‰¥ áŠ¥áŒ¥áˆ¨á‰µ áˆµáˆˆáŒˆáŒ áˆ˜á‹ áŠ¥áŒá‹› áŠ¥áŠ•á‹²á‹°áˆ¨áŒ áˆˆáˆ˜áˆ‹á‹ áŠ¢á‰µá‹®áŒµá‹«á‹á‹«áŠ• áŒ¥áˆª áˆ˜á‰…áˆ¨á‰¡ á‹­á‰³á‹ˆáˆ³áˆá¢\n",
      "\n",
      "á‹›áˆ¬ â€œ á‰ áˆ°á‹­á‰ áŠ¢á‰¢áŠ¤áˆµ á‹¨á‹©á‰±á‰¥ á‰»áŠáˆ â€ á‹µáŒ‹á áˆ›á‹µáˆ¨áŒŠá‹« áˆ˜áˆ­áˆ€ áŒá‰¥áˆ­ áŠ¥á‹¨á‰°áŠ«áˆ„á‹° áŠá‹á¢\n",
      "\n",
      "á‹°áŒ‹áŒŽá‰½ áˆáˆ‰ á‹µáŒ‹á áŠ¥áŠ•á‹µá‰³á‹°áˆ­áŒ‰ áŒ¥áˆª áŠ¥áŠ“á‰€áˆ­á‰£áˆˆáŠ•á¢\n",
      "\n",
      "á‹¨á‹µáŒ‹á áˆ›á‹µáˆ¨áŒŠá‹« áŠ áˆ›áˆ«áŒ®á‰½ áŠ¨áˆ‹á‹­ á‰°á‹«á‹­á‹˜á‹‹áˆá¢\n",
      "\n",
      "áˆ˜á‰„á‹¶áŠ•á‹« â€œ áˆ…áŠ•áƒá‹ áˆˆáˆ›áŒ áŠ“á‰€á‰… áŒˆáŠ•á‹˜á‰¥ á‰°á‰¸áŒáˆ¨áŠ“áˆá¢ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹ˆá‹° 5 á‰¢áˆŠá‹®áŠ• á‰¥áˆ­ á‹«áˆµáˆáˆáŒˆáŠ“áˆ â€ áˆ›áˆˆá‰± áŠ á‹­á‹˜áŠáŒ‹áˆá¢\n",
      "\n",
      "á‰ á‰€áŒ¥á‰³ á‹­áŠ¨á‰³á‰°áˆ‰ ðŸ‘‡\n",
      "https://www.youtube.com/live/q0bMjwt9PvM?feature=shared\n",
      "\n",
      "@tikvahethiopia\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=load_data(\"datas/totaldata.json\")\n",
    "number_of_contents=len(data)\n",
    "print(f'Total number of contents: {number_of_contents}\\n')\n",
    "print(f'First 5 contents: \\n')\n",
    "for news in data[:5]:\n",
    "    print(news)\n",
    "    print(\"---------------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrdFh1uNDnqq"
   },
   "source": [
    "Let's clean our data using clean_text function and sample our data to see the difference between the original and cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "executionInfo": {
     "elapsed": 72609,
     "status": "ok",
     "timestamp": 1740393331143,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "oYvDtSZFDnqq"
   },
   "outputs": [],
   "source": [
    "cleaned_data=[clean_text(content) for content in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZyTv0jpDnqq"
   },
   "source": [
    "Let's see the total number of words and total number of unique words in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6671,
     "status": "ok",
     "timestamp": 1740393337827,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "z_hxQRkgDnqq",
    "outputId": "f7539e5d-148d-4937-fc95-edb768caf372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the dataset: 15290227\n",
      "\n",
      "Total number of unique words in the dataset: 832978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total=0\n",
    "for news in cleaned_data:\n",
    "    news_array=news.split()\n",
    "    total+=len(news_array)\n",
    "print(f'Total number of words in the dataset: {total}\\n')\n",
    "alldata=\" \".join(cleaned_data)\n",
    "counter=Counter(alldata.split())\n",
    "print(f'Total number of unique words in the dataset: {len(counter)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's sample and see how the data looks like before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1740393340830,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "zr-flvxsDnqq",
    "outputId": "c485468f-99f2-4c87-ba8f-ba722bcc5680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at index 37430 before cleaning: \n",
      "\n",
      " updateâ¬†ï¸á‹¨á‰´á’ áŠ¨á‰°áˆ› á‹¨á‹›áˆ¬ áŒ¥á‹‹á‰µ á‹µá‰£á‰¥ áŠ¥áŠ“ áŠ áŒ á‰ƒáˆ‹á‹­ á‹¨áŠ¨á‰°áˆ›á‹‹ áˆáŠ”á‰³ áŠ¨áŠ¨á‰°áˆ›á‹ áŠá‹‹áˆª áŠ áˆá‹°á‰ á‰µá¢\n",
      "@tseabwolde @tikvahethiopia\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Data at index 37430 after cleaning: \n",
      "\n",
      " á‹¨á‰´á’ áŠ¨á‰°áˆ› á‹¨á‹›áˆ¬ áŒ¥á‹‹á‰µ á‹µá‰£á‰¥ áŠ¥áŠ“ áŠ áŒ á‰ƒáˆ‹á‹­ á‹¨áŠ¨á‰°áˆ›á‹‹ áˆáŠ”á‰³ áŠ¨áŠ¨á‰°áˆ›á‹ áŠá‹‹áˆª áŠ áˆá‹°á‰ á‰µá¢\n"
     ]
    }
   ],
   "source": [
    "index=37430\n",
    "print(f\"Data at index {index} before cleaning: \\n\\n\",data[index])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(f\"Data at index {index} after cleaning: \\n\\n\",cleaned_data[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-nCsPvzDnqq"
   },
   "source": [
    "### 3.3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRM-2OHMDnqq"
   },
   "source": [
    "#### Next, We Will Train the Tokenizer\n",
    "\n",
    "Tokenization is a critical step in natural language processing (NLP) as it converts raw text into smaller, meaningful units (tokens) that can be processed by machine learning models. Effective tokenization ensures that the model can understand and interpret the text accurately, which is essential for tasks like text classification, machine translation, and sentiment analysis.\n",
    "\n",
    "For this task, we will use the **SentencePiece tokenizer** instead of traditional word-based tokenization. The [SentencePieceTokenizer](https://www.tensorflow.org/text/api_docs/python/text/SentencepieceTokenizer) is a powerful tool that tokenizes text into **subword units**, which offers several advantages:\n",
    "\n",
    "1. **Handling Complex Word Structures**: SentencePiece breaks words into smaller subword units, making it effective for handling complex word structures and morphological variations, which are common in languages like Amharic.\n",
    "2. **Out-of-Vocabulary (OOV) Words**: By using subword tokenization, SentencePiece can handle out-of-vocabulary words more gracefully, as it can decompose them into known subword units.\n",
    "3. **Multilingual Support**: SentencePiece is language-agnostic, making it suitable for multilingual datasets. This is particularly useful for Amharic, as it can handle the repetition of common subwords and morphological patterns unique to the language.\n",
    "4. **Simplified Preprocessing**: SentencePiece works directly on raw text, eliminating the need for extensive preprocessing steps like word segmentation or stemming.\n",
    "5. **Seamless Integration**: It integrates seamlessly with popular machine learning frameworks like TensorFlow and PyTorch, ensuring consistent tokenization across training and inference pipelines.\n",
    "\n",
    "Given these benefits, SentencePiece is an ideal choice for tokenizing Amharic text, as it can effectively capture the language's unique characteristics while simplifying the overall preprocessing workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1LDIkg3Dnqq"
   },
   "source": [
    "Let's train sentencepiece tokenizer model first. in order to do that we need to save our cleaned data into a single corpus of text in .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now as I have saved the cleaned file as cleaned_data.txt inside datas folder, we can skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KKMDsq13Dnqq"
   },
   "outputs": [],
   "source": [
    "# with open(\"datas/cleaned_data.txt\", \"a\") as file:\n",
    "#    for content in cleaned_data:\n",
    "#        file.write(content + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains a tokenizing sentencepiece model and save the result. But as I have trained the model and the saved the model checkpoints inside sentencepiece_model directory we can skip the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Z4oMJhJ4Dnqq",
    "outputId": "1dadacc3-10fe-4a82-ccc5-9f95741bb29c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece model...\n",
      "Training complete! Check 'amharic_bpe.model' and 'amharic_bpe.vocab'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: datas/cleaned_data.txt\n",
      "  input_format: \n",
      "  model_prefix: amharic_sp_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 8192\n",
      "  num_threads: 6\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: datas/cleaned_data.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (9644 > 8192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(391) LOG(INFO) Reserved chars are found. Skipped: â•¯â–…â•°â•±â–”â–”â–”â–”â–”â–”â–”â•²â•¯â˜¼ â–•â–•â•±â•±â•±â•±â•±â•±â•±â•±â”›â–‚â•²â•² â•±â–‚â–‚â–‚â–‚â–‚â–‚â•±â•±â”â–•â•‹â–â•²â•² â–”â–â–‚â”—â”“â–‚â–•â–”â”›â–‚â”â–”â–‚â–•â–” â–•â–•â•‹â–â–•â•‹â–â–â–•â”â–â–•â•‹â–| áŠ¥áŠ› á‹¨áˆ™á‹µ áŠ¥áŠ•á‹«á‹ á‰¤á‰°áˆ°á‰¦á‰½ á‰µáŠ«á‹œ á‹µáŠ•á‹›á‹œ á‹á‹›á‹œ áŠ‘á‹›á‹œ á‹‰áˆµáŒ£á‰½áŠ• áŠ á‹°áˆˆáˆ áˆá‰³ áˆá‰€áŠ› áŠ¥áŠ•á‹²á‰ƒáŒ áˆ\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 191827 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 553 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=75721594\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.5055% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=222\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.995055\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 191827 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=37769524\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 768596 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 191827\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 784912\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 784912 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=355352 obj=13.161 num_tokens=1612991 num_tokens/piece=4.53914\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=303986 obj=11.552 num_tokens=1618339 num_tokens/piece=5.32373\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=227957 obj=11.5203 num_tokens=1690775 num_tokens/piece=7.41708\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=227779 obj=11.5045 num_tokens=1695108 num_tokens/piece=7.4419\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=170832 obj=11.5413 num_tokens=1794991 num_tokens/piece=10.5073\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=170822 obj=11.5417 num_tokens=1795487 num_tokens/piece=10.5109\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=128116 obj=11.5906 num_tokens=1899315 num_tokens/piece=14.825\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=128116 obj=11.5718 num_tokens=1899287 num_tokens/piece=14.8247\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=110000 obj=11.6223 num_tokens=1952244 num_tokens/piece=17.7477\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=110000 obj=11.6115 num_tokens=1952426 num_tokens/piece=17.7493\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: amharic_sp_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: amharic_sp_model.vocab\n"
     ]
    }
   ],
   "source": [
    "input_file=\"datas/cleaned_data.txt\"\n",
    "model_prefix=\"amharic_sp_model\"\n",
    "print(\"Training SentencePiece model...\")\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=100000,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=0.995,\n",
    "    num_threads=6,\n",
    "    max_sentence_length=8192,\n",
    "    split_by_whitespace=True,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "\n",
    ")\n",
    "print(\"Training complete! Check 'amharic_bpe.model' and 'amharic_bpe.vocab'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZUNG4vjDnqq"
   },
   "source": [
    "After training the sentencepeice tokenizer the next step is to load the trainied tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740393357180,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "bvTD7kDMDnqq"
   },
   "outputs": [],
   "source": [
    "tokenizer=spm.SentencePieceProcessor(model_file=\"./sentencepiece_model/amharic_sp_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3JeWvZlDnqr"
   },
   "source": [
    "This code shows the process of tokenizing individual words from a given text, in this case, the first entry of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1740393360406,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "BXJMtkhpDnqr",
    "outputId": "55ba846a-b5ff-4305-a1d7-dda2e790806f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t-->\tTokenization\n",
      "----------------------------------------\n",
      "á‰ áŠ áˆ›áˆ«    \t-->\t[503]\n",
      "áŠ­áˆáˆ     \t-->\t[41]\n",
      "áˆ˜á‹²áŠ“     \t-->\t[2571]\n",
      "á‰ á‰£áˆ…áˆ­    \t-->\t[3954]\n",
      "á‹³áˆ­      \t-->\t[577]\n",
      "áŠ¨á‰°áˆ›     \t-->\t[25]\n",
      "á‰€á‰ áˆŒ     \t-->\t[556]\n",
      "14      \t-->\t[589]\n",
      "á‰µáˆ‹áŠ•á‰µ    \t-->\t[414]\n",
      "áˆ˜áŒ‹á‰¢á‰µ    \t-->\t[915]\n",
      "29      \t-->\t[1161]\n",
      "á‹¨áˆ˜áŒáˆªá‰¥   \t-->\t[40582, 39560]\n",
      "áˆ°áˆ‹á‰µ     \t-->\t[22531]\n",
      "áˆ°áŒá‹°á‹    \t-->\t[80378]\n",
      "áˆ²áˆ˜áˆˆáˆ±    \t-->\t[5520]\n",
      "á‹¨áŠá‰ áˆ©    \t-->\t[296]\n",
      "áŠ á‰£á‰µ     \t-->\t[1033]\n",
      "áŠ¨3      \t-->\t[12, 391]\n",
      "áˆáŒ†á‰¹     \t-->\t[8106]\n",
      "áŠ¥áŠ•á‹²áˆáˆ   \t-->\t[56]\n",
      "áŠ áŠ•á‹µ     \t-->\t[60]\n",
      "áŒŽáˆ¨á‰¤á‰³á‰¸á‹áŠ• \t-->\t[63799, 5]\n",
      "áŒ¨áˆáˆ®     \t-->\t[166]\n",
      "áŠ áŒ á‰ƒáˆ‹á‹­   \t-->\t[462]\n",
      "5       \t-->\t[189]\n",
      "áˆ°á‹Žá‰½     \t-->\t[27]\n",
      "á‰ á‰°áŠ¨áˆá‰°á‰£á‰¸á‹\t-->\t[6, 31096]\n",
      "á‹¨áŒ¥á‹­á‰µ    \t-->\t[9820]\n",
      "áŠ¥áˆ©áˆá‰³    \t-->\t[22829]\n",
      "áˆ˜áŒˆá‹°áˆ‹á‰¸á‹  \t-->\t[4926]\n",
      "á‰°áŠáŒáˆ¯áˆá¢  \t-->\t[528]\n",
      "á‰µáˆ‹áŠ•á‰µáŠ“   \t-->\t[9146]\n",
      "áˆáˆ½á‰µ     \t-->\t[368]\n",
      "á‰ áŒá     \t-->\t[7180]\n",
      "á‹¨á‰°áŒˆá‹°áˆ‰á‰µ  \t-->\t[6444]\n",
      "á¥       \t-->\t[4, 1]\n",
      "áŠ á‰¶      \t-->\t[44]\n",
      "áˆ™áˆ„      \t-->\t[51148]\n",
      "á£       \t-->\t[33]\n",
      "áˆáŒƒá‰¸á‹    \t-->\t[4786]\n",
      "áŠ á‰ á‰£á‹‰    \t-->\t[43, 124]\n",
      "áˆ™áˆ„      \t-->\t[51148]\n",
      "á£       \t-->\t[33]\n",
      "áˆ½áŠ©áˆ­     \t-->\t[24929]\n",
      "áˆ™áˆ„      \t-->\t[51148]\n",
      "á£       \t-->\t[33]\n",
      "áˆ™áˆ‹á‰µ     \t-->\t[9158]\n",
      "áˆ™áˆ„      \t-->\t[51148]\n",
      "áŠ¥áŠ“      \t-->\t[7]\n",
      "áŒŽáˆ¨á‰¤á‰³á‰¸á‹  \t-->\t[63799]\n",
      "áŠ á‰¶      \t-->\t[44]\n",
      "áŠ¥áŠ•á‹µáˆªáˆµ   \t-->\t[17857]\n",
      "á‹¨á‰°á‰£áˆ‰    \t-->\t[1376]\n",
      "áˆ²áˆ†áŠ‘     \t-->\t[947]\n",
      "áˆµáˆ­á‹“á‰µ    \t-->\t[769]\n",
      "á‰€á‰¥áˆ«á‰¸á‹   \t-->\t[13051]\n",
      "á‰ á‹›áˆ¬á‹    \t-->\t[259]\n",
      "á‹•áˆˆá‰µ     \t-->\t[207]\n",
      "á‰°áˆá…áˆŸáˆá¢  \t-->\t[9115]\n",
      "áŠ¥áˆµáŠ«áˆáŠ•   \t-->\t[307]\n",
      "áŒˆá‹³á‹®á‰½    \t-->\t[19551]\n",
      "áˆµáˆˆáˆ˜á‹«á‹›á‰¸á‹ \t-->\t[45001]\n",
      "á‹¨á‰°á‰£áˆˆ    \t-->\t[801]\n",
      "áŠáŒˆáˆ­     \t-->\t[67]\n",
      "á‹¨áˆˆáˆá¢    \t-->\t[984]\n",
      "á‰ áŠ¨á‰°áˆ›á‹‹   \t-->\t[1197]\n",
      "áŠ¨á‰°áŒˆá‹°áˆ‰á‰µ  \t-->\t[10342]\n",
      "áˆ°á‹Žá‰½     \t-->\t[27]\n",
      "á‰£áˆ»áŒˆáˆ­    \t-->\t[2231]\n",
      "á‰£áˆ…áˆ­á‹³áˆ­   \t-->\t[2982]\n",
      "áŠ¨á‰°áˆ›     \t-->\t[25]\n",
      "áŠ á‰£á‹­     \t-->\t[4770]\n",
      "áˆ›á‹¶      \t-->\t[8609]\n",
      "á‹¨áˆšáŒˆáŠ˜á‹   \t-->\t[506]\n",
      "áˆ˜áˆµáŒ‚á‹µ    \t-->\t[8041]\n",
      "áŠ¨áá‰°áŠ›    \t-->\t[53]\n",
      "á‹¨áˆ˜áˆ³áˆªá‹«   \t-->\t[14257]\n",
      "á‹µá‰¥á‹°á‰£    \t-->\t[2667]\n",
      "áŠ¥áŠ•á‹°á‰°áˆá€áˆ˜á‰ á‰µ\t-->\t[33102]\n",
      "á‰°áŒˆáˆáŒ¿áˆá¢  \t-->\t[141]\n",
      "áŠ¨á‹šáˆ     \t-->\t[5286]\n",
      "áŒ‹áˆ­      \t-->\t[17]\n",
      "á‰ á‰°á‹«á‹«á‹˜   \t-->\t[526]\n",
      "á‹›áˆ¬      \t-->\t[57]\n",
      "á‹¨á‰£áˆ…áˆ­    \t-->\t[1839]\n",
      "á‹³áˆ­      \t-->\t[577]\n",
      "áˆ™áˆµáˆŠáˆžá‰½   \t-->\t[8870]\n",
      "á‰ áŠ­áˆáˆ‰    \t-->\t[491]\n",
      "á‰ áˆ™áˆµáˆŠáˆžá‰½  \t-->\t[63342]\n",
      "áˆ‹á‹­      \t-->\t[10]\n",
      "áŠ áŠáŒ£áŒ¥áˆ¨á‹‹áˆ \t-->\t[94902, 20007]\n",
      "á‹«áˆ‰á‰µáŠ•    \t-->\t[2078]\n",
      "áŒá‹µá‹«     \t-->\t[1090]\n",
      "áŠ¥áŠ“      \t-->\t[7]\n",
      "áŠ¥áŒˆá‰³     \t-->\t[5724]\n",
      "á‰ áˆ˜á‰ƒá‹ˆáˆ   \t-->\t[3574]\n",
      "áˆ°áˆá     \t-->\t[822]\n",
      "áˆ›á‹µáˆ¨áŒ‹á‰¸á‹áŠ• \t-->\t[2988]\n",
      "\"       \t-->\t[14]\n",
      "áˆ€áˆ©áŠ•     \t-->\t[33660]\n",
      "áˆšá‹²á‹«     \t-->\t[549]\n",
      "\"       \t-->\t[14]\n",
      "á‹˜áŒá‰§áˆá¢   \t-->\t[427]\n",
      "áŠ¥áˆµáŠ«áˆáŠ•   \t-->\t[307]\n",
      "á‰ áŠ áˆ›áˆ«    \t-->\t[503]\n",
      "áŠ­áˆáˆ     \t-->\t[41]\n",
      "áŠ¥áˆµáˆáˆáŠ“   \t-->\t[3278]\n",
      "áŒ‰á‹³á‹®á‰½    \t-->\t[231]\n",
      "áŠ¨áá‰°áŠ›    \t-->\t[53]\n",
      "áˆáŠ­áˆ­     \t-->\t[170]\n",
      "á‰¤á‰µáˆ     \t-->\t[28, 8]\n",
      "áˆ†áŠ      \t-->\t[289]\n",
      "á‰ áŠ¢á‰µá‹®áŒµá‹«  \t-->\t[126]\n",
      "áŠ¥áˆµáˆáˆáŠ“   \t-->\t[3278]\n",
      "áŒ‰á‹³á‹®á‰½    \t-->\t[231]\n",
      "áŒ á‰…áˆ‹á‹­    \t-->\t[158]\n",
      "áˆáŠ­áˆ­     \t-->\t[170]\n",
      "á‰¤á‰µ      \t-->\t[28]\n",
      "á‹¨á‰°áˆ°áŒ     \t-->\t[2476]\n",
      "áŠ áˆµá‰°á‹«á‹¨á‰µ  \t-->\t[620]\n",
      "á‹¨áˆˆáˆá¢    \t-->\t[984]\n",
      "á‰²áŠ­á‰«áˆ…    \t-->\t[1134]\n",
      "áŠ¢á‰µá‹®áŒµá‹«   \t-->\t[54]\n",
      "á‰ áŠ­áˆáˆ‰    \t-->\t[491]\n",
      "á‰°áˆá…áˆ˜á‹‹áˆ  \t-->\t[32369]\n",
      "áˆµáˆˆá‰°á‰£áˆ‰   \t-->\t[62519]\n",
      "áŒá‹µá‹«á‹Žá‰½   \t-->\t[9726]\n",
      "á£       \t-->\t[33]\n",
      "áŒ¥á‰ƒá‰¶á‰½    \t-->\t[2225]\n",
      "á£       \t-->\t[33]\n",
      "á‹˜áˆ¨á‹áŠ“    \t-->\t[27038]\n",
      "áŠ¥áŒˆá‰³á‹Žá‰½   \t-->\t[46541]\n",
      "á‹¨áŠ áˆ›áˆ«    \t-->\t[409]\n",
      "áŠ­áˆáˆ     \t-->\t[41]\n",
      "áŠ¥áˆµáˆáˆáŠ“   \t-->\t[3278]\n",
      "áŒ‰á‹³á‹®á‰½    \t-->\t[231]\n",
      "áŠ¨áá‰°áŠ›    \t-->\t[53]\n",
      "áˆáŠ­áˆ­     \t-->\t[170]\n",
      "á‰¤á‰µ      \t-->\t[28]\n",
      "áŠ¥áŠ“      \t-->\t[7]\n",
      "á‹¨áˆšáˆ˜áˆˆáŠ¨á‰³á‰¸á‹\t-->\t[2664]\n",
      "áŠ áŠ«áˆ‹á‰µáŠ•   \t-->\t[2983]\n",
      "áˆˆáˆ›áŠáŒ‹áŒˆáˆ­  \t-->\t[12139]\n",
      "áŒ¥áˆ¨á‰µ     \t-->\t[377]\n",
      "áŠ¥á‹«á‹°áˆ¨áŒˆ   \t-->\t[998]\n",
      "á‹­áŒˆáŠ›áˆá¤   \t-->\t[23033]\n",
      "áˆáˆ‹áˆ½     \t-->\t[305]\n",
      "áŠ¥áŠ•á‹³áŒˆáŠ˜   \t-->\t[14546]\n",
      "á‰°áŒ¨áˆ›áˆª    \t-->\t[224]\n",
      "áˆ˜áˆ¨áŒƒá‹Žá‰½áŠ•  \t-->\t[1140]\n",
      "á‹«á‰€áˆ­á‰£áˆá¢  \t-->\t[10129]\n"
     ]
    }
   ],
   "source": [
    "# printing the encoding of each word to see how subwords are tokenized\n",
    "tokenized_text = [(list(tokenizer.tokenize(word)), word) for word in cleaned_data[3000].split()]\n",
    "\n",
    "print(\"Word\\t\\t-->\\tTokenization\")\n",
    "print(\"-\"*40)\n",
    "for element in tokenized_text:\n",
    "    print(f\"{element[1]:<8}\\t-->\\t{element[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CwYXxIWDnqr"
   },
   "source": [
    "Now let's take data from the cleaned_data  and see how the tokenization of the whole content looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1740393360661,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "g-4Bi-FODnqr",
    "outputId": "818cfad2-6296-4363-af62-d9fa1f48164b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at index 890 before tokenization: \" áˆ°áˆ‹áˆ áŠ¨áŠ áŠ•áŒˆá‰µ á‰ áˆ‹á‹­áŠ“ á‹áˆ áˆ‹áˆˆáˆ›áˆˆá‰µ á‹«áˆ…áˆ á‹¨áˆáŠ•áŠ“áŒˆáˆ¨á‹ áˆ³á‹­áˆ†áŠ• á‹‹áŒ‹ áŠ¨ááˆˆáŠ• á‹¨áˆáŠ“áˆ˜áŒ£á‹ áŠá‹ \" - á‰…á‹±áˆµáŠá‰³á‰¸á‹ á‹›áˆ¬ á‹¨áˆ°áˆ‹áˆ áˆšáŠ’áˆµá‰´áˆ­ áŠ áŠ•á‹µ á‹“áˆˆáˆ áŠ á‰€á áŠ®áŠ•áˆáˆ¨áŠ•áˆµ áŠ á‹˜áŒ‹áŒ…á‰¶ áŠá‰ áˆ­á¢ á‰ á‹šáˆ… áˆ˜á‹µáˆ¨áŠ­ áˆ‹á‹­áˆ á‹¨áˆ°áˆ‹áˆ áˆšáŠ’áˆµá‰µáˆ­ áŠ á‰¶ á‰¥áŠ“áˆá áŠ á‹±á‹“áˆˆáˆ á£ á‹¨áŠ¢á‰µá‹®áŒµá‹« á‹¨áˆƒá‹­áˆ›áŠ–á‰µ á‰°á‰‹áˆ›á‰µ á‹¨á‰ áˆ‹á‹­ áŒ á‰£á‰‚ áŠ á‰£á‰¶á‰½á£ á‰¥áá‹“áŠ• áŠ á‰ á‹ áˆŠá‰ƒáŠ áŒ³áŒ³áˆ³á‰µ á‹ˆáŠ¤áŒ²áˆµ á‰†áŒ¶áˆ³á‰µ á£ á‹¨áˆ˜áŠ•áŒáˆ¥á‰µ á‰£áˆˆáˆµáˆáŒ£áŠ“á‰µ á£ áŠ á‰£áˆ³á‹°áˆ®á‰½ áŒ­áˆáˆ­ á‰°áŒˆáŠá‰°á‹ áŠá‰ áˆ­á¢ á‰ áˆ˜á‹µáˆ¨áŠ© á¤ á‰¥áá‹• á‹ˆá‰…á‹±áˆµ áŠ á‰¡áŠ áˆ›á‰µá‹«áˆµ á‰€á‹³áˆ›á‹Š á“á‰µáˆ­á‹«áˆ­áŠ­ áˆ­áŠ¥áˆ° áˆŠá‰ƒáŠ áŒ³áŒ³áˆ³á‰µ á‹˜áŠ¢á‰µá‹®áŒµá‹« áˆŠá‰€ áŒ³áŒ³áˆµ á‹˜áŠ áŠ­áˆ±áˆ á‹ˆáŠ¥áŒ¨áŒŒ á‹˜áˆ˜áŠ•á‰ áˆ¨ á‰°áŠ­áˆˆáˆƒá‹­áˆ›áŠ–á‰µ áˆ˜áˆá‹•áŠ­á‰µ áŠ áˆµá‰°áˆ‹áˆáˆá‹‹áˆá¢ á‰…á‹±áˆµáŠá‰³á‰¸á‹ áˆáŠ• áŠ áˆ‰ ? (áŠ¨áˆ˜áˆá‹•áŠ­á‰³á‰¸á‹ á‹¨á‰°á‹ˆáˆ°á‹°) \" áˆ°áˆ‹áˆ á‹¨áˆ°á‹ áˆáŒ†á‰½ ááˆ‹áŒŽá‰µá£ á‹¨á‰¥á‹™ áˆáŠ•á‹±á‰£áŠ• á‹¨á‹¨á‹•áˆˆá‰µ áŠ“áá‰†á‰µ áŠá‹á¢ á‹¨á‰ áˆ­áŠ«á‰³ á‹˜áˆ˜áŠ“á‰µ á‰…áˆ­áˆ¶á‰½á¤ áŒŠá‹œá£ áŒˆáŠ•á‹˜á‰¥ áŠ¥áŠ“ á‹¨áˆ°á‹ áŒ‰áˆá‰ á‰µ á‹¨áˆáˆ°áˆ°á‰£á‰¸á‹ áŒáŠ•á‰£á‰³á‹Žá‰½ á‰ áˆ°áˆ‹áˆ áˆ›áŒ£á‰µ á‰ á‰…áŒ½á‰ á‰µ á‹­áˆáˆ­áˆ³áˆ‰á¢ áˆ°áˆ‹áˆ áŠ«áˆˆ á‹¨á‹“áˆˆáˆ áˆ€á‰¥á‰µ áˆˆáˆáˆ‰áˆ á‰ á‰‚ áŠá‹á¢ áˆ°áˆ‹áˆ áˆ›áŒ£á‰µ áŒáŠ• á‰¥á‹™ áˆ áˆ«á‹Šá‰µá£ á‰¥á‹™ á‹¨áŒ¦áˆ­ áˆ˜áˆ³áˆªá‹« áŠ¥áŠ•á‹²á‹˜áŒ‹áŒ… áŠ¥á‹«á‹°áˆ¨áŒˆ áˆ€á‰¥á‰µáŠ• á‹«á‹ˆá‹µáˆ›áˆá¢ áŒ¦áˆ­áŠá‰µ áˆ›áˆˆá‰µ áˆ€á‰¥á‰µáŠ“ áˆ•á‹­á‹ˆá‰µáŠ• á‹ˆá‹°áˆšáŠá‹µ áŠ¥áˆ³á‰µ á‹áˆµáŒ¥ áˆ˜áŒ£áˆ áŠá‹á¢ á‹¨áŠ áŠ•á‹°áŠ›áŠ“ á‹¨áˆáˆˆá‰°áŠ› á‹“áˆˆáˆ áŒ¦áˆ­áŠá‰µá£ á‰³áˆªáŠ­ á‰¥á‰» áˆ³á‹­áˆ†áŠ• áŒ á‰£áˆ³á‹ áŠ áˆáŠ•áˆ á‹¨á‹“áˆˆáˆáŠ• áˆ˜áˆáŠ­ áŠ á‰ áˆ‹áˆ½á‰¶á‰³áˆá¢ áˆ°áˆ‹áˆ á‰ á‹áˆµáŒ¥á‹‹ áŒˆáˆ«áˆáŠá‰µá£ á‰µá‹•áŒáˆ¥á‰µá£ á‰³á‹›á‹¥áŠá‰µáŠ“ á‰ á‰µáˆ…á‰µáŠ“ á‹á‰… áˆ›áˆˆá‰µ áˆµáˆˆáˆšáŒˆáŠ™ áˆ˜áˆ«áˆ« á‰µáˆ˜áˆµáˆ‹áˆˆá‰½á¤ á‰ á‹áŒ¤á‰· áŒáŠ• áˆ€áŒˆáˆ­áŠ• áŠ¨áŒ¥á‹á‰µá£ áˆ•á‹á‰¥áŠ• áŠ¨áˆ˜áŠ¨áˆ« áˆ›á‰µáˆ¨á á‹¨áˆšá‰»áˆ á‰ áˆ˜áˆ†áŠ‘ á‹‹áŒ‹á‹‹ áŠ¨á á‹«áˆˆ áŠá‹á¢ á‰…á‹µáˆµá‰µ á‰¤á‰° áŠ­áˆ­áˆµá‰²á‹«áŠ“á‰½áŠ• á¦ Â° áˆ°áˆ‹áˆ á‹¨áˆ†áŠá‹ áŠ­áˆ­áˆµá‰¶áˆµ á‹¨áˆšáˆ°á‰ áŠ­á‰£á‰µá£ Â° á‹¨áˆ°áˆ‹áˆ áˆ˜áˆáŠ¥áŠ­á‰°áŠžá‰½ á‰ á‹áˆµáŒ¥á‹‹ á‹¨áˆšáˆ˜áˆ‹áˆˆáˆ±á‰£á‰µá£ Â° á‰ áŒá‰¥áˆ¨ áŠƒáŒ¢áŠ á‰µ á‹¨á‹ˆá‹°á‰á‰µ á‰ áŠ•áˆµáˆ“ áŠ¨áŠ¥áŒá‹šáŠ á‰¥áˆ”áˆ­ áŒ‹áˆ­ á‹¨áˆšá‰³áˆ¨á‰á‰£á‰µ á‹¨áˆ°áˆ‹áˆ á‹µáˆá‹µá‹­ áˆµáˆˆáˆ†áŠá‰½ á‰ áˆ¥áˆ­á‹“á‰° á‰…á‹³áˆ´á‹‹ áˆ°áˆ‹áˆáŠ• á‹°áŒ‹áŒáˆ› á‰³á‹áŒƒáˆˆá‰½á¤ á‰ áŒ¸áˆŽá‰· áˆˆáˆ˜áˆ‹á‹ á‹“áˆˆáˆ áˆ°áˆ‹áˆáŠ• á‰µáˆˆáˆáŠ“áˆˆá‰½á¤ á‰ áŒ‰áˆáˆ‹á‰µá‹‹ áˆ‹á‹­ á‹¨áˆ°á‰€áˆˆá‰½á‹ áˆ˜áˆµá‰€áˆáˆ áˆ°áˆ‹áˆáŠ• á‹¨áˆšáˆ°á‰¥áŠ­ áŠá‹á¤ á‹¨áˆ˜áˆµá‰€áˆ‰ á‰…áˆ­á… á‹ˆá‹° áˆ‹á‹­áŠ“ á‹ˆá‹° áŒŽáŠ• áˆ˜áˆ†áŠ‘áˆ áŠ¨áŠ¥áŒá‹šáŠ á‰¥áˆ”áˆ­áŠ“ áŠ¨áˆ°á‹ áŒ‹áˆ­ áˆ°áˆ‹áˆ áˆ˜áˆ†áŠ• áŠ¥áŠ•á‹³áˆˆá‰¥áŠ• á‹¨áˆšá‹«áˆµáŒˆáŠá‹á‰ áŠ• áŠá‹á¢ á‰³áˆªáŠ«á‰½áŠ• áŠ¥áŠ•á‹°áˆšáŠáŒáˆ¨áŠ• á‹ˆáŠ•á‹µáˆ›áˆ›á‰¾á‰½ áˆ²áŒ‹á‹°áˆ‰á£ á‰ áˆ•á‹á‰¥ áˆ˜áŠ«áŠ¨áˆ áˆ˜á‰°áˆ‹áˆˆá‰… áˆ²áˆ˜áŒ£ á‰¤á‰° áŠ­áˆ­áˆµá‰²á‹«áŠ• á‰³á‰¦á‰µ áŠ áŠ­á‰¥áˆ«á£ á‰ áŠ¥áˆ³á‰µ áˆ˜áŠ«áŠ¨áˆ áŒˆá‰¥á‰³ áˆ°áˆ‹áˆáŠ• áˆµá‰³á‹ˆáˆ­á‹µ á‹¨áŠ–áˆ¨á‰½ áŠ“á‰µá¢ á‰ áˆ€áŒˆáˆ­ á‹áˆµáŒ¥ áŒáŒ­á‰¶á‰½ á‰ á‰°áˆáŒ áˆ©á‰ á‰µ áŒŠá‹œáˆ á‹¨áˆ°áˆ‹áˆ áŒ¥áˆªáŠ• á‹«áˆ‹áˆµá‰°áˆ‹áˆˆáˆá‰½á‰ á‰µ á‰€áŠ•áŠ“ áˆ°á‹“á‰µ áŠ á‹­áŒˆáŠáˆá¡á¡ áˆ°áˆ‹áˆáŠ• á‹¨áˆ˜á‹ˆá‹«á‹« áˆ­áŠ¥áˆµ áŠ á‹µáˆ­áŒˆáŠ• áˆµáŠ•áˆ°á‰£áˆµá‰¥ á‰ áŒ¦áˆ­áŠá‰µ áˆ˜áŠ«áŠ¨áˆ á‹¨á‰°áŒ¨áŠá‰ áˆ•á‹á‰¦á‰½á£ áˆ«áˆ³á‰¸á‹áŠ• áˆ˜áŠ¨áˆ‹áŠ¨áˆ á‹¨áˆ›á‹­á‰½áˆ‰ áŠ¥áŠ“ áˆáŠ• áŠ¥á‹¨á‰°á‹°áˆ¨áŒˆ áŠ¥áŠ•á‹³áˆˆ á‰ á‹áˆ á‹¨áˆ›á‹­áŒˆáŠá‹˜á‰¡ á‹°áŠ«áˆžá‰½ á‰°áˆµá‹ á‹«á‹°áˆ­áŒ‰áŠ“áˆá¢ áˆµáˆˆá‹šáˆ… áˆ°áˆ‹áˆ áŠ¨áŠ áŠ•áŒˆá‰µ á‰ áˆ‹á‹­áŠ“ á‹áˆ áˆ‹áˆˆáˆ›áˆˆá‰µ á‹«áˆ…áˆ á‹¨áˆáŠ•áŠ“áŒˆáˆ¨á‹ áˆ³á‹­áˆ†áŠ• á‹‹áŒ‹ áŠ¨ááˆˆáŠ• á‹¨áˆáŠ“áˆ˜áŒ£á‹ áˆµáˆˆáˆ†áŠ á‹­áˆ… áŒ‰á‰£áŠ¤ áŠ¨á‹á‹­á‹­á‰µ á‰£áˆ»áŒˆáˆ­ á‰ á‰°áŒá‰£áˆ­ áŒ­áˆáˆ­ á‹¨áˆ°áˆ‹áˆ á‰°áˆáˆ³áˆŒá‰µ áˆ˜áˆ†áŠ• áŠ¥áŠ•á‹°áˆšáŒˆá‰£á‹ áˆˆáˆ›áˆ³áˆ°á‰¥ áŠ¥áŠ•á‹ˆá‹³áˆˆáŠ•á¢ \" (áˆ™áˆ‰ áˆ˜áˆá‹•áŠ­á‰³á‰¸á‹ áŠ¨áˆ‹á‹­ á‰°á‹«á‹­á‹Ÿáˆ)\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Data at index 890 after tokenization:  [14, 302, 43245, 42, 9, 2331, 37977, 512, 470, 85965, 464, 253, 26895, 82905, 15, 14, 20, 13326, 57, 413, 98, 60, 260, 169, 4331, 6235, 150, 117, 820, 10, 8, 413, 125, 44, 20762, 41831, 7207, 33, 49, 2386, 175, 3828, 1077, 8591, 11239, 13264, 4076, 4, 1, 1914, 341, 640, 1, 78, 4405, 1, 1914, 33, 824, 750, 33, 1272, 741, 33116, 976, 1712, 150, 3598, 70, 2416, 3762, 1418, 4528, 4776, 3102, 3686, 4076, 4, 1, 1914, 8056, 1537, 4, 1, 78, 1420, 9140, 341, 1534, 9197, 337, 3875, 9931, 455, 3299, 13326, 107, 713, 135, 29, 330, 78447, 3799, 34, 14, 302, 550, 737, 461, 13, 7905, 4, 82566, 11, 57955, 23146, 35, 5899, 12034, 6409, 40, 38, 13, 242, 7, 550, 6417, 11, 26756, 277, 12721, 655, 5382, 6, 41824, 69299, 32, 302, 1205, 459, 1059, 2044, 722, 35, 302, 5382, 55, 228, 19167, 228, 472, 610, 18291, 998, 30048, 95209, 32, 334, 406, 17334, 25171, 16, 457, 26524, 1762, 23, 9930, 35, 5276, 9, 2199, 260, 334, 13, 458, 76, 464, 13472, 22, 382, 459, 5, 1552, 79770, 1910, 302, 6291, 353, 4, 12544, 8, 17829, 51736, 13, 4, 90352, 9, 12318, 2654, 406, 18296, 21698, 52377, 40, 6, 37647, 2381, 55, 162, 5, 17892, 13, 11472, 12, 36295, 6815, 13053, 317, 253, 353, 642, 182, 35, 3748, 586, 18878, 218, 4, 1, 302, 532, 5930, 20707, 204, 220, 36963, 4, 1, 413, 82240, 6291, 353, 51140, 36963, 4, 1, 26825, 4445, 1, 7663, 67691, 6, 5, 78, 1, 25914, 17, 352, 39806, 937, 413, 2805, 27495, 36749, 23030, 353, 4018, 36594, 15010, 2612, 36501, 6, 88354, 2902, 260, 4018, 4, 24, 6308, 9, 36501, 6, 94522, 353, 10, 11, 55032, 1431, 1064, 8, 4018, 74659, 1286, 77950, 10479, 16, 10, 9, 16, 1160, 3091, 25914, 9, 4908, 17, 302, 397, 12955, 81805, 204, 5, 35, 30394, 64, 73782, 10677, 210, 58238, 13, 5301, 97, 197, 81257, 4822, 586, 936, 17534, 66722, 13, 3045, 97, 12996, 4018, 3755, 9326, 44290, 3145, 1114, 23, 2523, 16629, 69, 38, 8, 413, 226, 5, 99213, 4793, 47, 9, 90, 21116, 84, 4018, 30556, 14552, 5314, 2957, 67967, 3232, 97, 75389, 4144, 13, 2334, 1113, 7594, 7, 107, 778, 856, 5061, 1593, 30894, 9460, 678, 17474, 5179, 1263, 302, 43245, 42, 9, 2331, 37977, 512, 470, 85965, 464, 253, 26895, 82905, 1149, 103, 552, 12, 41082, 2231, 4329, 976, 413, 10344, 397, 7504, 21800, 11085, 14, 29, 4545, 18864, 624, 1510, 1, 73, 34]\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "Data at index 890 after tokenization: ['â–\"', 'â–áˆ°áˆ‹áˆ', 'â–áŠ¨áŠ áŠ•áŒˆá‰µ', 'â–á‰ áˆ‹á‹­', 'áŠ“', 'â–á‹áˆ', 'â–áˆ‹áˆˆáˆ›', 'áˆˆá‰µ', 'â–á‹«áˆ…áˆ', 'â–á‹¨áˆáŠ•áŠ“áŒˆáˆ¨á‹', 'â–áˆ³á‹­áˆ†áŠ•', 'â–á‹‹áŒ‹', 'â–áŠ¨ááˆˆáŠ•', 'â–á‹¨áˆáŠ“áˆ˜áŒ£á‹', 'â–áŠá‹', 'â–\"', 'â–-', 'â–á‰…á‹±áˆµáŠá‰³á‰¸á‹', 'â–á‹›áˆ¬', 'â–á‹¨áˆ°áˆ‹áˆ', 'â–áˆšáŠ’áˆµá‰´áˆ­', 'â–áŠ áŠ•á‹µ', 'â–á‹“áˆˆáˆ', 'â–áŠ á‰€á', 'â–áŠ®áŠ•áˆáˆ¨áŠ•áˆµ', 'â–áŠ á‹˜áŒ‹áŒ…á‰¶', 'â–áŠá‰ áˆ­á¢', 'â–á‰ á‹šáˆ…', 'â–áˆ˜á‹µáˆ¨áŠ­', 'â–áˆ‹á‹­', 'áˆ', 'â–á‹¨áˆ°áˆ‹áˆ', 'â–áˆšáŠ’áˆµá‰µáˆ­', 'â–áŠ á‰¶', 'â–á‰¥áŠ“áˆá', 'â–áŠ á‹±', 'á‹“áˆˆáˆ', 'â–á£', 'â–á‹¨áŠ¢á‰µá‹®áŒµá‹«', 'â–á‹¨áˆƒá‹­áˆ›áŠ–á‰µ', 'â–á‰°á‰‹áˆ›á‰µ', 'â–á‹¨á‰ áˆ‹á‹­', 'â–áŒ á‰£á‰‚', 'â–áŠ á‰£á‰¶á‰½á£', 'â–á‰¥áá‹“áŠ•', 'â–áŠ á‰ á‹', 'â–áˆŠá‰ƒáŠ', 'â–', 'áŒ³áŒ³', 'áˆ³á‰µ', 'â–á‹ˆ', 'áŠ¤', 'áŒ²', 'áˆµ', 'â–á‰†', 'áŒ¶', 'áˆ³á‰µ', 'â–á£', 'â–á‹¨áˆ˜áŠ•áŒáˆ¥á‰µ', 'â–á‰£áˆˆáˆµáˆáŒ£áŠ“á‰µ', 'â–á£', 'â–áŠ á‰£', 'áˆ³', 'á‹°áˆ®á‰½', 'â–áŒ­áˆáˆ­', 'â–á‰°áŒˆáŠá‰°á‹', 'â–áŠá‰ áˆ­á¢', 'â–á‰ áˆ˜á‹µáˆ¨áŠ©', 'â–á¤', 'â–á‰¥áá‹•', 'â–á‹ˆá‰…á‹±áˆµ', 'â–áŠ á‰¡áŠ', 'â–áˆ›á‰µá‹«áˆµ', 'â–á‰€á‹³áˆ›á‹Š', 'â–á“á‰µáˆ­á‹«áˆ­áŠ­', 'â–áˆ­áŠ¥áˆ°', 'â–áˆŠá‰ƒáŠ', 'â–', 'áŒ³áŒ³', 'áˆ³á‰µ', 'â–á‹˜áŠ¢á‰µá‹®áŒµá‹«', 'â–áˆŠá‰€', 'â–', 'áŒ³áŒ³', 'áˆµ', 'â–á‹˜', 'áŠ áŠ­áˆ±áˆ', 'â–á‹ˆ', 'áŠ¥', 'áŒ¨áŒŒ', 'â–á‹˜áˆ˜áŠ•', 'á‰ áˆ¨', 'â–á‰°áŠ­áˆˆáˆƒá‹­áˆ›áŠ–á‰µ', 'â–áˆ˜áˆá‹•áŠ­á‰µ', 'â–áŠ áˆµá‰°áˆ‹áˆáˆá‹‹áˆá¢', 'â–á‰…á‹±áˆµáŠá‰³á‰¸á‹', 'â–áˆáŠ•', 'â–áŠ áˆ‰', 'â–?', 'â–(', 'áŠ¨', 'áˆ˜áˆá‹•áŠ­á‰³á‰¸á‹', 'â–á‹¨á‰°á‹ˆáˆ°á‹°', ')', 'â–\"', 'â–áˆ°áˆ‹áˆ', 'â–á‹¨áˆ°á‹', 'â–áˆáŒ†á‰½', 'â–ááˆ‹áŒŽá‰µ', 'á£', 'â–á‹¨á‰¥á‹™', 'â–', 'áˆáŠ•á‹±á‰£áŠ•', 'â–á‹¨', 'á‹¨á‹•áˆˆá‰µ', 'â–áŠ“áá‰†á‰µ', 'â–áŠá‹á¢', 'â–á‹¨á‰ áˆ­áŠ«á‰³', 'â–á‹˜áˆ˜áŠ“á‰µ', 'â–á‰…áˆ­áˆ¶á‰½', 'á¤', 'â–áŒŠá‹œ', 'á£', 'â–áŒˆáŠ•á‹˜á‰¥', 'â–áŠ¥áŠ“', 'â–á‹¨áˆ°á‹', 'â–áŒ‰áˆá‰ á‰µ', 'â–á‹¨', 'áˆáˆ°áˆ°', 'á‰£á‰¸á‹', 'â–áŒáŠ•á‰£á‰³á‹Žá‰½', 'â–á‰ áˆ°áˆ‹áˆ', 'â–áˆ›áŒ£á‰µ', 'â–á‰ ', 'á‰…áŒ½á‰ á‰µ', 'â–á‹­áˆáˆ­áˆ³áˆ‰', 'á¢', 'â–áˆ°áˆ‹áˆ', 'â–áŠ«áˆˆ', 'â–á‹¨á‹“áˆˆáˆ', 'â–áˆ€á‰¥á‰µ', 'â–áˆˆáˆáˆ‰áˆ', 'â–á‰ á‰‚', 'â–áŠá‹á¢', 'â–áˆ°áˆ‹áˆ', 'â–áˆ›áŒ£á‰µ', 'â–áŒáŠ•', 'â–á‰¥á‹™', 'â–áˆ áˆ«á‹Šá‰µá£', 'â–á‰¥á‹™', 'â–á‹¨áŒ¦áˆ­', 'â–áˆ˜áˆ³áˆªá‹«', 'â–áŠ¥áŠ•á‹²á‹˜áŒ‹áŒ…', 'â–áŠ¥á‹«á‹°áˆ¨áŒˆ', 'â–áˆ€á‰¥á‰µáŠ•', 'â–á‹«á‹ˆá‹µáˆ›áˆ', 'á¢', 'â–áŒ¦áˆ­áŠá‰µ', 'â–áˆ›áˆˆá‰µ', 'â–áˆ€á‰¥á‰µáŠ“', 'â–áˆ•á‹­á‹ˆá‰µáŠ•', 'â–á‹ˆá‹°', 'áˆš', 'áŠá‹µ', 'â–áŠ¥áˆ³á‰µ', 'â–á‹áˆµáŒ¥', 'â–áˆ˜áŒ£áˆ', 'â–áŠá‹á¢', 'â–á‹¨áŠ áŠ•á‹°áŠ›', 'áŠ“', 'â–á‹¨áˆáˆˆá‰°áŠ›', 'â–á‹“áˆˆáˆ', 'â–áŒ¦áˆ­áŠá‰µ', 'á£', 'â–á‰³áˆªáŠ­', 'â–á‰¥á‰»', 'â–áˆ³á‹­áˆ†áŠ•', 'â–áŒ á‰£áˆ³', 'á‹', 'â–áŠ áˆáŠ•áˆ', 'â–á‹¨á‹“áˆˆáˆ', 'áŠ•', 'â–áˆ˜áˆáŠ­', 'â–áŠ á‰ áˆ‹áˆ½á‰¶', 'á‰³áˆá¢', 'â–áˆ°áˆ‹áˆ', 'â–á‰ á‹áˆµáŒ¥', 'á‹‹', 'â–', 'áŒˆáˆ«', 'áˆ', 'áŠá‰µá£', 'â–á‰µá‹•áŒáˆ¥á‰µ', 'á£', 'â–', 'á‰³á‹›á‹¥áŠá‰µ', 'áŠ“', 'â–á‰ á‰µáˆ…á‰µáŠ“', 'â–á‹á‰…', 'â–áˆ›áˆˆá‰µ', 'â–áˆµáˆˆáˆšáŒˆáŠ™', 'â–áˆ˜áˆ«áˆ«', 'â–á‰µáˆ˜áˆµáˆ‹áˆˆá‰½', 'á¤', 'â–á‰ ', 'á‹áŒ¤', 'á‰·', 'â–áŒáŠ•', 'â–áˆ€áŒˆáˆ­', 'áŠ•', 'â–áŠ¨áŒ¥á‹á‰µ', 'á£', 'â–áˆ•á‹á‰¥áŠ•', 'â–áŠ¨', 'áˆ˜áŠ¨áˆ«', 'â–áˆ›á‰µáˆ¨á', 'â–á‹¨áˆšá‰»áˆ', 'â–á‰ áˆ˜áˆ†áŠ‘', 'â–á‹‹áŒ‹', 'á‹‹', 'â–áŠ¨á', 'â–á‹«áˆˆ', 'â–áŠá‹á¢', 'â–á‰…á‹µáˆµá‰µ', 'â–á‰¤á‰°', 'â–áŠ­áˆ­áˆµá‰²á‹«áŠ“á‰½áŠ•', 'â–á¦', 'â–', 'Â°', 'â–áˆ°áˆ‹áˆ', 'â–á‹¨áˆ†áŠá‹', 'â–áŠ­áˆ­áˆµá‰¶áˆµ', 'â–á‹¨áˆšáˆ°', 'á‰ ', 'áŠ­', 'á‰£á‰µá£', 'â–', 'Â°', 'â–á‹¨áˆ°áˆ‹áˆ', 'â–áˆ˜áˆáŠ¥áŠ­á‰°áŠžá‰½', 'â–á‰ á‹áˆµáŒ¥', 'á‹‹', 'â–á‹¨áˆšáˆ˜áˆ‹áˆˆáˆ±', 'á‰£á‰µá£', 'â–', 'Â°', 'â–á‰ áŒá‰¥áˆ¨', 'â–áŠƒ', 'áŒ¢', 'áŠ á‰µ', 'â–á‹¨á‹ˆá‹°á‰á‰µ', 'â–á‰ ', 'áŠ•', 'áˆµ', 'áˆ“', 'â–áŠ¨áŠ¥áŒá‹šáŠ á‰¥áˆ”áˆ­', 'â–áŒ‹áˆ­', 'â–á‹¨áˆš', 'á‰³áˆ¨á‰', 'á‰£á‰µ', 'â–á‹¨áˆ°áˆ‹áˆ', 'â–á‹µáˆá‹µá‹­', 'â–áˆµáˆˆáˆ†áŠá‰½', 'â–á‰ áˆ¥áˆ­á‹“á‰°', 'â–á‰…á‹³áˆ´', 'á‹‹', 'â–áˆ°áˆ‹áˆáŠ•', 'â–á‹°áŒ‹áŒáˆ›', 'â–á‰³á‹', 'áŒƒ', 'áˆˆá‰½á¤', 'â–á‰ ', 'áŒ¸áˆŽá‰·', 'â–áˆˆáˆ˜áˆ‹á‹', 'â–á‹“áˆˆáˆ', 'â–áˆ°áˆ‹áˆáŠ•', 'â–', 'á‰µ', 'áˆˆáˆ', 'áŠ“', 'áˆˆá‰½á¤', 'â–á‰ ', 'áŒ‰áˆáˆ‹á‰µ', 'á‹‹', 'â–áˆ‹á‹­', 'â–á‹¨', 'áˆ°á‰€áˆˆ', 'á‰½á‹', 'â–áˆ˜áˆµá‰€áˆ', 'áˆ', 'â–áˆ°áˆ‹áˆáŠ•', 'â–á‹¨áˆšáˆ°á‰¥áŠ­', 'â–áŠá‹á¤', 'â–á‹¨áˆ˜áˆµá‰€áˆ‰', 'â–á‰…áˆ­á…', 'â–á‹ˆá‹°', 'â–áˆ‹á‹­', 'áŠ“', 'â–á‹ˆá‹°', 'â–áŒŽáŠ•', 'â–áˆ˜áˆ†áŠ‘áˆ', 'â–áŠ¨áŠ¥áŒá‹šáŠ á‰¥áˆ”áˆ­', 'áŠ“', 'â–áŠ¨áˆ°á‹', 'â–áŒ‹áˆ­', 'â–áˆ°áˆ‹áˆ', 'â–áˆ˜áˆ†áŠ•', 'â–áŠ¥áŠ•á‹³áˆˆá‰¥áŠ•', 'â–á‹¨áˆšá‹«áˆµáŒˆáŠá‹', 'á‰ ', 'áŠ•', 'â–áŠá‹á¢', 'â–á‰³áˆªáŠ«á‰½áŠ•', 'â–áŠ¥áŠ•á‹°', 'áˆšáŠáŒáˆ¨áŠ•', 'â–á‹ˆáŠ•á‹µáˆ›áˆ›á‰¾á‰½', 'â–áˆ²', 'áŒ‹á‹°áˆ‰', 'á£', 'â–á‰ áˆ•á‹á‰¥', 'â–áˆ˜áŠ«áŠ¨áˆ', 'â–áˆ˜', 'á‰°áˆ‹áˆˆá‰…', 'â–áˆ²áˆ˜áŒ£', 'â–á‰¤á‰°', 'â–áŠ­áˆ­áˆµá‰²á‹«áŠ•', 'â–á‰³á‰¦á‰µ', 'â–áŠ áŠ­á‰¥áˆ«', 'á£', 'â–á‰ áŠ¥áˆ³á‰µ', 'â–áˆ˜áŠ«áŠ¨áˆ', 'â–áŒˆá‰¥á‰³', 'â–áˆ°áˆ‹áˆáŠ•', 'â–áˆµá‰³', 'á‹ˆáˆ­á‹µ', 'â–á‹¨áŠ–áˆ¨á‰½', 'â–áŠ“á‰µá¢', 'â–á‰ áˆ€áŒˆáˆ­', 'â–á‹áˆµáŒ¥', 'â–áŒáŒ­á‰¶á‰½', 'â–á‰ á‰°áˆáŒ áˆ©', 'á‰ á‰µ', 'â–áŒŠá‹œ', 'áˆ', 'â–á‹¨áˆ°áˆ‹áˆ', 'â–áŒ¥áˆª', 'áŠ•', 'â–á‹«áˆ‹áˆµá‰°áˆ‹áˆˆáˆ', 'á‰½á‰ á‰µ', 'â–á‰€áŠ•', 'áŠ“', 'â–áˆ°á‹“á‰µ', 'â–áŠ á‹­áŒˆáŠáˆ', 'á¡á¡', 'â–áˆ°áˆ‹áˆáŠ•', 'â–á‹¨áˆ˜á‹ˆá‹«á‹«', 'â–áˆ­áŠ¥áˆµ', 'â–áŠ á‹µáˆ­áŒˆáŠ•', 'â–áˆµáŠ•', 'áˆ°á‰£áˆµá‰¥', 'â–á‰ áŒ¦áˆ­áŠá‰µ', 'â–áˆ˜áŠ«áŠ¨áˆ', 'â–á‹¨á‰°áŒ¨áŠá‰', 'â–áˆ•á‹á‰¦á‰½', 'á£', 'â–áˆ«áˆ³á‰¸á‹áŠ•', 'â–áˆ˜áŠ¨áˆ‹áŠ¨áˆ', 'â–á‹¨áˆ›á‹­á‰½áˆ‰', 'â–áŠ¥áŠ“', 'â–áˆáŠ•', 'â–áŠ¥á‹¨á‰°á‹°áˆ¨áŒˆ', 'â–áŠ¥áŠ•á‹³áˆˆ', 'â–á‰ á‹áˆ', 'â–á‹¨áˆ›á‹­', 'áŒˆáŠá‹˜á‰¡', 'â–á‹°áŠ«áˆžá‰½', 'â–á‰°áˆµá‹', 'â–á‹«á‹°áˆ­áŒ‰', 'áŠ“áˆá¢', 'â–áˆµáˆˆá‹šáˆ…', 'â–áˆ°áˆ‹áˆ', 'â–áŠ¨áŠ áŠ•áŒˆá‰µ', 'â–á‰ áˆ‹á‹­', 'áŠ“', 'â–á‹áˆ', 'â–áˆ‹áˆˆáˆ›', 'áˆˆá‰µ', 'â–á‹«áˆ…áˆ', 'â–á‹¨áˆáŠ•áŠ“áŒˆáˆ¨á‹', 'â–áˆ³á‹­áˆ†áŠ•', 'â–á‹‹áŒ‹', 'â–áŠ¨ááˆˆáŠ•', 'â–á‹¨áˆáŠ“áˆ˜áŒ£á‹', 'â–áˆµáˆˆáˆ†áŠ', 'â–á‹­áˆ…', 'â–áŒ‰á‰£áŠ¤', 'â–áŠ¨', 'á‹á‹­á‹­á‰µ', 'â–á‰£áˆ»áŒˆáˆ­', 'â–á‰ á‰°áŒá‰£áˆ­', 'â–áŒ­áˆáˆ­', 'â–á‹¨áˆ°áˆ‹áˆ', 'â–á‰°áˆáˆ³áˆŒá‰µ', 'â–áˆ˜áˆ†áŠ•', 'â–áŠ¥áŠ•á‹°áˆšáŒˆá‰£á‹', 'â–áˆˆáˆ›áˆ³áˆ°á‰¥', 'â–áŠ¥áŠ•á‹ˆá‹³áˆˆáŠ•á¢', 'â–\"', 'â–(', 'áˆ™áˆ‰', 'â–áˆ˜áˆá‹•áŠ­á‰³á‰¸á‹', 'â–áŠ¨áˆ‹á‹­', 'â–á‰°á‹«á‹­', 'á‹Ÿ', 'áˆ', ')']\n"
     ]
    }
   ],
   "source": [
    "index=890\n",
    "print(f\"Data at index {index} before tokenization:\", cleaned_data[index])\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(f\"Data at index {index} after tokenization: \", tokenizer.encode(cleaned_data[index]))\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(f\"Data at index {index} after tokenization:\", tokenizer.encode_as_pieces(cleaned_data[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFg5iFkpDnqr"
   },
   "source": [
    "To pretrain our Transformer network, we will use the Masked Language Model (MLM) approach. This technique involves randomly masking a percentage of words in a sentence and replacing them with special tokens. The model then attempts to predict these masked words, enabling it to learn contextual and semantic representations effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J_ml4H1Dnqr"
   },
   "source": [
    "I will be implementing the Masked language model (MLM) as shown in the following image.\n",
    "\n",
    "<img src = \"images/losses.png\" width=\"600\" height = \"400\">\n",
    "\n",
    "Assume you have the following text: <span style = \"color:blue\"> **áˆ°áˆ‹áˆ <span style = \"color:red\">á‹¨áˆ°á‹ áˆáŒ†á‰½ </span> ááˆ‹áŒŽá‰µá£ á‹¨á‰¥á‹™ áˆáŠ•á‹±á‰£áŠ• á‹¨á‹¨á‹•áˆˆá‰µ <span style = \"color:red\">áŠ“áá‰†á‰µ</span>  áŠá‹á¢** </span>     \n",
    "\n",
    "\n",
    "Now as input you will mask the words in red in the text:\n",
    "\n",
    "<span style = \"color:blue\"> **Input:**</span> áˆ°áˆ‹áˆ  **X** ááˆ‹áŒŽá‰µá£ á‹¨á‰¥á‹™ áˆáŠ•á‹±á‰£áŠ• á‹¨á‹¨á‹•áˆˆá‰µ **Y** áŠá‹á¢\n",
    "\n",
    "<span style = \"color:blue\">**Output:**</span> The model should predict the words(s) for **X** and **Y**.\n",
    "\n",
    "**[EOS]** will be used to mark the end of the target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuSshSDvDnqr"
   },
   "source": [
    "As you can see above, I were able to take a piece of string and tokenize it.\n",
    "\n",
    "Now I will create `input` and `target` pairs that will allow me to pre-train the model. The model uses the ids at the end of the vocab file as sentinels. For example, it will replace:\n",
    "   - `vocab_size - 1` by `<Z>`\n",
    "   - `vocab_size - 2` by `<Y>`\n",
    "   - and so forth.\n",
    "   \n",
    "It assigns every word a `chr`.\n",
    "\n",
    "The `pretty_decode` function below, which I will use in a bit, helps in handling the type when decoding.\n",
    "\n",
    "Notice that:\n",
    "```python\n",
    "string.ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740393361138,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "RFcxrZhwDnqr"
   },
   "outputs": [],
   "source": [
    "def get_sentinels(tokenizer, display=False):\n",
    "    sentinels = {}\n",
    "    vocab_size = tokenizer.vocab_size()\n",
    "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
    "        decoded_text = tokenizer.detokenize([vocab_size - i])\n",
    "\n",
    "        # Sentinels, ex: <Z> - <a>\n",
    "        sentinels[decoded_text] = f'<{char}>'\n",
    "\n",
    "        if display:\n",
    "            print(f'The sentinel is <{char}> and the decoded token is:', decoded_text)\n",
    "\n",
    "    return sentinels\n",
    "\n",
    "\n",
    "def pretty_decode(encoded_str_list, sentinels, tokenizer):\n",
    "    if isinstance(encoded_str_list, str):\n",
    "        for token, char in sentinels.items():\n",
    "            encoded_str_list = re.sub(re.escape(token), char, encoded_str_list)\n",
    "        return encoded_str_list\n",
    "\n",
    "    return pretty_decode(tokenizer.detokenize(encoded_str_list), sentinels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1740393361297,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "-3alnynSDnqr",
    "outputId": "dd0133f6-7061-49a2-8790-18dcadd6030f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentinel is <Z> and the decoded token is: áŒ‚á‹®á–áˆŠá‰³áŠ•\n",
      "The sentinel is <Y> and the decoded token is: áŠ®áˆŒáŒƒá‰½áŠ•\n",
      "The sentinel is <X> and the decoded token is: á‰¥áˆ®áŠ•á‹\n",
      "The sentinel is <W> and the decoded token is: áŠ®áŠ•á‰²áŠ”\n",
      "The sentinel is <V> and the decoded token is: áŠ áˆáŒááŒˆ\n",
      "The sentinel is <U> and the decoded token is: áŠ á‹­áˆáˆ®áŠ á‰½áŠ•\n",
      "The sentinel is <T> and the decoded token is: á‹¨á‰°á‹ˆáŒ áˆ¨\n",
      "The sentinel is <S> and the decoded token is: á‹«áŠ«á‰¥á‰±\n",
      "The sentinel is <R> and the decoded token is: á‹­á‰ áˆ¨á‰³áˆ\n",
      "The sentinel is <Q> and the decoded token is: áŒ‚áŠ áŠ•áŒ\n",
      "The sentinel is <P> and the decoded token is: á‹«áˆµá‰½áˆ‰á‰³áˆ\n",
      "The sentinel is <O> and the decoded token is: ,744\n",
      "The sentinel is <N> and the decoded token is: á‹ˆáŒ¤á‰³áˆ›\n",
      "The sentinel is <M> and the decoded token is: áŠ¥áˆ°áŒ£á‰½áŠ‹áˆˆáˆ\n",
      "The sentinel is <L> and the decoded token is: á‹«áˆ³áˆáŠáŠ\n",
      "The sentinel is <K> and the decoded token is: á‰ á‰µáˆáˆ­á‰µ\n",
      "The sentinel is <J> and the decoded token is: á‹«áˆµáˆáˆáŒ‹á‰½áŠ‹áˆ\n",
      "The sentinel is <I> and the decoded token is: áŠ áˆáˆ†áŠáˆáŠáˆ\n",
      "The sentinel is <H> and the decoded token is: á‰µáŒ áˆá‰…\n",
      "The sentinel is <G> and the decoded token is: áŒ áˆáŒ á\n",
      "The sentinel is <F> and the decoded token is: áŠ áŒ½áˆ›á‰¸á‹\n",
      "The sentinel is <E> and the decoded token is: á‰¥á‰µá‹ˆá‹µá‰…\n",
      "The sentinel is <D> and the decoded token is: áˆ›áˆµáˆá‰°áŠ“á‰¸á‹\n",
      "The sentinel is <C> and the decoded token is: áŒ‹áˆ¨á‹°á‰½\n",
      "The sentinel is <B> and the decoded token is: áŠ®áŠ•á‰²áŠáˆ­\n",
      "The sentinel is <A> and the decoded token is: áŠ áˆµá‰°áŠá‰³\n",
      "The sentinel is <z> and the decoded token is: áŠ¥áŠ•á‹µá‰³áŠ¨á‰¥áˆ©\n",
      "The sentinel is <y> and the decoded token is: áˆ¥áˆáŒ¤\n",
      "The sentinel is <x> and the decoded token is: á‹¨áˆˆáˆ¾á‰½\n",
      "The sentinel is <w> and the decoded token is: á‹¨áˆ°á‰ áˆ°á‰ á‰½á‹\n",
      "The sentinel is <v> and the decoded token is: áŒáˆ­á‹á‰±\n",
      "The sentinel is <u> and the decoded token is: á‰€áˆ‹á‰…áˆˆá‹‹áˆ\n",
      "The sentinel is <t> and the decoded token is: áˆ˜áˆˆáŒ áŒ¥\n",
      "The sentinel is <s> and the decoded token is: áŠ áˆá‰€áˆ­áˆ\n",
      "The sentinel is <r> and the decoded token is: á‹áŒ¥áŠ–á‰½\n",
      "The sentinel is <q> and the decoded token is: á‰ áˆšá‹«áˆµáŠ¬á‹°\n",
      "The sentinel is <p> and the decoded token is: áŠ¥á‹¨á‰€áˆ¨áˆ\n",
      "The sentinel is <o> and the decoded token is: áˆ°á‰ áˆ°á‰§á‰¸\n",
      "The sentinel is <n> and the decoded token is: á‹«áŠ­á‰¥áˆ­áˆáŠ•\n",
      "The sentinel is <m> and the decoded token is: á‹¨á‹°á‰ á‹°á‰ á‹\n",
      "The sentinel is <l> and the decoded token is: á‹°á‰£áˆˆá‰\n",
      "The sentinel is <k> and the decoded token is: áˆ˜á‰¥á‰¶á‰»á‰½áŠ•\n",
      "The sentinel is <j> and the decoded token is: á‰µáˆ…áˆáˆ­á‰µ\n",
      "The sentinel is <i> and the decoded token is: áŠ«áˆáˆ°á‰ \n",
      "The sentinel is <h> and the decoded token is: á‹²á•áˆ¬áˆ½áŠ•\n",
      "The sentinel is <g> and the decoded token is: á‰°á‰†áˆ¨áˆ°\n",
      "The sentinel is <f> and the decoded token is: á‰ áˆ›áŒ£á‰´\n",
      "The sentinel is <e> and the decoded token is: 60.88\n",
      "The sentinel is <d> and the decoded token is: áˆšá‹«á‹‹áŒ¡á‰µ\n",
      "The sentinel is <c> and the decoded token is: á‰áˆˆáˆ­\n",
      "The sentinel is <b> and the decoded token is: áŠ®áˆªá‹°áˆ©\n",
      "The sentinel is <a> and the decoded token is: áŠ¥á‹¨á‰°á‰€áŠáˆ°\n"
     ]
    }
   ],
   "source": [
    "sentinels = get_sentinels(tokenizer, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0w0c-H1Dnqr"
   },
   "source": [
    "Now, let's use the `pretty_decode` function in the following sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7T42JCfDnq2"
   },
   "source": [
    "<a name='1-5'></a>\n",
    "### 3.4 - Tokenizing and Masking\n",
    "\n",
    "In this task, I will implement the `tokenize_and_mask` function, which tokenizes and masks input words based on a given probability. The probability is controlled by the `noise` parameter, typically set to mask around `15%` of the words in the input text. The function will generate two lists of tokenized sequences following the algorithm outlined below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoeVutTjDnq2"
   },
   "source": [
    "\n",
    "###  tokenize_and_mask\n",
    "\n",
    "- Start with two empty lists: `inps` and `targs`\n",
    "- Tokenize the input text using the given tokenizer.\n",
    "- For each `token` in the tokenized sequence:\n",
    "  - Generate a random number(simulating a weighted coin toss)\n",
    "  - If the random value is greater than the given threshold(noise):\n",
    "    - Add the current token to the `inps` list\n",
    "  - Else:\n",
    "    - If a new sentinel must be included:\n",
    "      - Compute the next sentinel ID using a progression.\n",
    "      - Add a sentinel into the `inps` and `targs` to mark the position of the masked element.\n",
    "    - Add the current token to the `targs` list.\n",
    "\n",
    "** There's a special case to consider. If two or more consecutive tokens get masked during the process, no need to add a new sentinel to the sequences. To account for this, use the `prev_no_mask` flag, which starts as `True` but is turned to `False` each time I mask a new element. The code that adds sentinels will only be executed if, before masking the token, the flag was in the `True` state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1740393361789,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "-nDh8QySDnq2"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_mask(text,\n",
    "                      noise=0.15,\n",
    "                      randomizer=np.random.uniform,\n",
    "                      tokenizer=None):\n",
    "    \"\"\"Tokenizes and masks a given input.\n",
    "\n",
    "    Args:\n",
    "        text (str or bytes): Text input.\n",
    "        noise (float, optional): Probability of masking a token. Defaults to 0.15.\n",
    "        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.\n",
    "        tokenizer (function, optional): Tokenizer function. Defaults to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        inps, targs: Lists of integers associated to inputs and targets.\n",
    "    \"\"\"\n",
    "\n",
    "    cur_sentinel_num = 0\n",
    "    inps, targs = [], []\n",
    "    vocab_size = int(tokenizer.vocab_size())\n",
    "    eos = tokenizer.piece_to_id(\"</s>\")\n",
    "    prev_no_mask = True\n",
    "    \n",
    "    for token in tokenizer.tokenize(text):\n",
    "        rnd_val = randomizer()\n",
    "        \n",
    "        if noise > rnd_val:\n",
    "            if prev_no_mask:\n",
    "                cur_sentinel_num += 1\n",
    "                end_id = vocab_size - cur_sentinel_num\n",
    "                targs.append(end_id)\n",
    "                inps.append(end_id)\n",
    "            targs.append(token)\n",
    "            prev_no_mask = False\n",
    "\n",
    "        else:\n",
    "            inps.append(token)\n",
    "            prev_no_mask = True\n",
    "    targs.append(eos)\n",
    "\n",
    "    return inps, targs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sveFiDohDnq2"
   },
   "source": [
    "I will now take random value from the cleaned_data and pass it to `tokenize_and_mask` function and see how it randomly masks and separate inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1740393362154,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "VUcwzgGxDnq2",
    "outputId": "828cbf6f-d9b8-47be-b65f-61b3f8cde3e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random data before Masking : \n",
      "\n",
      " á‹¨á€áŒ¥á‰³ áˆáŠ”á‰³á‹ áŠ áˆµá‰°áˆ›áˆ›áŠ áŠá‹~áŒ…áŒáŒ…áŒ‹! . . áˆµáˆáŠ•á‰°áŠ›á‹áŠ• á‹¨áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ áˆˆáˆ›áŠ«áˆ„á‹µ á‹áŒáŒ…á‰± áˆ˜áŒ áŠ“á‰€á‰áŠ•áŠ“ áˆáŠ•áˆ áŠ á‹­áŠá‰µ á‹¨á€áŒ¥á‰³ á‰½áŒáˆ­ áŠ áˆˆáˆ˜áŠ–áˆ©áŠ• á‹¨áŠ¨á‰°áˆ› áˆáˆ›á‰µáŠ“ áŠ®áŠ•áˆµá‰µáˆ«áŠ­áˆ½áŠ• áˆšáŠ’áˆµá‰´áˆ­ áˆšáŠ’áˆµá‰µáˆ­ á‹´áŠ¤á‰³ áŠ á‰¶ áŠ«áˆ³áˆáŠ• áŒŽáŒ á‰°áŠ“áŒˆáˆ©á¢ á‹áŒáŒ…á‰±áŠ• áŠ áˆµáˆ˜áˆáŠ­á‰¶ á‰ áŒ…áŒáŒ…áŒ‹ áˆ˜áŒáˆˆáŒ« á‹¨áˆ°áŒ¡á‰µ áˆšáŠ’áˆµá‰µáˆ­ á‹´áŠ¤á‰³á‹ áŒ…áŒ…áŒ‹ áˆ‹á‹­ á‹¨á€áŒ¥á‰³ áˆáŠ”á‰³á‹ áŠ áˆµá‰°áˆ›áˆ›áŠ áŠá‹\"á¤ á‹¨áŠ­áˆáˆ‰ á–áˆŠáˆµ áŠ¨áŒá‹´áˆ«áˆ á‹¨á€áŒ¥á‰³ áŠ áŠ«áˆ‹á‰µ áŒ‹áˆ­áˆ á‰ á‰…áŠ•áŒ…á‰µ áŠ¥á‹¨áˆ°áˆ« áŠá‹ á‰¥áˆˆá‹‹áˆá¢ á‰ áŠ¨á‰°áˆ›á‹‹ áŠ áˆµá‰°áˆ›áˆ›áŠ á‹¨á€áŒ¥á‰³ áˆáŠ”á‰³ áˆ˜áŠ–áˆ©áŠ• á‹¨áŒˆáˆˆáá‰µ áˆšáŠ’áˆµá‰µáˆ­ á‹´áŠ¤á‰³á‹ á‰°áˆ³á‰³áŠ áŠ¨á‰°áˆžá‰½ á‹ˆá‹°áŒ…áŒáŒ…áŒ‹ áŒˆá‰¥á‰°á‹ á‹áŒáŒ…á‰³á‰¸á‹áŠ• áŒ€áˆáˆ¨á‹‹áˆ á‰¥áˆˆá‹‹áˆá¢ á‹áŒáŒ…á‰¶á‰½áŠ• á‰ á‰°áˆ˜áˆˆáŠ¨á‰° áŠ¨áŠáŒˆ áŒ€áˆáˆ® áŠ¥áˆµáŠ¨ áˆ¨á‰¡á‹• áŠ¨áˆšáŠ–áˆ© á‹áŒáŒ…á‰¶á‰½ áˆ˜áŠ«áŠ¨áˆ á‰ áŒ…áŒáŒ…áŒ‹ áˆµá‰´á‹²á‹¨áˆ á‹¨áˆ˜áŠ­áˆá‰» áˆµáŠáˆµáˆ­á‹á‰µá£ á‰ áŠ¨á‰°áˆžá‰½ á‹¨áˆµáˆ« á‹•á‹µáˆ áˆáŒ áˆ« áŠ¢áŠ•á‰°áˆ­á•áˆ«á‹­á‹žá‰½ áˆáˆ›á‰µá£ á‰ áˆ˜áˆ¬á‰µ áˆáˆ›á‰µ áˆ›áŠ”áŒ…áˆ˜áŠ•á‰µá£ á‰ á‹˜áˆ­á‰ áŠ áŒ€áŠ•á‹³á‹Žá‰½ áˆ›áˆˆá‰µáˆ á‰ áŠ¨á‰°áˆ› áˆáˆ›á‰µá£ áŠ áˆ¨áŠ•áŒ“á‹´ áˆáˆ›á‰µáŠ“ áŠ áŠ«á‰£á‰¢ áŒ¥á‰ á‰ƒá£ á‰ á‰¤á‰¶á‰½áŠ“ áŠ®áŠ•áˆµá‰µáˆ«áŠ­áˆ½áŠ• áŠ¥áŠ•á‹²áˆáˆ á‰°á‹«á‹«á‹¥ á‹˜áˆ­áŽá‰½ á‹™áˆªá‹« áŒ¥áŠ“á‰³á‹Š á…áˆáŽá‰½ á‹­á‰€áˆ­á‰£áˆ‰á¢ áŠ á‰¶ áŠ«áˆ³áˆáŠ• áŠ¨á‰°áˆžá‰½ áˆ«áˆ³á‰¸á‹áŠ• á‹¨áˆšá‹«áˆµá‰°á‹‹á‹á‰á‰ á‰µ áŠ¤áŒá‹šá‰¢áˆ½áŠ• á‹¨áˆšáŠ«áˆ„á‹µ áˆ²áˆ†áŠ• áŠ¨á‹šáˆ… á‰ áŠá‰µ á‰ á‰°á‹°áˆ¨áŒ‰ áŽáˆ¨áˆžá‰½ á‹¨á‰°áˆµá‰°á‹‹áˆˆá‹ á‹¨á‹µáˆá… á‰¥áŠ­áˆˆá‰µ á‰ á‹šáˆ… áŠ áˆ˜á‰µ áŠ¥áŠ•á‹³á‹­áŠ–áˆ­ áŠ¨áŠ¨á‰°áˆžá‰½ áŒ‹áˆ­ áˆ˜áŒá‰£á‰£á‰µ áˆ‹á‹­ á‰°á‹°áˆ­áˆ·áˆ á‰¥áˆˆá‹‹áˆá¢ áˆ€áˆ™áˆµ á‰ áˆšáŠ–áˆ¨á‹ á‹¨áˆ›áŒ á‰ƒáˆˆá‹« áˆµáˆ­á‹“á‰µ áˆˆáˆžá‹´áˆ áŠ¢áŠ•á‰°áˆ­á•áˆ«á‹­á‹žá‰½á£ áˆˆáˆ´á‰µ áˆµáˆ« áˆáŒ£áˆªá‹Žá‰½á£ á‰ áˆáˆ‰áˆ áŠ­áˆáˆŽá‰½ áŠ«áˆ‰ á‹¨áˆ´áŠ­á‰°áˆ­ á‰°á‰‹áˆ›á‰µ á‰ áŠ áˆáƒá€áˆ á‰¥áˆáŒ« áˆ‹áŒˆáŠ™ áŠ¥áŠ•á‹²áˆáˆ áˆˆá‹©áŠ’á‰¨áˆ­áˆ²á‰² á‰°áˆ˜áˆ«á‰‚ áˆµáˆ« áˆáŒ£áˆªá‹Žá‰½ áŠ¥á‹á‰…áŠ“áŠ“ áˆ½áˆáˆ›á‰µ á‹­áˆ°áŒ£áˆ á‰°á‰¥áˆáˆá¢ á‰ á‰°áˆ˜áˆ³áˆ³á‹­ á‹¨á‹˜áŒ áŠáŠ›á‹ á‹¨áŠ¢á‰µá‹®áŒµá‹« áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ áŠ á‹˜áŒ‹áŒ… á‰ áŠ¥áˆˆá‰± á‹­á‹ á‹­á‹°áˆ¨áŒ‹áˆá¤ á‹¨á‹‹áŠ•áŒ« áˆ­áŠ­áŠ­á‰¥áˆ á‹­áŠ–áˆ«áˆ á‰¥áˆˆá‹‹áˆ áˆšáŠ’áˆµá‰µáˆ­ á‹´áŠ¤á‰³á‹á¢ áˆµáˆáŠ•á‰°áŠ›á‹áŠ• á‹¨áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ á‹¨á‰°áˆˆá‹¨ áˆˆáˆ›á‹µáˆ¨áŒ áŠ¨áˆ›áˆŒá‹¥á‹« áŠ áˆˆáˆáŠ á‰€á á‹¨áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ áˆáˆá‹µ á‰°á‹ˆáˆµá‹·áˆ á‹«áˆ‰á‰µ áŠ á‰¶ áŠ«áˆ³áˆáŠ• áŽáˆ¨áˆ™ áˆˆáˆ˜áŒ€áˆ˜áˆªá‹« áŒŠá‹œ á‰ á‰³á‹³áŒŠ áŠ­áˆáˆ áˆ˜áŠ«áˆ„á‹±áˆ áˆá‹© á‹«á‹°áˆ­áŒˆá‹‹áˆ á‰¥áˆˆá‹‹áˆá¢ á‹¨áˆ±áˆ›áˆŒ áŠ­áˆáˆ áŠ¨á‰°áˆ› áˆáˆ›á‰µáŠ“ áŠ®áŠ•áˆµá‰µáˆ«áŠ­áˆ½áŠ• á‰¢áˆ® áˆ€áˆ‹áŠ á‹¶/áˆ­ á‰ á‰ áŠ©áˆ‹á‰¸á‹ áŠ­áˆáˆ‰ áŽáˆ¨áˆ™áŠ• áˆˆáˆ›áˆµá‰°áŠ“áŒˆá‹µ á‹áŒáŒ…á‰±áŠ• áŠ áŒ áŠ“á‰‹áˆá¤ á‰°áˆ³á‰³áŠ áŠ¨á‰°áˆžá‰½áˆ á‹ˆá‹° áŒ…áŒáŒ…áŒ‹ áŠ¥á‹¨áŒˆá‰¡ áŠá‹ á‰¥áˆˆá‹‹áˆá¢ á‰¢áˆ® áˆ€áˆ‹áŠá‹ á‰ áŒ¥á‰‚á‰µ áŒáˆˆáˆ°á‰¦á‰½ áŠ¨á‹ˆáˆ«á‰µ á‰ áŠá‰µ á‰°áŠ¨áˆµá‰¶ á‹¨áŠá‰ áˆ¨á‹ á‰½áŒáˆ­ áŒˆá…á‰³á‰½áŠ•áŠ• áŠ á‰ áˆ‹áˆ½á‰¶ á‹¨áŠá‰ áˆ¨ á‰¢áˆ†áŠ•áˆ áŠ áˆáŠ• áŒáŠ• áˆáŠ•áˆ á‹¨á€áŒ¥á‰³áˆ á‹­áˆáŠ• á‹¨á‹°áˆ…áŠ•áŠá‰µ á‰½áŒáˆ­ á‹¨áˆˆáˆ á‰¥áˆˆá‹‹áˆá¢ á‹¶/áˆ­ áŠ á‰¥á‹±áˆáˆá‰³áˆ… áˆµáˆáŠ•á‰°áŠ›á‹ á‹¨áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ á‹¨áŠ­áˆáˆ‹á‰½áŠ•áŠ• á‰¥áˆŽáˆ á‹¨áŠ¨á‰°áˆ›á‰½áŠ•áŠ• áŠ áˆµá‰°áˆ›áˆ›áŠ áˆ°áˆ‹áˆ á‹¨áˆáŠ“áˆ¨áŒ‹áŒáŒ¥á‰ á‰µáŠ“ á‰ á‰°áŒá‰£áˆ­áˆ á‹¨áˆáŠ“áˆ³á‹­á‰ á‰µ áŠ¥áŠ•á‹²áˆ†áŠ• áˆ°áŠ áˆµáˆ« áˆ°áˆ­á‰°áŠ“áˆ á‹áŒ¤á‰±áŠ•áˆ áŠ¥á‹«á‹¨áŠ• áŠá‹ á‰¥áˆˆá‹‹áˆá¢ áˆµáˆáŠ•á‰°áŠ›á‹ á‹¨áŠ¢á‰µá‹®áŒµá‹« áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ áŠ¨á‹¨áŠ«á‰²á‰µ 9-14/2011 ''áˆ˜á‹°áˆ˜áˆ­ áˆˆáŠ¢á‰µá‹®áŒµá‹« áŠ¨á‰°áˆžá‰½ á‰¥áˆá…áŒáŠ“'' á‰ áˆšáˆ áˆ˜áˆª á‰ƒáˆ á‰ áŒ…áŒáŒ…áŒ‹ á‹­áŠ«áˆ„á‹³áˆá¢ áˆáŠ•áŒ­á¦\n"
     ]
    }
   ],
   "source": [
    "random_data=cleaned_data[32000]\n",
    "print(\"Random data before Masking : \\n\\n\", random_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1740393362212,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "uvPtbfVODnq2"
   },
   "outputs": [],
   "source": [
    "inps_sample,targs_sample=tokenize_and_mask(random_data,noise=0.15,randomizer=np.random.uniform,tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1740393362395,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "miCCmUR5Dnq2",
    "outputId": "9d2266cb-4bd1-4482-f04f-4a303b1f7e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "\n",
      " <Z> áˆáŠ”á‰³á‹ áŠ áˆµá‰°áˆ›áˆ›áŠ áŠá‹ â‡ áŒ…áŒáŒ…áŒ‹! . . áˆµáˆáŠ•á‰°áŠ›á‹áŠ• á‹¨áŠ¨á‰°áˆžá‰½<Y> áˆˆáˆ›áŠ«áˆ„á‹µ á‹áŒáŒ…á‰± áˆ˜áŒ áŠ“á‰€á‰áŠ•áŠ“ áˆáŠ•áˆ áŠ á‹­áŠá‰µ á‹¨á€áŒ¥á‰³<X> áŠ áˆˆáˆ˜áŠ–áˆ©áŠ• á‹¨áŠ¨á‰°áˆ› áˆáˆ›á‰µáŠ“ áŠ®áŠ•áˆµá‰µáˆ«áŠ­áˆ½áŠ• áˆšáŠ’áˆµá‰´áˆ­ áˆšáŠ’áˆµá‰µáˆ­ á‹´áŠ¤á‰³ áŠ á‰¶ áŠ«áˆ³áˆáŠ• áŒŽáŒ á‰°áŠ“áŒˆáˆ©á¢ á‹áŒáŒ…á‰±áŠ• áŠ áˆµáˆ˜áˆáŠ­á‰¶ á‰ áŒ…áŒáŒ…áŒ‹ áˆ˜áŒáˆˆáŒ« á‹¨áˆ°áŒ¡á‰µ áˆšáŠ’áˆµá‰µáˆ­ á‹´áŠ¤á‰³á‹ áŒ…áŒ…áŒ‹ áˆ‹á‹­ á‹¨á€áŒ¥á‰³ áˆáŠ”á‰³á‹ áŠ áˆµá‰°áˆ›áˆ›áŠ áŠá‹ <W>á¤ á‹¨áŠ­áˆáˆ‰ á–áˆŠáˆµ áŠ¨áŒá‹´áˆ«áˆ á‹¨á€áŒ¥á‰³ áŠ áŠ«áˆ‹á‰µ áŒ‹áˆ­áˆ á‰ á‰…áŠ•áŒ…á‰µ áŠ¥á‹¨áˆ°áˆ« áŠá‹ á‰¥áˆˆá‹‹áˆá¢ á‰ áŠ¨á‰°áˆ›á‹‹ áŠ áˆµá‰°áˆ›áˆ›áŠ á‹¨á€áŒ¥á‰³ áˆáŠ”á‰³ áˆ˜áŠ–áˆ©áŠ• <V> áˆšáŠ’áˆµá‰µáˆ­ á‹´áŠ¤á‰³á‹ á‰°áˆ³á‰³áŠ áŠ¨á‰°áˆžá‰½<U>áŒ…áŒáŒ…áŒ‹ áŒˆá‰¥á‰°á‹ á‹áŒáŒ…á‰³á‰¸á‹áŠ• áŒ€áˆáˆ¨á‹‹áˆ <T> á‹áŒáŒ…á‰¶á‰½áŠ• á‰ á‰°áˆ˜áˆˆáŠ¨á‰° áŠ¨áŠáŒˆ áŒ€áˆáˆ® áŠ¥áˆµáŠ¨ áˆ¨á‰¡á‹• áŠ¨áˆšáŠ–áˆ©<S> áˆ˜áŠ«áŠ¨áˆ á‰ áŒ…áŒáŒ…áŒ‹ áˆµá‰´á‹²á‹¨áˆ á‹¨áˆ˜áŠ­áˆá‰» <R>á‰µá£<Q> á‹¨áˆµáˆ« á‹•á‹µáˆ áˆáŒ áˆ« áŠ¢áŠ•á‰°áˆ­á•áˆ«á‹­á‹žá‰½ áˆáˆ›á‰µá£ á‰ áˆ˜áˆ¬á‰µ <P> áˆ›áŠ”áŒ…áˆ˜áŠ•á‰µá£ á‰ á‹˜áˆ­á‰<O> áˆ›áˆˆá‰µáˆ á‰ áŠ¨á‰°áˆ› áˆáˆ›á‰µá£ áŠ áˆ¨áŠ•áŒ“á‹´ áˆáˆ›á‰µáŠ“ áŠ áŠ«á‰£á‰¢ áŒ¥á‰ á‰ƒá£<N>áŠ“ áŠ®áŠ•áˆµá‰µáˆ«áŠ­áˆ½áŠ• áŠ¥áŠ•á‹²áˆáˆ <M> á‹˜áˆ­áŽá‰½ á‹™áˆªá‹« áŒ¥áŠ“á‰³á‹Š á…áˆáŽá‰½ á‹­á‰€áˆ­á‰£áˆ‰á¢ áŠ á‰¶ áŠ«áˆ³áˆáŠ• áŠ¨á‰°áˆžá‰½ áˆ«áˆ³á‰¸á‹áŠ• á‹¨áˆšá‹«áˆµá‰°á‹‹á‹á‰á‰ á‰µ áŠ¤áŒá‹šá‰¢áˆ½áŠ• á‹¨áˆšáŠ«áˆ„á‹µ áˆ²áˆ†áŠ• áŠ¨á‹šáˆ… á‰ áŠá‰µ á‰ á‰°á‹°áˆ¨áŒ‰<L> á‹¨á‰°áˆµá‰°á‹‹áˆˆá‹ á‹¨á‹µáˆá… á‰¥áŠ­áˆˆá‰µ á‰ á‹šáˆ… áŠ áˆ˜á‰µ áŠ¥áŠ•á‹³á‹­áŠ–áˆ­ <K> áŒ‹áˆ­ áˆ˜áŒá‰£á‰£á‰µ áˆ‹á‹­ á‰°á‹°áˆ­áˆ·áˆ á‰¥áˆˆá‹‹áˆá¢ áˆ€áˆ™áˆµ á‰  <J> á‹¨áˆ›áŒ á‰ƒáˆˆá‹« áˆµáˆ­á‹“á‰µ áˆˆáˆžá‹´áˆ áŠ¢áŠ•á‰°áˆ­á•áˆ«á‹­á‹žá‰½á£ áˆˆáˆ´á‰µ áˆµáˆ« áˆáŒ£áˆªá‹Žá‰½á£ á‰ áˆáˆ‰áˆ áŠ­áˆáˆŽá‰½ áŠ«áˆ‰ á‹¨áˆ´áŠ­á‰°áˆ­ á‰°á‰‹áˆ›á‰µ á‰ áŠ áˆáƒá€áˆ á‰¥áˆáŒ« áˆ‹áŒˆáŠ™ áŠ¥áŠ•á‹²áˆáˆ áˆˆá‹©áŠ’á‰¨áˆ­áˆ²á‰² á‰°áˆ˜áˆ«á‰‚ áˆµáˆ« <I> áŠ¥á‹á‰…áŠ“áŠ“ áˆ½áˆáˆ›á‰µ<H> á‰ á‰°áˆ˜áˆ³áˆ³á‹­ á‹¨á‹˜áŒ áŠáŠ›á‹ á‹¨áŠ¢á‰µá‹®áŒµá‹« áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ áŠ á‹˜áŒ‹áŒ… á‰ áŠ¥áˆˆá‰± á‹­á‹ á‹­á‹°áˆ¨áŒ‹áˆá¤<G> áˆ­áŠ­áŠ­á‰¥áˆ á‹­áŠ–áˆ«áˆ <F> áˆšáŠ’áˆµá‰µáˆ­ á‹´áŠ¤á‰³á‹á¢ áˆµáˆáŠ•á‰°áŠ›á‹áŠ• á‹¨áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ á‹¨á‰°áˆˆá‹¨ áˆˆáˆ›á‹µáˆ¨áŒ áŠ¨áˆ›áˆŒá‹¥á‹« áŠ áˆˆáˆáŠ á‰€á á‹¨áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ áˆáˆá‹µ á‰°á‹ˆáˆµ â‡ áˆ á‹«áˆ‰á‰µ áŠ á‰¶ áŠ«áˆ³áˆáŠ• áŽáˆ¨áˆ™ áˆˆáˆ˜áŒ€áˆ˜áˆªá‹« <E> á‰ á‰³á‹³áŒŠ áŠ­áˆáˆ <D>áˆ áˆá‹© á‹«á‹°áˆ­áŒˆá‹‹áˆ á‰¥áˆˆá‹‹áˆá¢ á‹¨áˆ±áˆ›áˆŒ áŠ­áˆáˆ áŠ¨á‰°áˆ›<C> áŠ®áŠ•áˆµá‰µáˆ«áŠ­áˆ½áŠ•<B> á‹¶/áˆ­ á‰ á‰ áŠ©áˆ‹á‰¸á‹ áŠ­áˆáˆ‰ <A>áŠ• áˆˆáˆ›áˆµá‰°áŠ“áŒˆá‹µ á‹áŒáŒ…á‰±áŠ• áŠ áŒ áŠ“á‰‹áˆá¤ á‰°áˆ³á‰³áŠ áŠ¨á‰°áˆžá‰½áˆ <z> áŒ…áŒáŒ…áŒ‹ áŠ¥á‹¨áŒˆá‰¡ áŠá‹ á‰¥áˆˆá‹‹áˆá¢ á‰¢áˆ® áˆ€áˆ‹áŠá‹ á‰ áŒ¥á‰‚á‰µ<y> á‰°áŠ¨áˆµá‰¶ á‹¨áŠá‰ áˆ¨á‹ á‰½áŒáˆ­ áŒˆá…á‰³á‰½áŠ•áŠ• áŠ á‰ áˆ‹áˆ½á‰¶ á‹¨áŠá‰ áˆ¨<x> áŠ áˆáŠ• áŒáŠ• áˆáŠ•áˆ á‹¨á€áŒ¥á‰³áˆ á‹­áˆáŠ• á‹¨á‹°áˆ…áŠ•áŠá‰µ <w> á‹¨áˆˆáˆ á‰¥áˆˆá‹‹áˆá¢ á‹¶/áˆ­ áŠ á‰¥á‹±áˆáˆá‰³áˆ… áˆµáˆáŠ•á‰°áŠ›á‹ á‹¨áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ á‹¨áŠ­áˆáˆ‹á‰½áŠ•áŠ• á‰¥áˆŽáˆ<v>áŠ• áŠ áˆµá‰°áˆ›áˆ›áŠ áˆ°áˆ‹áˆ á‹¨áˆáŠ“áˆ¨áŒ‹áŒáŒ¥á‰ á‰µáŠ“ á‰ á‰°áŒá‰£áˆ­áˆ á‹¨áˆáŠ“áˆ³á‹­á‰ á‰µ áŠ¥áŠ•á‹²áˆ†áŠ• áˆ°áŠ áˆµáˆ« <u> á‹áŒ¤á‰±áŠ•áˆ áŠ¥á‹«á‹¨áŠ• áŠá‹ á‰¥áˆˆá‹‹áˆá¢ áˆµáˆáŠ•á‰°áŠ›á‹<t> áŠ¨á‰°áˆžá‰½ áŽáˆ¨áˆ áŠ¨á‹¨áŠ«á‰²á‰µ 9- <s>/2011 '' <r> áˆˆáŠ¢á‰µá‹®áŒµá‹« áŠ¨á‰°áˆžá‰½ á‰¥áˆá…áŒáŠ“'' á‰ áˆšáˆ <q> á‰ áŒ…áŒáŒ…áŒ‹ á‹­áŠ«áˆ„á‹³áˆá¢ áˆáŠ•áŒ­á¦\n",
      "\n",
      "Targets: \n",
      "\n",
      " <Z> á‹¨á€áŒ¥á‰³<Y> áŽáˆ¨áˆ<X> á‰½áŒáˆ­ <W>\" <V> á‹¨áŒˆáˆˆáá‰µ<U> á‹ˆá‹° <T> á‰¥áˆˆá‹‹áˆá¢<S> á‹áŒáŒ…á‰¶á‰½ <R> áˆµáŠáˆµáˆ­á‹<Q> á‰ áŠ¨á‰°áˆžá‰½ <P> áˆáˆ›á‰µ<O> áŠ áŒ€áŠ•á‹³á‹Žá‰½<N> á‰ á‰¤á‰¶á‰½ <M> á‰°á‹«á‹«á‹¥<L> áŽáˆ¨áˆžá‰½ <K> áŠ¨áŠ¨á‰°áˆžá‰½ <J>áˆšáŠ–áˆ¨á‹ <I> áˆáŒ£áˆªá‹Žá‰½<H> á‹­áˆ°áŒ£áˆ á‰°á‰¥áˆáˆá¢<G> á‹¨á‹‹áŠ•áŒ« <F> á‰¥áˆˆá‹‹áˆ <E> áŒŠá‹œ <D> áˆ˜áŠ«áˆ„á‹±<C> áˆáˆ›á‰µáŠ“<B> á‰¢áˆ® áˆ€áˆ‹áŠ <A> áŽáˆ¨áˆ™ <z> á‹ˆá‹°<y> áŒáˆˆáˆ°á‰¦á‰½ áŠ¨á‹ˆáˆ«á‰µ á‰ áŠá‰µ<x> á‰¢áˆ†áŠ•áˆ <w> á‰½áŒáˆ­<v> á‹¨áŠ¨á‰°áˆ›á‰½áŠ• <u> áˆ°áˆ­á‰°áŠ“áˆ<t> á‹¨áŠ¢á‰µá‹®áŒµá‹« <s>14 <r>áˆ˜á‹°áˆ˜áˆ­ <q> áˆ˜áˆª á‰ƒáˆ\n"
     ]
    }
   ],
   "source": [
    "print('Inputs: \\n\\n', pretty_decode(inps_sample, sentinels, tokenizer))\n",
    "print('\\nTargets: \\n\\n', pretty_decode(targs_sample, sentinels, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htYqIAIiDnq2"
   },
   "source": [
    "### 3.5 Creating Training Data Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcYDODVEDnq2"
   },
   "source": [
    "Now I will create pairs using the cleaned_data by iterating over the data and create(inp,targ) pairs using the function I defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiUUxG8zDnq2"
   },
   "source": [
    "After completing data preprocessing and defining the dataset, I encountered an issue while training the model: the maximum sequence length is around 1300 tokens, causing the model to run out of memory and crash. This happens because transformer models, when trained using a masked language model (MLM) objective, have quadratic complexity (O(nÂ²)) concerning sequence length, leading to excessive memory usage for long sequences. To resolve this issue, we need to reduce the size of the input sequences by spliting a single news into multiple small news(max_size words per news). this process will reduce the size of each news but will increase the number of training datas as we split single data into multiple datas.\n",
    "\n",
    "This problem arises because I have very limited RAM and GPU to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4201,
     "status": "ok",
     "timestamp": 1740393368793,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "p8Wj3GiHDnq3",
    "outputId": "3b4f13dd-1c51-4d8a-a289-4ee8dd5e62de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of contents before reduced: 193419\n",
      "Number of contents after reduced: 408671\n",
      "maximum size before reduced: 836\n",
      "maximum size after reduced: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_size=50\n",
    "\n",
    "def reduce_size(data, max_size):\n",
    "    for element in data: \n",
    "        sub_array = element.split()\n",
    "        for i in range(0, len(sub_array), max_size): \n",
    "            yield  ' '.join(sub_array[i:i + max_size])\n",
    "\n",
    "cleaned_data_reduced=list(reduce_size(cleaned_data,max_size))\n",
    "print(f'Number of contents before reduced: {len(cleaned_data)}')\n",
    "print(f'Number of contents after reduced: {len(cleaned_data_reduced)}')\n",
    "print(f\"maximum size before reduced: {max([len(content.split()) for content in cleaned_data])}\")\n",
    "print(f\"maximum size after reduced: {max([len(content.split()) for content in cleaned_data_reduced])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jz7jmJMDnq3"
   },
   "source": [
    "let us see sample the news before and after reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740393368796,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "NyCb93RhDnq3",
    "outputId": "17b76500-83e6-4d9a-cb31-4cf84ccf2b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before reduced: \n",
      "\n",
      " áˆ°á‹áŠ• áˆˆáˆ˜áˆ­á‹³á‰µ áˆ°á‹ áˆ˜áˆ†áŠ• á‰ á‰‚ áŠá‹ ! á‰µáˆ‹áŠ•á‰µ á‹¨áŠ«á‰²á‰µ 1/2017 á‹“/áˆ á‰ áŒ€áˆ˜áˆ¨á‹ á‹¨áˆ˜á‰„á‹¶áŠ•á‹« á‹¨áŠ áˆ¨áŒ‹á‹Šá‹«áŠ• áŠ¥áŠ“ á‹¨áŠ áŠ¥áˆáˆ® áˆ…áˆ™áˆ›áŠ• áˆ˜áˆ­áŒƒ áˆ›á‹•áŠ¨áˆ á‹¨á‹µáŒ‹á áˆ›áˆ°á‰£áˆ°á‰¥ á‹˜áˆ˜á‰» áŠ¥áˆµáŠ©áŠ• 120,000,000 á‰¥áˆ­ á‰°áˆ°á‰¥áˆµá‰§áˆá¢ áˆ˜á‰„á‹¶áŠ•á‹« á‰ áˆšá‹«áˆµáŒˆáŠá‰£á‹ áˆ†áˆµá’á‰³áˆ áŒ­áˆáˆ­ á‹«áˆˆá‹ áˆ…áŠ•áƒ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹¨áŒˆáŠ•á‹˜á‰¥ áŠ¥áŒ¥áˆ¨á‰µ áŠ áŒ‹áŒ¥áˆžá‰³áˆá¢ áˆ…áŠ•áƒá‹ áˆˆáˆ›áŒ áŠ“á‰€á‰… áŒˆáŠ•á‹˜á‰¥ á‰°á‰¸áŒáˆ¨áŠ“áˆá¢ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹ˆá‹° 5 á‰¢áˆŠá‹®áŠ• á‰¥áˆ­ á‹«áˆµáˆáˆáŒ‹áˆá¢ á‰ á‰€áŒ¥á‰³ á‹­áŠ¨á‰³á‰°áˆ‰ á‹¨áˆá‰µá‰½áˆ‰á‰µáŠ• áˆáˆ‰ á‹µáŒ‹á áŠ á‹µáˆ­áŒ‰á¢\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Data after reduced: \n",
      "\n",
      " áˆ°á‹áŠ• áˆˆáˆ˜áˆ­á‹³á‰µ áˆ°á‹ áˆ˜áˆ†áŠ• á‰ á‰‚ áŠá‹ ! á‰µáˆ‹áŠ•á‰µ á‹¨áŠ«á‰²á‰µ 1/2017 á‹“/áˆ á‰ áŒ€áˆ˜áˆ¨á‹ á‹¨áˆ˜á‰„á‹¶áŠ•á‹« á‹¨áŠ áˆ¨áŒ‹á‹Šá‹«áŠ• áŠ¥áŠ“ á‹¨áŠ áŠ¥áˆáˆ® áˆ…áˆ™áˆ›áŠ• áˆ˜áˆ­áŒƒ áˆ›á‹•áŠ¨áˆ á‹¨á‹µáŒ‹á áˆ›áˆ°á‰£áˆ°á‰¥ á‹˜áˆ˜á‰» áŠ¥áˆµáŠ©áŠ• 120,000,000 á‰¥áˆ­ á‰°áˆ°á‰¥áˆµá‰§áˆá¢ áˆ˜á‰„á‹¶áŠ•á‹« á‰ áˆšá‹«áˆµáŒˆáŠá‰£á‹ áˆ†áˆµá’á‰³áˆ áŒ­áˆáˆ­ á‹«áˆˆá‹ áˆ…áŠ•áƒ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹¨áŒˆáŠ•á‹˜á‰¥ áŠ¥áŒ¥áˆ¨á‰µ áŠ áŒ‹áŒ¥áˆžá‰³áˆá¢ áˆ…áŠ•áƒá‹ áˆˆáˆ›áŒ áŠ“á‰€á‰… áŒˆáŠ•á‹˜á‰¥ á‰°á‰¸áŒáˆ¨áŠ“áˆá¢ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹ˆá‹° 5 á‰¢áˆŠá‹®áŠ• á‰¥áˆ­ á‹«áˆµáˆáˆáŒ‹áˆá¢ á‰ á‰€áŒ¥á‰³ á‹­áŠ¨á‰³á‰°áˆ‰ á‹¨áˆá‰µá‰½áˆ‰á‰µáŠ• áˆáˆ‰\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Data after reduced: \n",
      "\n",
      " á‹µáŒ‹á áŠ á‹µáˆ­áŒ‰á¢\n"
     ]
    }
   ],
   "source": [
    "print(\"Data before reduced: \\n\\n\", cleaned_data[0])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Data after reduced: \\n\\n\", cleaned_data_reduced[0])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Data after reduced: \\n\\n\", cleaned_data_reduced[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 86813,
     "status": "ok",
     "timestamp": 1740393455611,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "xfGEuCrWDnq3"
   },
   "outputs": [],
   "source": [
    "inputs_targets_pairs=[tokenize_and_mask(text.encode('utf-8', errors=\"ignore\").decode('utf-8'),tokenizer=tokenizer) for text in cleaned_data_reduced]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740393455617,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "-WtWW6b3Dnq3",
    "outputId": "274ae909-63cd-4646-a751-4a8c84dcbaae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "\n",
      " áˆ°á‹áŠ• áˆˆáˆ˜áˆ­á‹³á‰µ áˆ°á‹ áˆ˜áˆ†áŠ• á‰ á‰‚ áŠá‹ !<Z> á‹¨áŠ«á‰²á‰µ 1/2017 á‹“/áˆ á‰ áŒ€áˆ˜áˆ¨á‹ á‹¨<Y> á‹¨áŠ áˆ¨áŒ‹á‹Šá‹«áŠ• áŠ¥áŠ“ á‹¨áŠ áŠ¥áˆáˆ®<X> áˆ˜áˆ­áŒƒ áˆ›á‹•áŠ¨áˆ <W> áˆ›áˆ°á‰£áˆ°á‰¥ á‹˜áˆ˜á‰» áŠ¥áˆµáŠ©áŠ• 120,000,000 <V> á‰°áˆ°á‰¥áˆµá‰§áˆá¢ áˆ˜á‰„á‹¶áŠ•á‹« á‰ <U> áˆ†áˆµá’á‰³áˆ <T> á‹«áˆˆá‹ áˆ…áŠ•áƒ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹¨áŒˆáŠ•á‹˜á‰¥ áŠ¥áŒ¥áˆ¨á‰µ<S> áˆˆáˆ›áŒ áŠ“á‰€á‰… áŒˆáŠ•á‹˜á‰¥ á‰°á‰¸áŒáˆ¨áŠ“áˆá¢ áˆˆáˆ›áŒ áŠ“á‰€á‰… á‹ˆá‹° 5 <R> á‰¥áˆ­ á‹«áˆµáˆáˆáŒ‹áˆá¢<Q> á‹­áŠ¨á‰³á‰°áˆ‰ á‹¨áˆá‰µá‰½áˆ‰á‰µáŠ• áˆáˆ‰\n",
      "\n",
      "Targets: \n",
      "\n",
      " <Z> á‰µáˆ‹áŠ•á‰µ<Y>áˆ˜á‰„á‹¶áŠ•á‹«<X> áˆ…áˆ™áˆ›áŠ• <W> á‹¨á‹µáŒ‹á <V> á‰¥áˆ­<U>áˆšá‹«áˆµáŒˆáŠá‰£á‹ <T> áŒ­áˆáˆ­<S> áŠ áŒ‹áŒ¥áˆžá‰³áˆá¢ áˆ…áŠ•áƒá‹ <R> á‰¢áˆŠá‹®áŠ•<Q> á‰ á‰€áŒ¥á‰³\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Inputs: \n",
      "\n",
      " á‹µáŒ‹á áŠ á‹µáˆ­áŒ‰á¢\n",
      "\n",
      "Targets: \n",
      "\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pairs in inputs_targets_pairs[:2]:\n",
    "    print('Inputs: \\n\\n', pretty_decode(pairs[0], sentinels, tokenizer))\n",
    "    print('\\nTargets: \\n\\n', pretty_decode(pairs[1], sentinels, tokenizer))\n",
    "    print(\"\\n---------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5gHNgATDnq3"
   },
   "source": [
    "Let's split the data into training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740393455623,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "ySWi5JERDnq3",
    "outputId": "32e7c01f-3c20-4c8f-ce60-c4f418affa9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inputs and targets pairs:  408671\n",
      "Training data size: 326936\n",
      "Validation data size: 81735\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of inputs and targets pairs: \",len(inputs_targets_pairs))\n",
    "training_size=int(len(inputs_targets_pairs)*0.8)\n",
    "training_data=inputs_targets_pairs[:training_size]\n",
    "validation_data=inputs_targets_pairs[training_size:]\n",
    "print(f\"Training data size: {len(training_data)}\")\n",
    "print(f\"Validation data size: {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpgTPQv8Dnq3"
   },
   "source": [
    "For training a Tensorflow model we need to arrange the data into datasets. Now, I will get the `inputs` and the `targets` for the transformer model from the `training_data and validation_data`. Before creating the dataset, I need to be sure that all `inputs` have the same length by truncating the longer sequences and padding the shorter ones with `0`. The same must be done for the targets. The function `tf.keras.preprocessing.sequence.pad_sequences` will help us here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 3103,
     "status": "ok",
     "timestamp": 1740393458727,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "BWevt6CqDnq3"
   },
   "outputs": [],
   "source": [
    "training_data_inputs_padded=tf.keras.utils.pad_sequences(\n",
    "    [pairs[0] for pairs in training_data],\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")\n",
    "training_data_targets_padded=tf.keras.utils.pad_sequences(\n",
    "    [pairs[1] for pairs in training_data],\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")\n",
    "\n",
    "validation_data_inputs_padded=tf.keras.utils.pad_sequences(\n",
    "    [pairs[0] for pairs in validation_data],\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")\n",
    "\n",
    "validation_data_targets_padded=tf.keras.utils.pad_sequences(\n",
    "    [pairs[1] for pairs in validation_data],\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")\n",
    "BUFFER_SIZE = 12000\n",
    "BATCH_SIZE = 64\n",
    "training_dataset_final=tf.data.Dataset.from_tensor_slices((training_data_inputs_padded,training_data_targets_padded))\n",
    "validation_dataset_final=tf.data.Dataset.from_tensor_slices((validation_data_inputs_padded,validation_data_targets_padded))\n",
    "training_dataset_final=training_dataset_final.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset_final=validation_dataset_final.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740393458734,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "gwocqHPNDnq3",
    "outputId": "3b3eab0e-0f3a-43f7-b05e-bfb688a38eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum size of training data inputs: 205\n",
      "maximum size of training data targets: 67\n",
      "maximum size of validation data inputs: 139\n",
      "maximum size of validation data targets: 51\n",
      "seems good size for small memory devices\n"
     ]
    }
   ],
   "source": [
    "print(f'maximum size of training data inputs: {training_data_inputs_padded.shape[1]}')\n",
    "print(f'maximum size of training data targets: {training_data_targets_padded.shape[1]}')\n",
    "print(f'maximum size of validation data inputs: {validation_data_inputs_padded.shape[1]}')\n",
    "print(f'maximum size of validation data targets: {validation_data_targets_padded.shape[1]}')\n",
    "print(\"seems good size for small memory devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKv3IKdBDnq3"
   },
   "source": [
    "Let's Tokenize both the training and validation sets using our tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCwA8nEjDnq3"
   },
   "source": [
    "## 4. Pretraining the Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IcMNkEqDnq3"
   },
   "source": [
    "Now I am going to define the structure of the transformer network and pretrain it on the dataset given above. The general structure of the transformer model we will build is shown in the figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7W1CiBWDnq3"
   },
   "source": [
    "<center> <img src = \"images/fulltransformer.png\" width=\"500\" height=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZ0hYjFZDnq3"
   },
   "source": [
    "## Positional Encoding\n",
    "\n",
    "As you can see in the figure, the input embeddings are added with positional embedding vectors to capture the position of words in a sentence. The following function creates positional encoding given the embedding vectors.\n",
    "\n",
    "In sequence-to-sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model. However, when you train a Transformer network using multi-head attention, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful - you can specifically encode the positions of your inputs and pass them into the network using these sine and cosine formulas:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*   `d` is the dimension of the word embedding and positional encoding.\n",
    "*   `pos` is the position of the word.\n",
    "*   `i` refers to each of the different dimensions in the positional encodings, where `i = k // 2`.\n",
    "\n",
    "To develop some intuition about positional encodings, you can think of them broadly as a feature that contains the information about the relative positions of words. The sum of the positional encoding and word embedding is ultimately what is fed into the model.  If you just hard code the positions in, say by adding a matrix of 1's or whole numbers to the word embedding, the semantic meaning is distorted. Conversely, the values of the sine and cosine equations are small enough (between -1 and 1) that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted, and is instead enriched with positional information. Using a combination of these two equations helps your Transformer network attend to the relative positions of your input data.\n",
    "\n",
    "### Sine and Cosine Angles\n",
    "\n",
    "Notice that even though the sine and cosine positional encoding equations take in different arguments (`2i` versus `2i+1`, or even versus odd numbers) the inner terms for both equations are the same:\n",
    "\n",
    "$$\\theta(pos, i, d) = \\frac{pos}{10000^{\\frac{2i}{d}}}$$\n",
    "\n",
    "Consider the inner term as you calculate the positional encoding for a word in a sequence:\n",
    "\n",
    "$PE_{(pos, 0)}= sin\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right)$, since solving `2i = 0` gives `i = 0`\n",
    "\n",
    "$PE_{(pos, 1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right)$, since solving `2i + 1 = 1` gives `i = 0`\n",
    "\n",
    "The angle is the same for both! The angles for $PE_{(pos, 2)}$ and $PE_{(pos, 3)}$ are the same as well, since for both, `i = 1` and therefore the inner term is $\\left(\\frac{pos}{{10000}^{\\frac{2}{d}}}\\right)$. This relationship holds true for all paired sine and cosine curves:\n",
    "\n",
    "| k             | 0                         | 1                         | 2                         | 3                         | ... | d - 2                     | d - 1                     |\n",
    "| ------------- | ------------------------- | ------------------------- | ------------------------- | ------------------------- | --- | ------------------------- | ------------------------- |\n",
    "| encoding(0) = | [sin(Î¸(0, 0, d))         | cos(Î¸(0, 0, d))         | sin(Î¸(0, 1, d))         | cos(Î¸(0, 1, d))         | ... | sin(Î¸(0, d//2, d))        | cos(Î¸(0, d//2, d))        |\n",
    "| encoding(1) = | [sin(Î¸(1, 0, d))         | cos(Î¸(1, 0, d))         | sin(Î¸(1, 1, d))         | cos(Î¸(1, 1, d))         | ... | sin(Î¸(1, d//2, d))        | cos(Î¸(1, d//2, d))        |\n",
    "| ...           | ...                       | ...                       | ...                       | ...                       | ... | ...                       | ...                       |\n",
    "| encoding(pos) =| [sin(Î¸(pos, 0, d))        | cos(Î¸(pos, 0, d))        | sin(Î¸(pos, 1, d))        | cos(Î¸(pos, 1, d))        | ... | sin(Î¸(pos, d//2, d))       | cos(Î¸(pos, d//2, d))]      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740396510745,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "WPz_2rgnDnq4"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings\n",
    "\n",
    "    Arguments:\n",
    "        positions (int): Maximum number of positions to be encoded\n",
    "        d_model (int): Encoding size\n",
    "\n",
    "    Returns:\n",
    "        pos_encoding (tf.Tensor): A matrix of shape (1, position, d_model) with the positional encodings\n",
    "    \"\"\"\n",
    "\n",
    "    position = np.arange(positions)[:, np.newaxis]\n",
    "    k = np.arange(d_model)[np.newaxis, :]\n",
    "    i = k // 2\n",
    "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
    "    angle_rads = position * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVmF4UVyDnq4"
   },
   "source": [
    "\n",
    "\n",
    "## 4.2 Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch_q3p9VDnq4"
   },
   "source": [
    "The masking we will define here is different from the masking we used while preparing the data for Masked Language modeling.\n",
    "\n",
    "\n",
    "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence.\n",
    "\n",
    "### 1.1 - Padding Mask\n",
    "\n",
    "Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. Let's say the maximum length of your model is five, it is fed the following sequences:\n",
    "\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"],\n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]\n",
    "    ]\n",
    "\n",
    "which might get vectorized as:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]\n",
    "    ]\n",
    "    \n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0],\n",
    "    ]\n",
    "    \n",
    "Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, zeros will also be added for padding.\n",
    "\n",
    "When pasing these vectors through the attention layers, the zeros will typically disappear  (you will get completely new vectors given the mathematical operations that happen in the attention block). However, you still want the network to attend only to the first few numbers in that vector (given by the sentence length) and this is when a padding mask comes in handy. You will need to define a boolean mask that specifies to which elements you must attend (1) and which elements you must ignore (0) and you do this by looking at all the zeros in the sequence. Then you use the mask to set the values of the vectors (corresponding to the zeros in the initial vector) close to negative infinity (-1e9).\n",
    "\n",
    "Imagine your input vector is `[87, 600, 0, 0, 0]`. This would give you a mask of `[1, 1, 0, 0, 0]`. When your vector passes through the attention mechanism, you get another (randomly looking) vector, let's say `[1, 2, 3, 4, 5]`, which after masking becomes `[1, 2, -1e9, -1e9, -1e9]`, so that when you take the softmax, the last three elements (where there were zeros in the input) don't affect the score.\n",
    "\n",
    "The [MultiheadAttention](https://keras.io/api/layers/attention_layers/multi_head_attention/) layer implemented in Keras, uses this masking logic.\n",
    "\n",
    "**Note:** The below functions create the masking of both types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1740396511196,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "2rPkLel2Dnq4"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "\n",
    "    Arguments:\n",
    "        decoder_token_ids (matrix like): matrix of size (n, m)\n",
    "\n",
    "    Returns:\n",
    "        mask (tf.Tensor): binary tensor of size (n, 1, m)\n",
    "    \"\"\"\n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular matrix filled with ones\n",
    "\n",
    "    Arguments:\n",
    "        sequence_length (int): matrix size\n",
    "\n",
    "    Returns:\n",
    "        mask (tf.Tensor): binary tensor of size (sequence_length, sequence_length)\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27woRJV4Dnq4"
   },
   "source": [
    "\n",
    "## 4.3 - Self-Attention\n",
    "\n",
    "As the authors of the Transformers paper state, \"Attention is All You Need\".\n",
    "\n",
    "<center><img src=\"images/attention.png\" alt=\"Encoder\" width=\"600\"/></center>\n",
    "\n",
    "<center><caption><font color='purple'><b>Figure 1: Self-Attention calculation visualization</font></</caption></center>\n",
    "    \n",
    "\n",
    "The use of self-attention paired with traditional convolutional networks allows for parallelization which speeds up training. we will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to return rich, attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries\n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply\n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode\n",
    "\n",
    "\n",
    "This will be handled by Tensorlfow so we will not searately define a function to handel self-attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmdHBzy6Dnq4"
   },
   "source": [
    "## 4.4 Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehZrYgncDnq4"
   },
   "source": [
    "Now we will define the encoder part of the transformer using multi-head attention and feed forward.\n",
    "The structure of the model we will implement will look like the following figure.\n",
    "\n",
    "<center><img src=\"images/encoders.png\" alt=\"Encoder\" width=\"400\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lodvunODnq4"
   },
   "source": [
    "As you can see in the above figure inside the Encoder there is a feed forward layer. Here we will use 2 Dense layers as part of the Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1740396511945,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "8KowxPAyDnq4"
   },
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    \"\"\"\n",
    "    Returns a sequential model consisting of two dense layers. The first dense layer has\n",
    "    fully_connected_dim neurons and is activated by relu. The second dense layer has\n",
    "    embedding_dim and no activation.\n",
    "\n",
    "    Arguments:\n",
    "        embedding_dim (int): output dimension\n",
    "        fully_connected_dim (int): dimension of the hidden layer\n",
    "\n",
    "    Returns:\n",
    "        _ (tf.keras.Model): sequential model\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, d_model)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tj2V6ajDnq4"
   },
   "source": [
    "Next we will define the encoder layer class that contains both the multi-head attention and the FullyConnected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740396512247,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "TRZWalVlDnq4"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network.\n",
    "    This architecture includes a residual connection around each of the two\n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "\n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            encoder_layer_out (tf.Tensor): Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        self_mha_output = self.mha(x, x, x, mask)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
    "        skip_x_attention = self.layernorm1(x + self_mha_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        return encoder_layer_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymwwL086Dnq4"
   },
   "source": [
    "Now let's define the full Encoder Layer including the Embedding of the tokens and addition of positional embedding with Dropout layer before entering the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740396512544,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "vA3vVfNSDnq4"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer\n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps)\n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "\n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
    "                    treated as part of the input\n",
    "\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding dim)\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP8QOHZfDnq4"
   },
   "source": [
    "## 4.6 Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSe1qc_3Dnq4"
   },
   "source": [
    "Now we will define the decoder part of the transformer using masked multi-head attention, multi-head attention and feed forward.\n",
    "\n",
    "<b>N.B  pre-training transformer with both Encoder and decoder with MLM is not common task as most models like BERT which are pretrained using masked language modeling(MLM) need only encoders. But here I used both encoder and decoders so that the model can be further pre-trained or fine-tuned for tasks that need both encoder and decoder, like Neural Machine translation as well.</b>\n",
    "\n",
    "The structure of the decoder layer we will implement will look like the following figure.\n",
    "\n",
    "<center><img src=\"images/decoders.png\" alt=\"Encoder\" width=\"400\" /> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1740396513123,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "jAZIzC47Dnq5"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks,\n",
    "    one that takes the new input and uses self-attention, and the other\n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "\n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            enc_output (tf.Tensor): Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attn_weights_block1 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "            attn_weights_block2 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        mult_attn_out1, attn_weights_block1 = self.mha1(x,x,x,look_ahead_mask, return_attention_scores=True)\n",
    "        Q1 = self.layernorm1(mult_attn_out1 + x)\n",
    "        mult_attn_out2, attn_weights_block2 = self.mha2(Q1,enc_output,enc_output, padding_mask, return_attention_scores=True)\n",
    "        mult_attn_out2 = self.layernorm2(mult_attn_out2+Q1)\n",
    "        ffn_output = self.ffn(mult_attn_out2)\n",
    "        ffn_output =self.dropout_ffn(ffn_output)\n",
    "        out3 =self.layernorm3(ffn_output+mult_attn_out2)\n",
    "\n",
    "\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rt7_Jyv2Dnq5"
   },
   "source": [
    "Now let's define the full Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740396513431,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "8tOUfZ0NDnq5"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the target input to an embedding layer\n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps)\n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "\n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
    "            enc_output (tf.Tensor):  Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask=look_ahead_mask,\n",
    "                                                   padding_mask=padding_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1_self_att'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2_decenc_att'] = block2\n",
    "\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z4T-SnhDnq5"
   },
   "source": [
    "## 4.6 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH7P693wDnq5"
   },
   "source": [
    "Now we will combine both the encoder and the decoder layers defined above into a single transformer model. The full structure of the model we will form is depicted below. In the code, in addition to encoder and decoder we will add a final Dense layer with softmax  activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBKZ3UsnDnq5"
   },
   "source": [
    "<center><img src=\"images/transformer.png\" width=\"3\" height=\"2\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740396514663,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "7vDk7gNsDnq5"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               target_vocab_size, max_positional_encoding_input,\n",
    "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
    "\n",
    "\n",
    "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            input_sentence (tf.Tensor): Tensor of shape (batch_size, input_seq_len)\n",
    "                              An array of the indexes of the words in the input sentence\n",
    "            output_sentence (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
    "                              An array of the indexes of the words in the output sentence\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            enc_padding_mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
    "                    treated as part of the input\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            dec_padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            final_output (tf.Tensor): The final output of the model\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        enc_output = self.encoder(input_sentence, training, enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(output_sentence, enc_output, training,\n",
    "           look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_cU780tDnq5"
   },
   "source": [
    "## 4.7 Intialize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIW65PS8Dnq5"
   },
   "source": [
    "Now we will intialize our model to pre-train it on the data we defined. Most of the parameter values we will use here are taken form the <a href=\"https://arxiv.org/abs/1706.03762\">Attention is All You Need</a> paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1740396520764,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "ekELswYoDnq5",
    "outputId": "1696c7f8-1c97-4411-9c5e-6c9bff27e132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum positional encoding length for input: 205\n",
      "Maximum positional encoding length for target: 67\n"
     ]
    }
   ],
   "source": [
    "#let's find the maximum length in our training data to set it as the maximum positional encoding\n",
    "POSITIONAL_ENCODING_INPUT_LENGTH =training_data_inputs_padded.shape[1]\n",
    "POSITIONAL_ENCODING_TARGET_LENGTH =training_data_targets_padded.shape[1]\n",
    "print(f\"Maximum positional encoding length for input: {POSITIONAL_ENCODING_INPUT_LENGTH}\")\n",
    "print(f\"Maximum positional encoding length for target: {POSITIONAL_ENCODING_TARGET_LENGTH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1740393663314,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "jZ9A9BcmDnq5"
   },
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "NUM_LAYERS = 6\n",
    "EMBEDDING_DIM = 512\n",
    "FULLY_CONNECTED_DIM = 2048\n",
    "NUM_HEADS= 8\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "transformer = Transformer(\n",
    "    NUM_LAYERS,\n",
    "    EMBEDDING_DIM,\n",
    "    NUM_HEADS,\n",
    "    FULLY_CONNECTED_DIM,\n",
    "    vocab_size,\n",
    "    vocab_size,\n",
    "    POSITIONAL_ENCODING_INPUT_LENGTH,\n",
    "    POSITIONAL_ENCODING_TARGET_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the total number of Parameters of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  114219008 \n",
      "                                                                 \n",
      " decoder_1 (Decoder)         multiple                  164633600 \n",
      "                                                                 \n",
      " dense_49 (Dense)            multiple                  51300000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 330152608 (1.23 GB)\n",
      "Trainable params: 330152608 (1.23 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 0 is the padding value\n",
    "sentence_a = np.array([[2, 3, 1, 3, 0, 0, 0]])\n",
    "sentence_b = np.array([[1, 3, 4, 0, 0, 0, 0]])\n",
    "\n",
    "enc_padding_mask = create_padding_mask(sentence_a)\n",
    "dec_padding_mask = create_padding_mask(sentence_a)\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(sentence_a.shape[1])\n",
    "\n",
    "test_summary, att_weights = transformer(\n",
    "    sentence_a,\n",
    "    sentence_b,\n",
    "    False,\n",
    "    enc_padding_mask,\n",
    "    look_ahead_mask,\n",
    "    dec_padding_mask\n",
    ")\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQk9GwCgDnq5"
   },
   "source": [
    "## 4.8 Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKwbSGrsDnq5"
   },
   "source": [
    "\n",
    "We have finished defining our model and processing our traning and validation datas. The next step will be training !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740393675433,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "AyXEsMH6Dnq6"
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embedding_dim, warmup_steps=80000):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = tf.cast(embedding_dim, tf.float32)  \n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)   \n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)  \n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embedding_dim) * tf.math.minimum(arg1, arg2) \n",
    "\n",
    "learning_rate = CustomSchedule(EMBEDDING_DIM)\n",
    "optimizer = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = SparseCategoricalAccuracy(name='train_accuracy')\n",
    "val_loss = Mean(name='val_loss')\n",
    "val_accuracy = SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "\n",
    "def loss_function(real, pred, mask):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        real (tf.Tensor): Ground truth labels\n",
    "        pred (tf.Tensor): Model predictions\n",
    "        mask (tf.Tensor): Mask to ignore padding tokens\n",
    "    Returns:\n",
    "        loss (tf.Tensor): Computed loss\n",
    "    \"\"\"\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        inp (tf.Tensor): Input sequence\n",
    "        tar (tf.Tensor): Target sequence\n",
    "    Returns:\n",
    "        enc_padding_mask (tf.Tensor): Padding mask for encoder\n",
    "        look_ahead_mask (tf.Tensor): Look-ahead mask for decoder\n",
    "        dec_padding_mask (tf.Tensor): Padding mask for decoder\n",
    "    \"\"\"\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        inp (tf.Tensor): Input sequence\n",
    "        tar (tf.Tensor): Target sequence\n",
    "    \"\"\"\n",
    "    tar_inp = tar[:, :-1]  \n",
    "    tar_real = tar[:, 1:]  \n",
    "\n",
    "\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions, create_padding_mask(tar_real))\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "\n",
    "@tf.function\n",
    "def val_step(inp, tar):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        inp (tf.Tensor): Input sequence\n",
    "        tar (tf.Tensor): Target sequence\n",
    "    \"\"\"\n",
    "    tar_inp = tar[:, :-1]  \n",
    "    tar_real = tar[:, 1:]  \n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    predictions, _ = transformer(inp, tar_inp, False, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions, create_padding_mask(tar_real))\n",
    "    val_loss(loss)\n",
    "    val_accuracy(tar_real, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BftSKjBDnq6"
   },
   "source": [
    "Now, let's train our model.\n",
    "\n",
    "Note: As you can see from the above summary of the model, the total number of training parameters in our model is 330,152,608, and the training dataset consists of approximately 400k samples. Training for multiple epochs would take several days, even with a high-end GPU. Due to GPU constraints, I trained the model for only 3 epochs, as even a single epoch requires several hours.\n",
    "\n",
    "The model trained for 3 hours has been saved inside model_weights/pretrained_model_checkpoint as model_weights_v3.h5. We will use this model for our fine-tuning step.\n",
    "\n",
    "Important: Due to GPU limitations, we could not train beyond 3 epochs. The training was conducted on RunPod.io using an H100 SXM GPU, costing approximately $20. Since the model has only been trained for 3 epochsâ€”far fewer than the optimal number of epochsâ€”it is not highly powerful. However, if you have access to better GPUs and a sufficient budget, you can train it for more epochs to significantly enhance its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1784589,
     "status": "ok",
     "timestamp": 1740396067332,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "Z84fRrHxDnq6",
    "outputId": "c8785743-eccc-469c-d779-b85b62ed9ae6"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m val_accuracy\u001b[38;5;241m.\u001b[39mreset_states()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch, (inp, tar)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_dataset_final):\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:890\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    893\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_function_spec  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    894\u001b[0m       \u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    895\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:147\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m--> 147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:398\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m args \u001b[38;5;241m=\u001b[39m placeholder_bound_args\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m    396\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[0;32m--> 398\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:305\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m   arg_names \u001b[38;5;241m=\u001b[39m base_arg_names\n\u001b[1;32m    304\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m monomorphic_function\u001b[38;5;241m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 305\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m    316\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1055\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1052\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1054\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1055\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:597\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 597\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/y6/b7qt1wsj5hs191q5tfnn06dw0000gn/T/__autograph_generated_fileg6tj2gvd.py:20\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(inp, tar)\u001b[0m\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(loss_function), (ag__\u001b[38;5;241m.\u001b[39mld(tar_real), ag__\u001b[38;5;241m.\u001b[39mld(predictions), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(create_padding_mask), (ag__\u001b[38;5;241m.\u001b[39mld(tar_real),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     19\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(transformer)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(train_loss), (ag__\u001b[38;5;241m.\u001b[39mld(loss),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     22\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(train_accuracy), (ag__\u001b[38;5;241m.\u001b[39mld(tar_real), ag__\u001b[38;5;241m.\u001b[39mld(predictions)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:460\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1230\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1229\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    651\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 652\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1260\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_with_dtensor:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;66;03m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m-> 1260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1352\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1352\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m   1357\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:2992\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2989\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   2990\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2991\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2994\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2995\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4062\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4060\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4061\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4062\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4068\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4064\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4065\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4066\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4067\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4068\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4070\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1349\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step_xla(grad, var, \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(var)))\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:241\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(variable) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimizer cannot recognize variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/optimizers/adam.py:174\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    171\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_momentums[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict[var_key]]\n\u001b[1;32m    172\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_velocities[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict[var_key]]\n\u001b[0;32m--> 174\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta_2_power\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta_1_power\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gradient, tf\u001b[38;5;241m.\u001b[39mIndexedSlices):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Sparse gradients.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     m\u001b[38;5;241m.\u001b[39massign_add(\u001b[38;5;241m-\u001b[39mm \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1))\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:1466\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1462\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[0;32m-> 1466\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1468\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1472\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:1649\u001b[0m, in \u001b[0;36mtruediv\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.truediv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruediv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_binary_elementwise_api\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtruediv\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Divides x / y elementwise (using Python 3 division operator semantics).\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \n\u001b[1;32m   1624\u001b[0m \u001b[38;5;124;03m  NOTE: Prefer using the Tensor operator or tf.divide which obey Python\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;124;03m    TypeError: If `x` and `y` have different dtypes.\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1649\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_truediv_python3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:1587\u001b[0m, in \u001b[0;36m_truediv_python3\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1585\u001b[0m   x \u001b[38;5;241m=\u001b[39m cast(x, dtype)\n\u001b[1;32m   1586\u001b[0m   y \u001b[38;5;241m=\u001b[39m cast(y, dtype)\n\u001b[0;32m-> 1587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal_div\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:7902\u001b[0m, in \u001b[0;36mreal_div\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   7900\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m   7901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 7902\u001b[0m   _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7903\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRealDiv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7904\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   7905\u001b[0m   _result \u001b[38;5;241m=\u001b[39m _dispatch\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m   7906\u001b[0m         real_div, (), \u001b[38;5;28mdict\u001b[39m(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   7907\u001b[0m       )\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:775\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    770\u001b[0m   _ExtractDefaultTypesAndAllowedTypes(op_def, default_type_attr_map,\n\u001b[1;32m    771\u001b[0m                                       allowed_list_attr_map)\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# Requires that op_def has passed validation (using the C++\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# ValidateOpDef() from ../framework/op_def_util.h).\u001b[39;00m\n\u001b[0;32m--> 775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, ops\u001b[38;5;241m.\u001b[39mname_scope(name) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[1;32m    776\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m fallback:\n\u001b[1;32m    777\u001b[0m     _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n\u001b[1;32m    778\u001b[0m                            keywords, default_type_attr_map, attrs, inputs,\n\u001b[1;32m    779\u001b[0m                            input_types)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:443\u001b[0m, in \u001b[0;36mFuncGraph.as_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mas_default\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 443\u001b[0m   outer_cm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m   \u001b[38;5;129m@tf_contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    446\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner_cm\u001b[39m():\n\u001b[1;32m    447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for copying distribute.Strategy scope information.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3793\u001b[0m, in \u001b[0;36mGraph.as_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mas_default\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3754\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a context manager that makes this `Graph` the default graph.\u001b[39;00m\n\u001b[1;32m   3755\u001b[0m \n\u001b[1;32m   3756\u001b[0m \u001b[38;5;124;03m  This method should be used if you want to create multiple graphs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;124;03m    A context manager for using this graph as the default graph.\u001b[39;00m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3793\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_graph_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_controller\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:281\u001b[0m, in \u001b[0;36mcontextmanager.<locals>.helper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhelper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_GeneratorContextManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "   \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(training_dataset_final):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Batch {batch}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result()}')\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(validation_dataset_final):\n",
    "        val_step(inp, tar)\n",
    "\n",
    "    train_losses.append(train_loss.result())\n",
    "    val_losses.append(val_loss.result())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss.result()}, Train Accuracy: {train_accuracy.result()}')\n",
    "    print(f'Epoch {epoch + 1}, Val Loss: {val_loss.result()}, Val Accuracy: {val_accuracy.result()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7gteVmTqIzc"
   },
   "outputs": [],
   "source": [
    "# save the model checkout if you have trained the model\n",
    "transformer.save_weights(f\"./model_weights/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4xuYB36Dnq6"
   },
   "source": [
    "# 5. Fine-Tuning for Hate Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmJ4ON_iDnq6"
   },
   "source": [
    "Fine-tuning is the process of adapting a pre-trained model to a specific task, in this case, hate speech recognition. This involves taking the pre-trained transformer model and training it further on a smaller, task-specific dataset. Fine-tuning allows the model to leverage the general language understanding it gained during pre-training while specializing in the nuances of hate speech detection.  \n",
    "\n",
    "We have pre-trained the model using **Masked Language Modeling (MLM)**, a self-supervised task that enables the model to capture semantic meanings for Amharic text. The next step is to take the checkpoints of our pre-trained model, remove the final layer, and lock all the remaining layers. We will then add new layers and train only the newly added layers on our labeled hate speech dataset.  \n",
    "\n",
    "If this approach does not yield satisfactory results, we will unlock some of the pre-trained layers and fine-tune them alongside the new layers. This allows the model to adapt more effectively to the specific task while retaining the knowledge it gained during pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX32eV5GDnq6"
   },
   "source": [
    "## 5.1 Preparing the Fine-Tuning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "makG6XATDnq6"
   },
   "source": [
    "This time, instead of collecting and labeling a hate speech recognition dataset myself, I will utilize an existing dataset to fine-tune the model. The dataset I will use is available at [https://data.mendeley.com/datasets/ymtmxx385m/1](https://data.mendeley.com/datasets/ymtmxx385m/1). This dataset contains labeled examples of hate speech(around 30k labeled data), making it suitable for fine-tuning the pre-trained model for the specific task of hate speech recognition.  \n",
    "\n",
    "By leveraging this dataset, I can save time and resources while ensuring the model is trained on high-quality, annotated data. The dataset will be preprocessed to align with the input format required by the model.\n",
    "\n",
    "This approach allows me to focus on adapting the pre-trained model to the task of hate speech detection without the overhead of data collection and labeling. The fine-tuning process will involve training the model on this dataset, evaluating its performance, and iterating as needed to achieve the desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sgCBEOODnq6"
   },
   "source": [
    "The original dataset from the source is inside the folder /data/original_hate_speech_data. lets see what the datas look like and process it for our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1267,
     "status": "ok",
     "timestamp": 1740396554948,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "cuJm5p0rDnq6",
    "outputId": "a08aeeab-248f-417f-effd-ff4d16004b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of post datas: 30000\n",
      "Total number of label datas: 30000\n"
     ]
    }
   ],
   "source": [
    "with open(\"./datas/original_hate_speech_data/posts.txt\", \"r\") as file:\n",
    "    post_data = file.readlines()\n",
    "with open(\"./datas/original_hate_speech_data/labels.txt\", \"r\") as file:\n",
    "    labels = file.readlines()\n",
    "\n",
    "print(f\"Total number of post datas: {len(post_data)}\")\n",
    "print(f\"Total number of label datas: {len(labels)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1740396555256,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "dz26cF5RDnq6",
    "outputId": "48bca36a-c925-454c-962f-45d842855d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post 1: áŠ áŠ•á‰µ áŠ¥áŠ•áŠ¨á áˆ˜áˆ­áŒ áˆ… áŠ áˆ›áˆ« áˆ†áŠáˆ… á‰°á‹ˆáˆá‹°áˆ€áˆ\n",
      "\n",
      "Label 1: Hate\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 2: á‚áˆ‹ á‰ á‹á‰·áˆ áŠ¥áˆ³ áŠ®áˆœáŠ•á‰µ áˆ‹á‹­\n",
      "\n",
      "Label 2: Hate\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 3: áŠ á‰¥áŠ• á‹›áˆ¬ á‰£á‹ˆáŒ£á‹ áˆ˜áŒáˆˆáŒ« á‹¨áŠ á‰¥á‹­ áŠ áˆ…áˆ˜á‹µ áˆ˜áŠ•áŒáˆµá‰µ á‹«áˆˆáŠ áŒá‰£á‰¥ á‹«áˆ°áˆ«á‰¸á‹áŠ• áŠ áˆ˜áˆ«áˆ®á‰¹áŠ• áŠ¥áŠ“ áŠ á‰£áˆ‹á‰¶á‰¹áŠ• áŠ¥áŠ•á‹²áˆˆá‰…á‰… áŒ á‹­á‰‹áˆ\n",
      "\n",
      "Label 3: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 4: áŠ¨á‰ áŒ á‹áˆ­áá‹« á‹ˆá‹° á‰³áˆªáŠ­ á‹˜áˆ¨á‹\n",
      "\n",
      "Label 4: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 5: áŠ áˆ›áˆ« á‹¨áŠ¢á‰µá‹®áŒ²á‹« áˆ²áˆ áŠá‹ áˆšá‹«áˆáˆ­á‰ á‰µ\n",
      "\n",
      "Label 5: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 6: áŠ¸áˆ¨ áˆ¼áˆ áŠá‹ á‹¨á‰„áˆ³áˆ­áŠ• áˆˆá‰„áˆ³áˆ­\n",
      "\n",
      "Label 6: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 7: áŠ­áˆ­áˆµá‰²áˆµ áˆ°áˆ…áˆˆáŠáŠ­áˆ­áˆµá‰¶áˆµ áˆ˜áˆ€áˆ¨áŠ“á‹­á‰…áˆ­ á‰ áˆˆáŠ“\n",
      "\n",
      "Label 7: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 8: á‹ˆáŒˆáŠ• áŠ¥áŠ•á‹´á‰µ áŠ“á‰½áˆ áŠ á‹²áˆµ áˆ…á‹­á‹ˆá‰µ áŠ¥áŠ“ áŠ¢áŠ•á‰«á‹­áˆ®áŠ•áˆ˜áŠ•á‰µ á‹¨áˆ˜áˆˆáˆ›áˆ˜á‹µ áˆáŠ”á‰³ áˆ‹á‹­ áŠá‰ áˆ­áŠ© áŠáŒˆáˆ®á‰½áŠ• áŠ áˆµá‰°áŠ«áŠ­á‹¨ á‰¥á‰… á‰¥á‹«áˆˆáˆ\n",
      "\n",
      "Label 8: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 9: áŠ¨á‰µá‰‚á‰µ á‹ˆáˆ«á‰¶á‰½ á‰ áŠá‰µ á‹¨áŠ áˆ›áˆ« áˆ…á‹á‰¥ áŠ¥áŠ“ á–áˆˆá‰²áŠ¨áŠ› á‰µáˆá‰ áˆµáˆ«á‹‰ á‰ á‰£áˆˆá‰á‰µ  áŠ áˆ˜á‰³á‰µ á‹¨á‰°áŒˆáŠá‰¡ á‹¨á–áˆˆá‰²áŠ« áŠ¥áŠ“ á‹¨áŠ¢áŠ®áŠ–áˆš á‰°á‰‹áˆ›á‰µ     á‰ áˆ›áˆáˆ«áˆ¨áˆµ áŠ á‹³á‹²áˆµ á‰°á‰‹áˆ›á‰µáŠ• á‹¨áŠ áˆ›áˆ«áŠ• áˆ…á‹á‰¥áˆ áˆ†áŠ áˆŒáˆ‹á‹‰áŠ• á‰ áŠ¥áŠ©áˆáŠá‰µ á‰ áˆšáŒ á‰…áˆ™ áŠ¥áŠ“ á‰ áˆšá‹«áˆµá‰°áŠ“áŒá‹±á‰ á‰µ áˆ˜áŠ•áŒˆá‹µ áˆ˜áŒˆáŠ•á‰£á‰µ áŠ¥áŠ•á‹³áˆˆá‰£á‰¸á‹‰ áŠ¥áŠ“ á‰µáˆá‰ á‹¨á‰¤á‰µ áˆµáˆ«á‰½áŠ• á‹­áˆ… áˆ˜áˆ†áŠ‘áŠ• á‰ áˆ°áŠá‹‰ áŒˆáˆáŒ¨á‹‹áˆˆá‹‰\n",
      "\n",
      "Label 9: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Post 10:  á‹¨áŠ áˆ›áˆ«áŠ• áˆ…á‹á‰¥ á‰°á‰‹áˆ›á‰¶á‰½ áŠá‹‰ á‹¨áŒŽá‹±á‰µ á‰ á‹ˆá‰…á‰± á“á‹ˆáˆ­ á‹¨áŠá‰ áˆ¨á‹‰ á‰¡á‹µáŠ• á‰°á‰‹áˆ›á‰¶á‰½áŠ• áˆ²áŒˆáŠá‰£ á‰ áˆ›áŠ•áŠ›á‹‰áˆ áˆ˜áŠ•áŒˆá‹µ á‹¨áŠ áˆ›áˆ«áŠ• áˆ…á‹á‰¥ áŠ­á‰áŠ› á‰ áŠ¢áŠ®áŠ–áˆš á‰ á–áˆˆá‰²áŠ« áŠ¥áŠ“ áˆŒáˆŽá‰½ áˆ›áˆ…á‰ áˆ«á‹Š á‹˜áˆ­áŽá‰½ á‰ áˆšá‹«áˆ½áˆ˜á‹°áˆá‹µ áˆ˜áˆáŠ© áŠá‹‰ \n",
      "\n",
      "Label 10: Free\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=1800\n",
    "end=1810\n",
    "sample_post_data = post_data[start:end]\n",
    "sample_label_data = labels[start:end]\n",
    "for i,(post,label) in enumerate(zip(sample_post_data,sample_label_data)):\n",
    "    print(f\"Post {i+1}: {post}\")\n",
    "    print(f\"Label {i+1}: {label}\")\n",
    "    print(\"\\n---------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LvstU7QDnq6"
   },
   "source": [
    "Let's change the labels into binary. 1 mean Free and 0 mean hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1740396558046,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "zZb9ae7XDnq6",
    "outputId": "d3155c8e-b053-4675-e596-0cc31b12c8b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels either hate or free\n"
     ]
    }
   ],
   "source": [
    "#first lates make sure the labels are only Hate and Free\n",
    "labels_array=[label.strip() for label in labels]\n",
    "total_labels=0\n",
    "for label in labels_array:\n",
    "    if label.lower() in [\"hate\",\"free\"]:\n",
    "        total_labels+=1\n",
    "if total_labels==len(labels):\n",
    "    print(\"All labels either hate or free\")\n",
    "else:\n",
    "    print(\"Not all labels are hate or free, the total number of correct labels is: \",total_labels)\n",
    "    print(\"Please check the labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaW-AzsMDnq6"
   },
   "source": [
    "Now let as change the labels into binary and save. Uncomment the cell only if labels_binary.txt data is not available inside datas/original_hate_speech_data. Or you can  delete the file labels_binary.txt inside datas/original_hate_speech_data, uncomment the code and run it. Otherwise it will duplicate the labels data  2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "WlxACFD5Dnq6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# for label in labels_array:\n",
    "#     with open(\"/Users/mahder/Real Projects/Mahder AI/Training_models/Transformer_classifier_app_1/datas/original_hate_speech_data/labels_binary.txt\", \"a\") as file:\n",
    "#         if label.lower()==\"hate\":\n",
    "#             file.write(\"1\\n\")\n",
    "#         else:\n",
    "#             file.write(\"0\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1740396561573,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "T9spTx1ADnq6",
    "outputId": "0b512e7b-04f1-46ed-d812-22c463552fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels are correctly changed into binary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# let's check if it is correctly changed into binary\n",
    "with open(\"./datas/original_hate_speech_data/labels_binary.txt\", \"r\") as file:\n",
    "    binary_labels = file.readlines()\n",
    "binary_labels_array=[int(label.strip()) for label in binary_labels]\n",
    "for binary, label in zip(binary_labels_array,labels_array):\n",
    "    if label.lower()==\"hate\":\n",
    "        assert binary==1\n",
    "    else:\n",
    "        assert binary==0\n",
    "print(\"All labels are correctly changed into binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pbRmPyTDnq7"
   },
   "source": [
    "Let's process the datas into a form suitable for our model and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "executionInfo": {
     "elapsed": 1066,
     "status": "ok",
     "timestamp": 1740396564531,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "AvmsJs22Dnq7"
   },
   "outputs": [],
   "source": [
    "#tokenize the post data\n",
    "post_data_array=[post.strip() for post in post_data]\n",
    "tokenized_hate_speech_data=[tokenizer.tokenize(post) for post in post_data_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740396564555,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "FLgoBXF3Dnq7",
    "outputId": "da3801bc-b446-4b0a-8ee8-172ff4607667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original post data: \n",
      "\n",
      " áŠ áˆ›áˆ­áŠ›áŠ• áŠ¥áŠ“ áŠ áˆ›áˆ«á‹ á‹¨áˆšá‰£áˆˆá‹ á‰ á‹µáˆ® áŒŠá‹œ áˆáŠ• áŠ áŒˆáŠ“áŠ˜á‹\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Tokenized post data: \n",
      "\n",
      " [1649, 5, 7, 745, 22, 3638, 45221, 38, 107, 81333, 22]\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Tokenized data as subwords: \n",
      "\n",
      " ['â–áŠ áˆ›áˆ­áŠ›', 'áŠ•', 'â–áŠ¥áŠ“', 'â–áŠ áˆ›áˆ«', 'á‹', 'â–á‹¨áˆšá‰£áˆˆá‹', 'â–á‰ á‹µáˆ®', 'â–áŒŠá‹œ', 'â–áˆáŠ•', 'â–áŠ áŒˆáŠ“áŠ˜', 'á‹']\n"
     ]
    }
   ],
   "source": [
    "#let's visialize what the tokenization looks like\n",
    "print(\"Original post data: \\n\\n\", post_data_array[1742])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Tokenized post data: \\n\\n\", tokenized_hate_speech_data[1742])\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Tokenized data as subwords: \\n\\n\", tokenizer.id_to_piece(tokenized_hate_speech_data[1742]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1740396566714,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "bEzvnqoqDnq7"
   },
   "outputs": [],
   "source": [
    "tokenized_hate_speech_data_padded=tf.keras.utils.pad_sequences(\n",
    "    tokenized_hate_speech_data,\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1740396566753,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "HHTd4UU7xcWv"
   },
   "outputs": [],
   "source": [
    "training_hate_final_input=tokenized_hate_speech_data_padded[:28000]\n",
    "training_hate_final_output=binary_labels_array[:28000]\n",
    "validation_hate_final_input=tokenized_hate_speech_data_padded[28000:]\n",
    "validation_hate_final_output=binary_labels_array[28000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1740396567051,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "Sa33nsBpDnq7"
   },
   "outputs": [],
   "source": [
    "HATE_BUFFER_SIZE = 3000\n",
    "HATE_BATCH_SIZE = 32\n",
    "training_final=tf.data.Dataset.from_tensor_slices((training_hate_final_input,training_hate_final_output))\n",
    "validation_final=tf.data.Dataset.from_tensor_slices((validation_hate_final_input,validation_hate_final_output))\n",
    "training_final_dataset=training_final.shuffle(HATE_BUFFER_SIZE).batch(HATE_BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()\n",
    "validation_final_dataset=validation_final.batch(HATE_BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QugM6Qp4Dnq7"
   },
   "source": [
    "## 5.2 Adjusting the Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWOAdwhdDnq7"
   },
   "source": [
    "The pre trained model is inside model_weights/pretrained_model_checkpoin  we will load it and fine tune it in on the above processed dataset. The pretrained model has both encoder and decoder. but for this binary classification task we need only the ecncoder part so we need to take only the encoder part of the transformer and fine tune it ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740396570762,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "KD0H6czilwKY",
    "outputId": "aaa298e4-3e3c-47dd-96c7-dc2958333c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum positional encoding length for input: 378\n",
      "Maximum positional encoding length for target: 1\n"
     ]
    }
   ],
   "source": [
    "#let's find the maximum length in our training data to set it as the maximum positional encoding\n",
    "POSITIONAL_ENCODING_INPUT_LENGTH_HATE =training_hate_final_input.shape[1]\n",
    "POSITIONAL_ENCODING_TARGET_LENGTH_HATE =1\n",
    "print(f\"Maximum positional encoding length for input: {POSITIONAL_ENCODING_INPUT_LENGTH_HATE}\")\n",
    "print(f\"Maximum positional encoding length for target: {POSITIONAL_ENCODING_TARGET_LENGTH_HATE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1740396571493,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "ENoVGcQhDnq7"
   },
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "NUM_LAYERS = 6\n",
    "EMBEDDING_DIM = 512\n",
    "FULLY_CONNECTED_DIM = 2048\n",
    "NUM_HEADS= 8\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "transformer_pretrained = Transformer(\n",
    "    NUM_LAYERS,\n",
    "    EMBEDDING_DIM,\n",
    "    NUM_HEADS,\n",
    "    FULLY_CONNECTED_DIM,\n",
    "    vocab_size,\n",
    "    vocab_size,\n",
    "    POSITIONAL_ENCODING_INPUT_LENGTH_HATE,\n",
    "    POSITIONAL_ENCODING_TARGET_LENGTH_HATE,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 773,
     "status": "ok",
     "timestamp": 1740396573264,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "PTpKT2KUDnq7",
    "outputId": "e233ff02-2890-478f-99b5-13973f6de9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_3 (Encoder)         multiple                  114219008 \n",
      "                                                                 \n",
      " decoder_3 (Decoder)         multiple                  164633600 \n",
      "                                                                 \n",
      " dense_101 (Dense)           multiple                  51300000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 330152608 (1.23 GB)\n",
      "Trainable params: 330152608 (1.23 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build the model for the hate speech classification\n",
    "sentence_a = np.array([[2, 3, 1, 3, 0, 0, 0]])\n",
    "sentence_b = np.array([[1, 3, 4, 0, 0, 0, 0]])\n",
    "\n",
    "enc_padding_mask = create_padding_mask(sentence_a)\n",
    "dec_padding_mask = create_padding_mask(sentence_a)\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(sentence_a.shape[1])\n",
    "\n",
    "test_summary, att_weights = transformer_pretrained(\n",
    "    sentence_a,\n",
    "    sentence_b,\n",
    "    False,\n",
    "    enc_padding_mask,\n",
    "    look_ahead_mask,\n",
    "    dec_padding_mask\n",
    ")\n",
    "\n",
    "transformer_pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "executionInfo": {
     "elapsed": 2250,
     "status": "ok",
     "timestamp": 1740396587258,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "_RsDpwHoDnq7"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = './model_weights/model_weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#load the pretrained model weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtransformer_pretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model_weights/model_weights.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = './model_weights/model_weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "#load the pretrained model weights\n",
    "transformer_pretrained.load_weights(\"./model_weights/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1740396590410,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "O6-zJq2jDnq7"
   },
   "outputs": [],
   "source": [
    "class TransformerForBinaryClassification(tf.keras.Model):\n",
    "    def __init__(self, transformer, dropout_rate=0.1):\n",
    "        super(TransformerForBinaryClassification, self).__init__()\n",
    "        self.encoder = transformer.encoder\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.classifier = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, input_sentence, training):\n",
    "        enc_padding_mask = create_padding_mask(input_sentence)\n",
    "        enc_output = self.encoder(input_sentence, training, enc_padding_mask)\n",
    "        cls_output = enc_output[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output, training=training)\n",
    "        final_output = self.classifier(cls_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1740396593571,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "tFwfufnkDnq7"
   },
   "outputs": [],
   "source": [
    "# Initialize the binary classification model\n",
    "binary_classifier = TransformerForBinaryClassification(transformer_pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "executionInfo": {
     "elapsed": 4927,
     "status": "ok",
     "timestamp": 1740396742938,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "8T0BMsGbx3Ku"
   },
   "outputs": [],
   "source": [
    "# Create a dummy input to build the model\n",
    "dummy_input = tf.ones((1, POSITIONAL_ENCODING_INPUT_LENGTH), dtype=tf.int32)\n",
    "\n",
    "# Call the model with the dummy input to create the variables\n",
    "_ = binary_classifier(dummy_input, training=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1740396605230,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "ViyXiGGm765m",
    "outputId": "1894182c-4a26-4b0e-fed3-367d7c624e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_for_binary_classification_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_3 (Encoder)         multiple                  114219008 \n",
      "                                                                 \n",
      " dropout_95 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           multiple                  513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114219521 (435.71 MB)\n",
      "Trainable params: 114219521 (435.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "binary_classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we will compile the model and begin training.\n",
    "\n",
    "Since the model is still large and training for many epochs at once is time-consuming, we will divide the process into **20 epochs per cycle**. After each cycle, we will load the trained model and continue training for another **20 epochs**, repeating this process until we achieve the desired results.\n",
    "\n",
    "I have already **fine-tuned** the **pretrained model** for multiple epochs on **Google Colab** using an **A100 GPU** and saved it inside the **`model_weights/finetuned_model_weights`** directory as **`binary_classifier_weights_v21.h5`**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Fine-tuning Considerations**\n",
    "\n",
    "##### **No Need to Fine-tune Again**\n",
    "Since I have already fine-tuned the model, you can directly use the saved weights.\n",
    "\n",
    "##### **Fine-tuning Requires a Good GPU**\n",
    "If you want to fine-tune the model yourself, you will need a **high-end GPU** and several hours of training time to achieve good results.\n",
    "\n",
    "##### **Why Fine-tuning Took Many Epochs**\n",
    "- In the **pre-training** process using **Masked Language Modeling (MLM)**, the model was trained for only **3 epochs** due to **GPU constraints**, which is **far too low** compared to the required number of epochs for optimal performance.\n",
    "- As a result, the **loss was still high after 3 epochs**.\n",
    "- While **pre-training helps significantly** with fine-tuning, running the model for a larger number of epochs during pre-training would have made fine-tuning easier.\n",
    "- However, due to the **unavailability of high-end GPUs**, I had to **fine-tune the model for many epochs** to achieve the accuracy and loss values shown in the code below.\n",
    "\n",
    "---\n",
    "\n",
    "If you have access to **better GPUs**, you can train the **pre-training phase** for a **larger number of epochs**, which will make **fine-tuning much easier**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 865308,
     "status": "ok",
     "timestamp": 1740400335418,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "j83_61iJDnq7",
    "outputId": "3fb3f908-2e79-4af6-9b50-ed60e9cd10d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 17:38:00.509926: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-03-31 17:38:00.520128: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-03-31 17:38:00.543478: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-03-31 17:38:00.546724: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_3/decoder_3/embedding_7/embeddings:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/query/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/key/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/value/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/query/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/key/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/value/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/attention_output/bias:0', 'dense_89/kernel:0', 'dense_89/bias:0', 'dense_90/kernel:0', 'dense_90/bias:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_102/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_102/beta:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_103/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_103/beta:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_104/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_104/beta:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/query/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/key/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/value/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/query/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/key/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/value/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/attention_output/bias:0', 'dense_91/kernel:0', 'dense_91/bias:0', 'dense_92/kernel:0', 'dense_92/bias:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_105/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_105/beta:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_106/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_106/beta:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_107/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_107/beta:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/query/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/key/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/value/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/query/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/key/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/value/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/attention_output/bias:0', 'dense_93/kernel:0', 'dense_93/bias:0', 'dense_94/kernel:0', 'dense_94/bias:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_108/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_108/beta:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_109/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_109/beta:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_110/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_110/beta:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/query/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/key/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/value/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/query/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/key/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/value/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/attention_output/bias:0', 'dense_95/kernel:0', 'dense_95/bias:0', 'dense_96/kernel:0', 'dense_96/bias:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_111/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_111/beta:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_112/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_112/beta:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_113/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_113/beta:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/query/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/key/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/value/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/query/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/key/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/value/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/attention_output/bias:0', 'dense_97/kernel:0', 'dense_97/bias:0', 'dense_98/kernel:0', 'dense_98/bias:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_114/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_114/beta:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_115/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_115/beta:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_116/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_116/beta:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/query/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/key/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/value/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/query/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/key/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/value/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/attention_output/bias:0', 'dense_99/kernel:0', 'dense_99/bias:0', 'dense_100/kernel:0', 'dense_100/bias:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_117/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_117/beta:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_118/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_118/beta:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_119/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_119/beta:0', 'transformer_3/dense_101/kernel:0', 'transformer_3/dense_101/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_3/decoder_3/embedding_7/embeddings:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/query/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/key/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/value/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/query/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/key/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/value/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/attention_output/bias:0', 'dense_89/kernel:0', 'dense_89/bias:0', 'dense_90/kernel:0', 'dense_90/bias:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_102/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_102/beta:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_103/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_103/beta:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_104/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_104/beta:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/query/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/key/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/value/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/query/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/key/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/value/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/attention_output/bias:0', 'dense_91/kernel:0', 'dense_91/bias:0', 'dense_92/kernel:0', 'dense_92/bias:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_105/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_105/beta:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_106/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_106/beta:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_107/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_107/beta:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/query/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/key/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/value/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/query/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/key/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/value/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/attention_output/bias:0', 'dense_93/kernel:0', 'dense_93/bias:0', 'dense_94/kernel:0', 'dense_94/bias:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_108/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_108/beta:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_109/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_109/beta:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_110/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_110/beta:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/query/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/key/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/value/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/query/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/key/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/value/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/attention_output/bias:0', 'dense_95/kernel:0', 'dense_95/bias:0', 'dense_96/kernel:0', 'dense_96/bias:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_111/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_111/beta:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_112/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_112/beta:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_113/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_113/beta:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/query/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/key/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/value/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/query/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/key/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/value/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/attention_output/bias:0', 'dense_97/kernel:0', 'dense_97/bias:0', 'dense_98/kernel:0', 'dense_98/bias:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_114/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_114/beta:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_115/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_115/beta:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_116/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_116/beta:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/query/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/key/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/value/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/query/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/key/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/value/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/attention_output/bias:0', 'dense_99/kernel:0', 'dense_99/bias:0', 'dense_100/kernel:0', 'dense_100/bias:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_117/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_117/beta:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_118/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_118/beta:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_119/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_119/beta:0', 'transformer_3/dense_101/kernel:0', 'transformer_3/dense_101/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_3/decoder_3/embedding_7/embeddings:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/query/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/key/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/value/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/query/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/key/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/value/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/attention_output/bias:0', 'dense_89/kernel:0', 'dense_89/bias:0', 'dense_90/kernel:0', 'dense_90/bias:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_102/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_102/beta:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_103/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_103/beta:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_104/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_104/beta:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/query/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/key/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/value/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/query/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/key/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/value/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/attention_output/bias:0', 'dense_91/kernel:0', 'dense_91/bias:0', 'dense_92/kernel:0', 'dense_92/bias:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_105/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_105/beta:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_106/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_106/beta:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_107/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_107/beta:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/query/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/key/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/value/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/query/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/key/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/value/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/attention_output/bias:0', 'dense_93/kernel:0', 'dense_93/bias:0', 'dense_94/kernel:0', 'dense_94/bias:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_108/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_108/beta:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_109/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_109/beta:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_110/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_110/beta:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/query/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/key/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/value/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/query/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/key/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/value/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/attention_output/bias:0', 'dense_95/kernel:0', 'dense_95/bias:0', 'dense_96/kernel:0', 'dense_96/bias:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_111/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_111/beta:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_112/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_112/beta:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_113/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_113/beta:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/query/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/key/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/value/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/query/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/key/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/value/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/attention_output/bias:0', 'dense_97/kernel:0', 'dense_97/bias:0', 'dense_98/kernel:0', 'dense_98/bias:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_114/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_114/beta:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_115/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_115/beta:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_116/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_116/beta:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/query/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/key/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/value/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/query/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/key/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/value/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/attention_output/bias:0', 'dense_99/kernel:0', 'dense_99/bias:0', 'dense_100/kernel:0', 'dense_100/bias:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_117/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_117/beta:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_118/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_118/beta:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_119/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_119/beta:0', 'transformer_3/dense_101/kernel:0', 'transformer_3/dense_101/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['transformer_3/decoder_3/embedding_7/embeddings:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/query/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/key/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/value/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_60/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/query/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/key/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/value/bias:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_18/multi_head_attention_61/attention_output/bias:0', 'dense_89/kernel:0', 'dense_89/bias:0', 'dense_90/kernel:0', 'dense_90/bias:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_102/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_102/beta:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_103/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_103/beta:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_104/gamma:0', 'transformer_3/decoder_3/decoder_layer_18/layer_normalization_104/beta:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/query/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/key/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/value/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_62/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/query/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/key/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/value/bias:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_19/multi_head_attention_63/attention_output/bias:0', 'dense_91/kernel:0', 'dense_91/bias:0', 'dense_92/kernel:0', 'dense_92/bias:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_105/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_105/beta:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_106/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_106/beta:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_107/gamma:0', 'transformer_3/decoder_3/decoder_layer_19/layer_normalization_107/beta:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/query/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/key/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/value/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_64/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/query/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/key/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/value/bias:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_20/multi_head_attention_65/attention_output/bias:0', 'dense_93/kernel:0', 'dense_93/bias:0', 'dense_94/kernel:0', 'dense_94/bias:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_108/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_108/beta:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_109/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_109/beta:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_110/gamma:0', 'transformer_3/decoder_3/decoder_layer_20/layer_normalization_110/beta:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/query/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/key/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/value/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_66/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/query/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/key/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/value/bias:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_21/multi_head_attention_67/attention_output/bias:0', 'dense_95/kernel:0', 'dense_95/bias:0', 'dense_96/kernel:0', 'dense_96/bias:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_111/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_111/beta:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_112/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_112/beta:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_113/gamma:0', 'transformer_3/decoder_3/decoder_layer_21/layer_normalization_113/beta:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/query/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/key/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/value/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_68/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/query/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/key/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/value/bias:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_22/multi_head_attention_69/attention_output/bias:0', 'dense_97/kernel:0', 'dense_97/bias:0', 'dense_98/kernel:0', 'dense_98/bias:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_114/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_114/beta:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_115/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_115/beta:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_116/gamma:0', 'transformer_3/decoder_3/decoder_layer_22/layer_normalization_116/beta:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/query/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/key/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/value/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_70/attention_output/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/query/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/query/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/key/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/key/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/value/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/value/bias:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/attention_output/kernel:0', 'transformer_3/decoder_3/decoder_layer_23/multi_head_attention_71/attention_output/bias:0', 'dense_99/kernel:0', 'dense_99/bias:0', 'dense_100/kernel:0', 'dense_100/bias:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_117/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_117/beta:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_118/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_118/beta:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_119/gamma:0', 'transformer_3/decoder_3/decoder_layer_23/layer_normalization_119/beta:0', 'transformer_3/dense_101/kernel:0', 'transformer_3/dense_101/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/875 [..............................] - ETA: 3:06:46 - loss: 0.6413 - accuracy: 0.6719"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m binary_classifier\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-7\u001b[39m),\n\u001b[1;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(),\n\u001b[1;32m      4\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_final_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_final_dataset\u001b[49m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m binary_classifier\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./binary_classifier_weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "binary_classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-7),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Fine-tune the model\n",
    "history = binary_classifier.fit(\n",
    "    training_final_dataset,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_data=validation_final_dataset\n",
    "\n",
    ")\n",
    "binary_classifier.save_weights(\"./binary_classifier_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "Now, let's load the fine-tuned model checkpoints and assess its accuracy and precision on the training and validation datasets.\n",
    "\n",
    "##### **Expected Results**\n",
    "When you run the following three cells, you should obtain the following results:\n",
    "- **Validation Loss:** `0.03448324650526047`\n",
    "- **Validation Accuracy:** `0.9909999966621399`\n",
    "- **Training Loss:** `0.07335779070854187`\n",
    "- **Training Accuracy:** `0.9658928513526917`\n",
    "\n",
    "##### **Why is Validation Accuracy Higher than Training Accuracy?**\n",
    "Several factors contribute to this phenomenon:\n",
    "\n",
    "1. **Small Validation Set**: The validation dataset I took consists of only **2,000 samples** out of the total **30,000**, which is just **6%**. A more balanced split, such as **20% validation data**, If used would provide a more reliable accuracy estimate.\n",
    "\n",
    "2. **Nature of the Dataset**: The dataset used was taken from the internet. While it is the best available labeled dataset for **Amharic hate speech recognition**, it has certain limitations. Upon inspection, some sentences appear **very similar**, which may cause the model to generalize better on validation data, leading to a higher accuracy score.\n",
    "\n",
    "3. **Effect of Dropout Layers**: During training, **dropout layers** are applied to prevent overfitting. However, these layers are **not applied during validation**, which can lead to artificially higher validation accuracy compared to training accuracy.\n",
    "\n",
    "These factors explain why the validation accuracy appears higher than the training accuracy in this scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57800,
     "status": "ok",
     "timestamp": 1740401975556,
     "user": {
      "displayName": "Mahder Tesfaye",
      "userId": "10113678197611266820"
     },
     "user_tz": -180
    },
    "id": "kggaaYlPOLO7",
    "outputId": "6773bc2d-bcd5-422c-9747-e121572c6424"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "binary_classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-7),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "binary_classifier.load_weights(\"./model_weights/finetuned_model_weight/binary_classifier_weights_v21.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_loss,training_acc\u001b[38;5;241m=\u001b[39m\u001b[43mbinary_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_final_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/engine/training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2198\u001b[0m             ):\n\u001b[1;32m   2199\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2200\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/keras/src/engine/training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4000\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   4002\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:890\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    893\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_function_spec  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    894\u001b[0m       \u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    895\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/Real Projects/Mahder AI/MlnewEnv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loss,training_acc=binary_classifier.evaluate(training_final_dataset)\n",
    "print(f\"Training loss: {training_loss}\")\n",
    "print(f\"Training accuracy: {training_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 257s 4s/step - loss: 0.0345 - accuracy: 0.9910\n",
      "Validation loss: 0.03448324650526047\n",
      "Validation accuracy: 0.9909999966621399\n"
     ]
    }
   ],
   "source": [
    "val_loss,val_acc=binary_classifier.evaluate(validation_final_dataset)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deployment on Mahder AI App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **fine-tuned model checkpoints** stored in **`./model_weights/finetuned_model_weights/binary_classifier_weights_v21.h5`** will be deployed to **MahderAI**, a web application designed for real-time access to AI models.\n",
    "\n",
    "### **What is MahderAI?**\n",
    "MahderAI is a **collection of AI models** that I train, fine-tune, and deploy for solving real-world problems.\n",
    "\n",
    "### **Use Case: Telegram Hate Speech Detection**\n",
    "This trained model will be integrated into **MahderAI** to detect and evaluate the **degree of hate speech** in Amharic content from **Telegram channels**.\n",
    "\n",
    "To see how the model performs in detecting hate speech and solving real-life problems, visit **[MahderAI](https://mahderai.com)**. (replace this with the correct website)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This project successfully demonstrated the process of pre-training a Transformer model from scratch on a large Amharic text corpus and then fine-tuning it for a specific task: Amharic hate speech recognition. \n",
    "\n",
    "**Key Achievements:**\n",
    "\n",
    "*   **Data Collection and Preprocessing:** Successfully collected and cleaned a large Amharic dataset from various Telegram channels. Implemented effective text cleaning techniques and employed the SentencePiece tokenizer for subword tokenization, catering to the nuances of the Amharic language.\n",
    "*   **Transformer Model Implementation:** Built and trained a complete Transformer model with both Encoder and Decoder, including positional encoding, masking, multi-head attention, and feed-forward networks. This allowed the model to effectively learn contextualized word representations.\n",
    "*   **Masked Language Model (MLM) Pre-training:** Pre-trained the Transformer model using MLM, enabling it to understand Amharic language semantics and long-range dependencies.\n",
    "*   **Fine-Tuning for Hate Speech Recognition:** Fine-tuned the pre-trained model on a labeled hate speech dataset, adapting it to the specific task of identifying hate speech in Amharic text. Achieved promising results with high validation and training accuracies.\n",
    "*   **Deployment on Mahder AI:** Integrated the fine-tuned model into the Mahder AI web application, making it accessible for real-time hate speech detection in Amharic content from Telegram channels.\n",
    "\n",
    "**Challenges and Limitations:**\n",
    "\n",
    "*   **Computational Resources:** Limited GPU resources constrained the pre-training process to only 3 epochs, impacting the model's full potential. Longer pre-training would likely yield significantly better results.\n",
    "*   **Dataset Size and Quality:** While the hate speech dataset was the best available, its limited size (30k labeled data) and potential inconsistencies in labeling could be improved upon.\n",
    "*   **Validation Set Imbalance:** The small size and potential similarities within the validation set may have contributed to inflated accuracy scores.\n",
    "\n",
    "**Future Directions:**\n",
    "\n",
    "*   **Increased Pre-training:** Extend the pre-training phase with more epochs using more powerful GPU resources to enhance the model's language understanding.\n",
    "*   **Larger and Higher-Quality Dataset:** Curate and label a larger and more diverse Amharic hate speech dataset to improve the fine-tuning process and model performance.\n",
    "*   **Experimentation with Hyperparameters:** Explore different Transformer architectures and hyperparameters to optimize the model for hate speech detection.\n",
    "\n",
    "\n",
    "**Overall, this project demonstrates the potential of pre-trained Transformer models for Amharic natural language processing tasks. By addressing the identified limitations and pursuing future research directions, even more powerful and reliable Amharic NLP solutions can be developed and deployed for real-world applications.**\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MlnewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
